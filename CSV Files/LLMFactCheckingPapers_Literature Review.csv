Serial,Paper Title,Paper Link,Journal/Conference,Journal/Conference Name,Paper type,Year,Month,Country,Domain,Dataset ,Model Used,Problem,Methodology,RAG,Metrics,Findings,Result,RQ2,RQ4,RQ5,Additional Comments,XAI,GitHub Link
1,MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents,[2404.10774] MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents,Conference,EMNLP,A*,2024,November,USA,Natural Language Processing (NLP),"Synthetic: C2D (Claim to Doc), D2C (Doc to Claim), C2D-SIMP, D2C-SIMP.
Benchmark (LLM-AGGREFACT): AGGREFACT (CNN/XSum), TOFUEVAL (MediaS/MeetB), CLAIMVERIFY, LFQA, EXPERTQA, REVEAL, FACTCHECK-GPT, WICE.
Training Integration: ANLI.
Excluded: HALUEVAL, SUMMEDITS, FACTSCORE.","    *   Their models: MiniCheck-FT5 (fine-tuned Flan-T5), MiniCheck-RBTA (fine-tuned ROBERTa), MiniCheck-DBTA (fine-tuned DeBERTa). Used for efficient fact-checking after fine-tuning on synthetic data.
    *   Baseline Specialized Models: T5-NLI-Mixed, DAE, QAFactEval, SummaC-ZS, SummaC-CV, AlignScore, FT5-ANLI-L. These are prior specialized fact-checkers used for comparison.
    *   Baseline LLMs: Gemini-Pro, PaLM2-Bison, Mistral-8x7B, Mistral-Large, Claude 2.1, Claude 3 Opus, GPT-3.5, GPT-4. Used as fact-checkers for performance and cost comparison.
    *   Data Generation Models: GPT-3.5 (for claim decomposition, merging atomic facts), GPT-4 (for atomic fact expansion, document generation, decontextualization, entailment checks for filtering). Used to create the synthetic training data.","The main problem the paper is solving is the ""computationally expensive"" nature of current LLM-based fact-checking approaches, which ""requir[es] many calls to a model to check a single response"" by verifying ""each piece of a model generation against potential evidence using an LLM"". The paper aims to build ""small fact-checking models that have GPT-4-level performance but for 400x lower cost."""," Two methods:
        *   **C2D (Claim to Doc):** Start with human claims. 1) Decompose claim into atomic facts using GPT-3.5. 2) Expand atomic facts into sentence pairs using GPT-4 such that the fact is supported only by combining both sentences. 3) Generate supporting document from sentence pairs using GPT-4. 4) Generate non-supporting documents by omitting one sentence from a pair and checking non-support using GPT-4 entailment. 5) Pair subclaims (merged atomic facts) with generated documents, creating supported and unsupported instances based on atomic fact presence.
        *   **D2C (Doc to Claim):** Start with human documents. 1) Summarize document chunks into summary sentences using GPT-4. 2) Decompose summary sentence into atomic facts and create augmented subclaims. 3) Augment document-claim pairs (chunk + summary) by removing sentences from the chunk and checking support using GPT-4 entailment. 4) Augment cross-document-claim pairs by checking summary sentences against other chunks in the same document using GPT-4 entailment.
    *   **Model Training:** Fine-tune small models (Flan-T5, ROBERTa, DeBERTa) on the synthetic data, sometimes combined with ANLI, using standard cross-entropy loss. A two-stage fine-tuning is used for MiniCheck models, starting with ANLI+C2D data and then fine-tuning on D2C data.
    *   **Evaluation:** Evaluate models on the LLM-AGGREFACT benchmark using balanced accuracy (BAcc).","The paper does not integrate RAG into its MiniCheck models; instead, it evaluates the fact-checking performance of models (including their own) in settings that utilize RAG or post-hoc retrieval. The LLM-AGGREFACT benchmark includes tasks like Retrieve-Then-Generate and Post-Hoc Grounding, where LLM outputs are checked against retrieved documents (evidence). In these settings, evidence is typically retrieved for each claim/sentence being checked, and the MiniCheck models are applied to these claim-document pairs.",balanced accuracy (BAcc),"The authors found that their synthetic data generation methods effectively train smaller models for fact-checking on grounding documents. MiniCheck models, trained on this synthetic data, outperform prior specialized fact-checkers (like AlignScore) and can achieve performance comparable to GPT-4, but with significantly lower inference costs (400x cheaper for MiniCheck-FT5 vs GPT-4). They also found that claim decomposition is not necessary for high fact-checking performance in their settings, and explicit decontextualization did not consistently improve performance on their benchmark. The synthetic data also enhances model robustness in the absence of threshold tuning.","MiniCheck-FT5 (770M parameters) achieves an average balanced accuracy of 74.7% on the LLM-AGGREFACT benchmark (without threshold tuning), which is comparable to GPT-4 (75.3%). This significantly outperforms other specialized fact-checkers of similar or larger size, such as AlignScore (70.4%). MiniCheck-RBTA and -DBTA also outperform most specialized baselines. Crucially, MiniCheck-FT5 achieves this performance at an inference cost more than 400 times lower than GPT-4 API calls on the benchmark test set.","The paper introduces Self-Reflective Generation (SR-GEN) as a novel method to reduce hallucinations in LLMs. The approach works in three stages:

Initial Generation – The model produces a response.

Self-Evaluation – It identifies factual errors in its own output.

Revision – It corrects the response based on detected issues.

To support this, they implement:

Prompt-Level Interventions: Prompts are designed to encourage the model to reflect on its output, reducing hallucinations without relying on external retrieval systems.

Supervised Fine-Tuning: Models are fine-tuned using annotated datasets to penalize hallucinated content and reinforce factual correctness.

No Dependency on RAG: The method is self-contained, avoiding the complexity and potential pitfalls of retrieval-augmented generation.","Prompt design is crucial for the quality of the synthetic data generated by LLMs like GPT-4. The paper uses sophisticated, multi-step prompting (Section 3, Appendix H) to ensure realistic yet challenging synthetic instances, which they show leads to better model performance compared to simpler prompting (Appendix A.1). Fine-tuning various architectures (Flan-T5, ROBERTa, DeBERTa) on this synthetic data allows them to identify which backbones perform best (MiniCheck-FT5 performs strongest, Table 2), showing architectural choices and training data selection are important. Domain-specific training is implicitly handled by the diverse datasets in LLM-AGGREFACT (news, dialogue, science, healthcare). The paper shows their models generalize well across these domains after training on their synthetic data. Interpretability is a limitation; specialized models don't reveal internal decisions. Claim decomposition can improve interpretability by localizing errors to atomic facts, but the paper finds it unnecessary for high accuracy (Section 7.1). Generating explanations is suggested as future work for interpretability.","The paper doesn't integrate RAG into the fact-checking model itself but evaluates fact-checkers on the outputs of systems that utilize RAG or post-hoc retrieval. The LLM-AGGREFACT benchmark includes datasets representing these settings, demonstrating that their MiniCheck models perform well in verifying outputs when grounding documents (often retrieved per claim) are provided. Challenges discussed include the need for decontextualization in RAG settings to ensure meaningful retrieval (Section 7.2), although they found it didn't help their checkers on this benchmark. Another challenge, highlighted as a limitation, is reasoning over evidence that is significantly separated or spread across multiple documents, which is not strongly evaluated by their current benchmark datasets (Section 9). The diverse domains in LLM-AGGREFACT represent a challenge for generalization, which their synthetic data helps models address.",,No,https://github.com/Liyan06/MiniCheck
2,"Towards LLM-based Fact Verification on News Claims with a Hierarchical
Step-by-Step Prompting Method",https://aclanthology.org/2023.ijcnlp-main.64.pdf?,Conference,Acl-anthology,A*,2023,November,Singapore,News Claims & Misinformation Detection,"RAWFC ,LIAR",GPT-3.5,"The main problem the paper addresses is the under-exploration and sub-optimal performance of Large Language Models (LLMs) in the domain of news claim verification, particularly when using standard or Chain-of-Thought (CoT) prompting. The paper notes that ""leveraging LLM reasoning in the context of fake news related tasks remains under-explored"" and that vanilla CoT can suffer from ""omission of necessary thoughts"" and ""fact hallucination,"" leading to incorrect judgments. The paper aims to boost LLM performance and mitigate these issues for more reliable fact verification.","Hierarchical Step-by-Step (HiSS) Prompting:

Decomposes complex claims into smaller subclaims.

Sequential verification of each subclaim using the LLM.

Final aggregation of subclaim verifications to classify the original claim as true or false.  Their methodology involves using in-context learning (ICL) with a few-shot approach, specifically using 4-shot demonstration examples. They evaluated several prompting methods, including standard prompting and CoT-based methods. To address the shortcomings of vanilla CoT, they propose a novel method called Hierarchical Step-by-Step (HiSS) prompting","Yes, the paper uses a method similar to Retrieval-Augmented Generation (RAG). HiSS prompts the LLM to employ a search engine for providing up-to-date external information, aiding the model in reasoning and mitigating the hallucination problem. During the subclaim verification step, if the LLM is not confident in its ability to answer a probing question, it prompts for external knowledge acquisition via web search. The search results are then inserted back into the prompt to facilitate the generation of the answer. This integration of external, retrieved information is a key component of the HiSS method.","Accuracy,F1 Score,Precision,Recall    For evaluating the quality of explanations, they conducted a human evaluation using scores (1, 2, 3, higher is better) based on the criteria of Coverage, Non-redundancy, Readability, and Overall quality"," The authors found that LLMs with In-Context Learning (ICL), using only four-shot demonstration examples, can achieve performance comparable to or even better than previous supervised models for news claim verification. The proposed HiSS prompting method outperforms state-of-the-art fully-supervised approaches and strong few-shot ICL baselines. They found that vanilla CoT performs worse than standard prompting due to significant issues with fact hallucination and omission of necessary thoughts. HiSS effectively addresses both hallucination and thought omission issues. Claim decomposition and the step-by-step verification process are important for the performance improvement of HiSS. Integrating search results improves performance, especially when the model is not confident, suggesting the model has a reasonably good estimation of its own confidence. Human evaluation indicates that HiSS-prompted explanations offer superior coverage and readability compared to a strong supervised explainable model.","The proposed HiSS prompting method, using GPT-3.5 with 4-shot examples, achieved state-of-the-art performance for few-shot news claim verification. It outperformed the previous fully-supervised state-of-the-art model (CofCED) by 1.9% in F1 on RAWFC and 8% in F1 on LIAR. HiSS also significantly surpassed other few-shot ICL methods like standard prompting, vanilla CoT, and ReAct. Furthermore, HiSS generated more fine-grained and human-preferred explanations.","The paper identifies ""Fact hallucination"" as a key issue where LLMs generate relevant but unreliable ""facts,"" misleading the final prediction (as seen in Figure 1 with vanilla CoT). This significantly undermines fact-checking reliability.
Strategies proposed (within HiSS) to mitigate this are:
Claim Decomposition: Breaking complex claims into smaller subclaims reduces the scope for extensive, ungrounded reasoning that can lead to hallucination.
External Knowledge Integration: Prompting the LLM to use a search engine (Google Search) to retrieve up-to-date external information when it expresses low confidence in answering its own probing questions. This provides factual grounding for its responses.
The results (Table 3) show HiSS reduces fact hallucination errors to 5%, compared to 43% for vanilla CoT and 28% for ReAct.","Prompt Design: The paper extensively demonstrates that prompt design is critical. The Hierarchical Step-by-Step (HiSS) prompt, which incorporates claim decomposition, iterative question-answering for subclaims, and confidence-based external search, significantly improves accuracy (outperforming standard prompts, vanilla CoT, and ReAct) and interpretability (generating more fine-grained, human-preferred explanations) compared to simpler prompting strategies.
Fine-tuning Architectures: The paper does not explore fine-tuning architectures. It focuses exclusively on in-context learning (ICL) with a frozen pre-trained LLM (GPT-3.5 text-davinci-003), where learning happens via demonstration examples at inference time.
Domain-specific Training: The paper does not involve domain-specific model training. However, the demonstration examples used for ICL are domain-specific (i.e., examples of news claim verification). The effectiveness of HiSS relies on these few-shot, domain-relevant examples to guide the LLM's behavior.","Impact of Integrating RAG (or similar): The paper's HiSS method integrates a RAG-like mechanism by allowing the LLM to trigger web searches when it lacks confidence. The ablation study (Section 4.3, Figure 3) shows this is highly impactful: ""HiSS w/o search"" performs poorly (49.8% F1 on RAWFC), indicating that relying solely on LLM's internal knowledge is insufficient. HiSS (with self-decided search) achieves 54.4% F1, and ""HiSS always search"" achieves a slightly higher 55.4% F1. This demonstrates that access to external, retrieved information significantly boosts performance and mitigates issues like hallucination.
Challenges in Specialized Domains: While the paper focuses on general news claims, it highlights a limitation in Section 6 that would be a challenge in specialized domains: ""it operates under the assumption that pertinent information is readily accessible through web search. However, not all information is indexed or available in search engines."" For specialized domains, crucial verifying information might not be on the public web or easily discoverable by general search engines. Furthermore, claims ""beyond established world knowledge when necessary relevant knowledge is not complete or even not available"" pose a significant challenge, requiring advanced inference capabilities beyond current RAG setups.",,"Yes, they use Explainable AI (XAI) concepts and evaluate the explainability of their method. They investigate leveraging LLM reasoning (like CoT) for improved accuracy and explainability. Their HiSS method is designed to provide superior explanations that are more fine-grained and easier to follow",https://github.com/jadeCurl/HiSS
3,Are Fact-Checking Tools Helpful? An Exploration of the Usability of Google Fact Check,https://link.springer.com/chapter/10.1007/978-3-031-97352-9_7,Conference, Data Information in Online Environments,Unranked,2024,August,USA,computational social science and natural language processing (NLP),FakeCovid dataset,evaluates the performance of Google Fact Check,"evaluates the performance and usability of an existing tool, Google Fact Check, a search engine for fact-checking resultsUnderstanding the usability and performance of fact-checking-specific search tools in combating misinformation","1.  **Data Collection:** Selected 1,000 false COVID-19 claims from the FakeCovid dataset. Used the Google Fact Check API to retrieve fact-checking results for these claims.
    2.  **Data Sanitization - Relevance:** Three coders individually rated whether each retrieved fact-checked claim was relevant to the corresponding input claim (addressed the same issue). Krippendorff’s alpha was used to measure inter-coder agreement.
    3.  **Data Sanitization - Verdict Mapping:** Original fact-checking verdicts were reviewed and mapped into four standardized categories: ""False,"" ""Partly False,"" ""True,"" and ""Unratable,"" based on definitions adopted by the sources. Three coders reviewed and mapped complex verdicts, with Krippendorff’s alpha used for agreement.
    4.  **Source Reliability Assessment:** Referenced two media evaluation websites (Interactive Media Bias Chart by Ad Fontes Media and Media Bias/Fact Check) to evaluate the reliability and political leaning of the fact-checking sources found in the results. Their ratings were mapped to standardized categories.
    5.  **Linguistic Analysis of Input Claims:** Used LIWC to extract linguistic features (word count, analytical thinking, clout, authentic, emotional tone, words per sentence, dictionary words) from the 1,000 input claims.
    6.  **Correlation Analysis:**
        *   Calculated Spearman's rank correlation between linguistic characteristics of input claims and the number of retrieved fact-checking results.
        *   Performed Kruskal-Wallis H tests to find significant differences in input claim characteristics based on: (a) relevance of results, (b) fact-checking verdicts, and (c) fact-checking sources.
    7.  **Input Claim Variation Analysis:** Coders tagged input claims rated ""relevant"" with keywords to identify claims addressing the same issue but described differently. The Jaccard index was calculated to quantify the congruence of their fact-checking results.
","the paper does not use Retrieval-Augmented Generation (RAG). Google Fact Check, the tool being evaluated, is a retrieval system (a specialized search engine) that fetches *existing* fact-checking reports.","Percentage of retrieved results, relevance, verdict distribution, source reliability, correlation coefficients, Kruskal-Wallis H test p-values, Krippendorff’s alpha, and Jaccard index","•
Retrieval Rate: Google Fact Check was able to retrieve fact-checking results for only 15.8% (158 out of 1,000) of the input claims. 84.2% of claims did not retrieve any results.
•
Relevance of Results: Of the retrieved results, 94.46% were rated as relevant to the input claims.
•
Verdict Distribution: Among the relevant results, 91.54% were rated either ""False"" (79.78%) or ""Partly False"" (11.76%). Only one result (0.37%) was rated ""True"" and 8.09% were ""Unratable"".
•
Source Reliability: Among the sources rated by external websites, all were rated ""Trustworthy"" or ""Relatively Trustworthy,"" indicating high reliability. A majority of ratings (63.64%) indicated a ""Center"" political leaning","The study found that Google Fact Check has limited coverage, retrieving fact-checking results for only 15.8% of the 1,000 input COVID-19 false claims. However, when results were retrieved, they were highly relevant (94.46%) and reliable, with most (91.54%) correctly debunking the claims as ""false"" or ""partly false"" from trustworthy sources. Linguistic characteristics of input claims generally did not significantly influence retrieval success, though emotional tone correlated with verdict types, and claim length/dictionary words correlated with the checking source. A key result was that minor variations in input claim wording often led to different fact-checking results, suggesting users might need to experiment with phrasing to find information.",,,,,,
4,PACAR: Automated Fact-Checking with Planning and Customized Action Reasoning Using Large Language Models,https://aclanthology.org/2024.lrec-main.1099.pdf,Conference,aclanthology,A*,2024,May,China,"biomedical literature, general domains","SciFact, FEVEROUS, and HOVER","PACAR,gpt-3.5-turbo-0301",effectively evaluating the truthfulness and accuracy of information,"a claim decomposer with self-reflection to break down complex claims into sub-claims and verify the decomposition, an LLM-centric planner that dynamically selects customized actions (like numerical reasoning and entity disambiguation) and decides when external retrieval is necessary, an executor that carries out the planned actions and retrieves evidence when needed, and a verifier that assesses the veracity of the original claim and generates explanations based on the overall multi-step reasoning process, all built upon LLMs operating in a zero-shot manner","Yes, the paper uses a Retrieval-Augmented Generation (RAG) like method. The framework includes a ""Retrieval Planner"" that decides whether external evidence is necessary for verifying a sub-claim. If deemed necessary, an ""Evidence Executor"" module retrieves this information. For ""Open-Book"" settings, this involves actively accessing external knowledge sources like Wikipedia using the Serper API (Google search). Specifically, for each sub-claim requiring external evidence, the top paragraph (Recall@1) retrieved from an online website via the Serper API is used as supporting evidence. This retrieved evidence is then fed to the subsequent ""Action Executor"" and ""Verifier"" modules along with the sub-claims to aid in reasoning and final veracity prediction. This process of dynamically retrieving and incorporating external information to inform the generation/reasoning process is characteristic of RAG systems.",macro-F1 score,"The authors found that PACAR significantly outperforms baseline methods (LLM-based, few-shot, conventional fine-tuning) across three diverse datasets (HOVER, FEVEROUS, SciFact). The framework excels with complex claims requiring multi-evidence reasoning and also in professional domains like scientific literature (SciFact) by effectively retrieving pertinent evidence. The self-reflection module notably improves performance by correcting and refining sub-claims, especially for complex claims. Customized agents (numerical reasoning, entity disambiguation) effectively address specific challenges and improve inference from multiple evidence sources. PACAR also enhances interpretability compared to black-box LLM approaches. Error analysis revealed persistent challenges in syntax, semantics, and reasoning.",,"The paper mentions that hallucination is one of the inherent limitations of LLMs in effectively addressing fact-checking tasks, which leads to sub-optimal performance.
◦
The paper proposes that their PACAR model addresses LLM shortcomings, including hallucination and limited reasoning ability, by incorporating forward claim decomposition with backward self-reflection and customized reasoning actions performed by specific agents","Prompt Design: PACAR uses specific prompts to define the roles of its different modules (Claim Decomposer, Action Planner, Action Executor, Verifier). For instance, ""we define the role of the selected agent by providing specific prompts"" and ""we define the role of verifier by providing specific prompts."" The effectiveness of PACAR implies that its structured, multi-stage prompting strategy contributes to improved accuracy and interpretability (as seen in the qualitative analysis).
Fine-tuning Architectures: The paper positions PACAR as an alternative to fine-tuning methods. It compares against fine-tuned baselines (BERT-FC, LisT5, RoBERTa-NLI, MULTiVERS) and generally outperforms them, suggesting that its zero-shot, planning-based approach can be more effective or data-efficient. It does not explore different fine-tuning architectures for its own LLM components.
Domain-Specific Training: PACAR itself does not undergo domain-specific training; it operates in a zero-shot manner. However, its evaluation on domain-specific datasets like SciFact (biomedical research) shows its ability to adapt. The retrieval component allows it to fetch domain-specific information, thus indirectly addressing the need for domain knowledge without explicit domain-specific training of the LLM itself.","Impact of Integrating RAG: The paper demonstrates a positive impact of integrating a RAG-like mechanism. The ""Retrieval Planner"" and ""Evidence Executor"" modules allow PACAR to ""utilize external retrieval as a supplementary method to obtain more comprehensive and accurate information."" This is crucial for grounding claims in factual evidence, especially for claims that the LLM cannot verify from its parametric knowledge alone. The strong performance on datasets like SciFact, which requires external evidence from scientific papers, underscores the benefit. The retrieval planner also optimizes efficiency by initiating retrieval ""only when deemed necessary.""
Challenges in Specialized Domains: The paper doesn't explicitly detail ""challenges"" of RAG in specialized domains but rather shows how its RAG-like approach addresses the inherent challenge of fact-checking in such domains. The primary challenge is accessing and correctly interpreting specialized knowledge. PACAR's retrieval component (using Serper API for SciFact) aims to overcome this by fetching relevant external documents. The effectiveness of PACAR on SciFact suggests its approach is viable for specialized domains, but potential underlying challenges could include the quality/availability of retrievable specialized documents and the LLM's ability to correctly synthesize information from highly technical texts even when retrieved. The paper's error analysis (syntax, semantic, reasoning errors) points to general challenges that would also apply to interpreting retrieved specialized content.",,"Yes, the paper emphasizes explainability and interpretability. The verifier module generates a comprehensive explanation for the veracity prediction. The method of generating explanations guided by specific agents is presented as a novel technique to improve accuracy and interpretability. The explicit claim decomposition, dynamic action planner, and executor are highlighted as contributing to improved interpretability for human understanding. Providing informative and contextually relevant explanations significantly improves transparency and interpretability.",
5,Multi-source Knowledge Enhanced Graph Attention Networks for Multimodal Fact Verification,https://arxiv.org/pdf/2407.10474,,ICME,B,2024,July,China,Multimodal Fact Verification,FACTIFY and MOCHEG,"Multi-Source Knowledge-enhanced Graph Attention Network (MultiKE-GAT),  TAGME: Used for extracting textual entities from claim and evidence texts. Faster R-CNN: Used for detecting visual entities (objects) from claim and evidence images.LLM (GPT-turbo-3.5): Used to extract key information (phrases) from claim and evidence texts.DeBERTa: Used as the pre-trained language model to obtain initial textual features for entities and global text representations (claim/evidence text).Swin Transformer: Used as the pre-trained visual model to extract initial visual features for entities and global image representations (claim/evidence image).MLP (Multi-Layer Perceptron) Classifier: Used as the final classifier to predict the veracity based on the fused multimodal representations.","The main problem the paper is solving is the effective fusion of features from different modalities (text and image) to learn meaningful multimodal representations for fact verification. Specifically, it addresses the challenge of incorporating fine-grained knowledge from diverse sources and modalities to improve understanding and judgment of facts, while also mitigating inconsistencies and noise introduced by such external knowledge. As stated in the abstract: ""The main challenge in this area is to effectively fuse features from different modalities to learn meaningful multimodal representations."" The first research question further elaborates: ""How can knowledge from different modalities and different sources be effectively integrated to understand better and judge the truthfulness of facts?""","extracting multi-source knowledge (textual and visual entities, key information from LLMs) and constructing a heterogeneous graph to capture cross-modal and cross-source interactions. They then use a Knowledge-aware Graph Fusion (KGF) module to learn knowledge-enhanced representations and reduce noise. Finally, they use an MLP-Classifier for verification based on the fused representations","The paper uses a method analogous to the ""retrieval"" and ""augmentation"" aspects of Retrieval-Augmented Generation (RAG), but not the ""generation"" part for fact-checking itself. It explicitly ""introduces external multimodal knowledge from different sources"" (textual entities from TAGME, visual entities from Faster R-CNN, and key information from LLMs). This extracted knowledge is used to construct a heterogeneous graph, thereby augmenting the original claim and evidence data. This augmented graph representation is then processed by the KGF module for representation learning and classification, rather than being fed into an LLM to directly generate a fact-check or explanation. So, it retrieves and uses external knowledge to enhance input representations for a discriminative model.",weighted-average F1 score and accuracy,"The paper introduces MultiKE-GAT, a model that enhances multimodal fact verification by integrating external knowledge and constructing a heterogeneous graph. The Knowledge-aware Graph Fusion (KGF) module refines representations and eliminates inconsistencies, leading to improved claim verification. Experimental results show that MultiKE-GAT outperforms existing approaches, demonstrating its effectiveness in handling complex interactions across multiple modalities.","The proposed MultiKE-GAT model achieves state-of-the-art performance on both the FACTIFY and MOCHEG benchmark datasets for multimodal fact verification. On FACTIFY, it achieved a weighted F1-score of 79.64% for the 5-way classification task and 83.68% for the 3-way task, outperforming all baseline methods. On MOCHEG, it achieved an F1-score of 70.14%, again surpassing compared methods. These results demonstrate the effectiveness and superiority of integrating multi-source knowledge through the proposed knowledge-enhanced graph attention network.","Its second research question is: ""How can inconsistencies and noise across different modalities be eliminated to ensure the accuracy of the predictions?"" This refers to noise introduced by the external entity extraction process (e.g., irrelevant entities identified by TAGME or Faster R-CNN, or less relevant keyphrases from the LLM). The strategy proposed to mitigate this ""noise"" (not LLM hallucination in generation) is the Knowledge-oriented Graph Fusion (KGF) module. This module uses global representations of the claim and evidence to guide the attention mechanism during graph convolution, thereby emphasizing relevant entities/information and marginalizing insignificant or noisy ones.","The paper does not delve into these aspects of LLM usage for fact-checking. It uses an LLM (GPT-turbo-3.5) primarily as a tool for key phrase extraction from texts, where the prompt design is implicit and not discussed. It also includes GPT-3.5 as a baseline textual fact verification method, which is used without fine-tuning (""GPT-3.5 leverage only textual information to verify the claim without fine-tuning""). The paper focuses on its graph-based architecture and the integration of pre-trained unimodal encoders (DeBERTa, Swin Transformer) rather than exploring LLM fine-tuning strategies or domain-specific training for LLMs themselves in the context of fact-checking accuracy or interpretability.","The paper's approach can be seen as integrating a RAG-like retrieval and augmentation step, where external knowledge (textual entities, visual objects, key phrases) is explicitly extracted and used to build an enriched input representation (the heterogeneous graph) for a discriminative model. The ablation studies (Table III, ""w/o Multi-Knowledge"") show a significant drop in performance (from 79.64 to 73.29 w-F1) when this multi-source external knowledge is removed, demonstrating its positive impact on the system's effectiveness.
However, the paper does not discuss the challenges of applying this approach or general RAG systems in specialized domains. Its experiments are conducted on general news-like multimodal fact verification datasets (FACTIFY, MOCHEG).",,,
6,AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web,https://openreview.net/pdf?id=fKzSz0oyaI,,NeurIPS 2023 ,A*,2023,December,China, Automated Fact-Checking (AFC),AVERITEC,"BERT-large and BART-large ,BLOOM-7b and Vicuna-13b,","substantial limitations in existing automated fact-checking datasets such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, including evidence published after the claim, context dependence, evidence insufficiency, and temporal leaks","Their methodology involves creating a new dataset, AVERITEC, of real-world claims annotated through a multi-round process. This process includes claim normalization, generating and answering question-answer pairs based on web evidence, and providing textual justifications for the veracity labels. They also developed a baseline model to explore the feasibility of the task, using question generation, web search, evidence reranking, stance detection for veracity prediction, and justification generation","Yes, the paper uses a Retrieval-Augmented Generation (RAG)-like method, primarily for its BLOOM-based question generation components in the baseline model. For generating initial search questions from a claim, the system first retrieves the 10 most similar claims from the training set using BM25. The annotated questions from these retrieved claims are then used to construct a few-shot prompt for BLOOM to generate new questions for the input claim. A similar process is used for generating questions from candidate evidence sentences during the evidence selection phase: 10 similar question-answer pairs from the training set are retrieved to prompt BLOOM. This approach leverages existing annotated data to guide the generation process of the LLM in a few-shot, in-context learning manner, rather than just relying on the LLM's pre-trained knowledge.","Inter-Annotator Agreement: Free-marginal κ (kappa), Fleiss' κ.
Lexical/Semantic Overlap (for evidence and justifications): Hungarian METEOR (which uses METEOR for pairwise comparison), METEOR.
Traditional Classification (for veracity): Accuracy (conditioned on evidence retrieval), F1-scores (per-class and macro-average).","The authors successfully created AVERITEC, a dataset of 4,568 real-world claims with QA decomposition and justifications, achieving substantial inter-annotator agreement (κ=0.619). They found that their multi-round annotation process helps avoid context dependence, evidence insufficiency, and temporal leakage. The baseline model highlighted that automated fact-checking, especially evidence retrieval, remains very challenging. Hungarian METEOR scores between human annotators for the same claim were relatively low (0.22 for Q+A), indicating the difficulty of automatic evaluation and suggesting λ=0.25 as a reasonable cutoff. The baseline's performance was significantly gapped from an oracle using gold evidence. ChatGPT showed strong raw question generation but its overall performance was hampered by unverified/hallucinated answers as it lacked a retriever for the AVERITEC task setup. The veracity prediction model particularly struggled with the ""Conflicting Evidence/Cherrypicking"" class.",,"The paper observes that LLMs like ChatGPT can hallucinate answers, providing information that is not supported by actual sources or even inventing sources (e.g., the USDA example on page 9). This significantly affects reliability because the generated evidence, while potentially fluent and plausible-sounding, may be factually incorrect or untraceable, undermining the verification process.
The paper notes their current evaluation metric (Hungarian METEOR) ""cannot detect"" if answers are truly supported by their claimed underlying sources, only comparing text similarity to gold answers (page 10). To mitigate this, they primarily propose:
Improved Evaluation: Stating ""Further research is needed on evaluation to counteract this [hallucination]"" (page 10).
Human Evaluation: Suggesting that ""crowd-sourced human evaluation"" is a ""more reliable alternative"" (page 6), echoing recommendations for the FEVER dataset. Their own annotation process for AVERITEC (evidence sufficiency check) serves as a model for such human evaluation.
They do not propose specific architectural changes to the LLMs themselves to reduce hallucination within this paper, focusing more on dataset creation and evaluation challenges.","The paper discusses the use of fine-tuning for smaller models like BERT and BART on the AVERITEC training set for specific tasks like classification (stance detection, reranking) and generation (justifications). For LLMs (BLOOM, Vicuna), they use a few-shot setup with retrieved in-context examples instead of fine-tuning the entire model. Prompt design is explicitly discussed as crucial for generating useful questions for search queries; they found incorporating the speaker and claim in an ""adversarial tone"" prompt (""Outrageously, SPEAKER claimed that CLAIM. Criticism includes questions like: "") was highly effective for claim question generation. They also experimented with different prompt designs for passage question generation and justification generation","The paper's baseline integrates retrieval by using Google Search restricted by date to find evidence documents. It then uses retrieved in-context examples (a form of RAG) to generate search questions for the LLM (BLOOM), and also uses retrieved examples to generate questions matching evidence sentences. The impact observed is that these generated questions are useful as additional search terms, suggesting that retrieval augmentation can help guide the search process. The paper highlights that the challenge remains in retrieval itself; even with their search strategy guided by generated questions, useful evidence is found in only about half the analyzed cases. This suggests that effective implementation of RAG relies heavily on the quality of the retrieval component, especially query/question generation. The difficulty in finding useful evidence underscores the challenges in retrieval for open-domain, real-world fact-checking, which could be seen as a ""specialized domain"" due to the need for timely, relevant, and verifiable information from a vast and dynamic web. ",,,https://github.com/MichSchli/AVeriTeC
7,"LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve the Factual Consistency of Language Models
(Not published but exists in OpenReview)",https://arxiv.org/html/2410.23526v1,,Arxiv,Preprint,2024,October,Unknown,medical domain,"BioASQ-Y/N, and MedMCQA,USMLE ,MMLU-Medical,PubMedQA,MedRAG","Llama 3 70B Instruct, Llama 3 8B Instruct, Qwen2-72B-Instruct , ColBERT, and  GPT-3.5 and ArmoRM","""Large language models (LLMs) have shown remarkable capabilities in various natural language processing tasks, yet they often struggle with maintaining factual accuracy, particularly in knowledge-intensive domains like healthcare."" (Abstract) More specifically, ""LLMs sometimes generate plausible yet factually incorrect or unverified content... This issue is particularly concerning in domains such as healthcare, where the accuracy and reliability of information are critical."" (Introduction)","LEAF (Learning and Evaluation Augmented by Fact-Checking) employs two main strategies:
◦Fact-Check-Then-RAG: Uses fact-checking results to guide the retrieval process in RAG without updating model parameters.
◦Learning from Fact-Checks via Self-Training: Updates LLM parameters through Supervised Fine-Tuning (SFT) and Simple Preference Optimization (SimPO) using fact-checked responses","Yes, the paper extensively uses Retrieval-Augmented Generation (RAG). Its primary novel usage is in the ""Fact-Check-Then-RAG"" (FC-RAG) mechanism. In FC-RAG, after an initial LLM response is fact-checked, if inaccuracies are found, the documents retrieved by the fact-checker specifically for the unsupported facts are then used to augment a new prompt. This guides the LLM to regenerate a more accurate response, directly addressing the identified factual gaps. This is contrasted with standard RAG (e.g., MedRAG as a baseline), where documents are typically retrieved based on the initial question before the first generation.","accuracy,filtered accuracy,fact-check score,","LEAF effectively detects inaccurate responses and significantly enhances model accuracy. Integrating fact-checked responses, either through RAG enhancement (Fact-Check-Then-RAG) or self-training (SFT or SimPO), improves the reliability and factual correctness of LLM outputs. Specifically, Fact-Check-Then-RAG consistently outperformed standard RAG (MedRAG), which sometimes degraded performance. Self-training on LEAF-verified data also led to substantial accuracy gains. LEAF's fact-checking mechanism proved superior to Factcheck-GPT for filtering and ranking, and using LEAF-ranked responses with SimPO yielded better results than using ArmoRM-ranked responses. The adapted fact-checking system (using Qwen2-72B-Instruct and MedRAG) outperformed a Factcheck-GPT baseline.",Result is given in table ,"The paper states, ""LLMs often struggle to capture fine-grained knowledge and frequently produce inaccurate or fabricated information, commonly referred to as hallucination"" (Related Work). While it doesn't explicitly detail how hallucinations affect fact-checking reliability itself, the entire LEAF framework is designed to mitigate hallucinations in LLM outputs.
Strategies proposed to mitigate hallucinations (inaccuracies) include:
Identification: The core fact-checking process (adapted SAFE) aims to identify sentences that are not supported by retrieved knowledge, which would include hallucinated content.
Correction/Regeneration (Fact-Check-Then-RAG): If an inaccuracy (potentially a hallucination) is detected, RAG is used with guiding documents retrieved by the fact-checker to help the LLM generate a more factually grounded response.
Preventative Learning (Self-Training):
SFT: Fine-tuning the LLM only on responses that pass the fact-check (i.e., are deemed factually correct and thus less likely to be hallucinated) helps the model learn to generate more factual content.
SimPO: Using fact-checking scores to rank responses allows the model to learn to prefer more factual (less hallucinated) outputs over less factual ones.","ntegrating RAG is explored in two ways. Standard RAG (MedRAG) can improve contextual grounding but was shown to sometimes harm performance on certain medical datasets, potentially by introducing noise. However, the Fact-Check-Then-RAG (FC-RAG) approach, which specifically integrates knowledge retrieved during fact-checking into the RAG process, has a positive impact, consistently improving accuracy across all tested medical datasets by leveraging verified information during generation.","The source highlights challenges with general-purpose models and methods in specialized domains. Proprietary models often cannot be deployed on private datasets crucial in sensitive domains like healthcare, and they are not specialized or easily fine-tuned for nuanced understanding in specific areas. The source implies that accessing relevant and reliable domain-specific knowledge sources is key, noting that using general sources like Google Search may not always be relevant or accessible for medical queries, which is why they used the MedRAG corpus. Their approach aims to overcome these challenges by using adaptable components (like a specialized corpus and rater) and open-source models",,,
8,A Dataset for Evaluating Clinical Research Claims in Large Language Models,https://www.nature.com/articles/s41597-025-04417-x,,Arxiv,Preprint,2025,January,Switzerland,clinical research claim verification,"CliniFact,","BioBERT, PubMedBERT, and RoBERTa, Llama3, Meditron, and OpenBioLLM","They aimed to create a benchmark to scrutinize LLMs for issues like hallucination and comprehension of logical statements, as existing datasets had limitations in this specific area in verifying scientific claims from clinical research",They created the CliniFact dataset .They evaluated the performance of several LLMs (both discriminative and generative) on this dataset by treating claim verification as a multiclass classification problem,No,"accuracy  F1-macro score,precision and recall","The study found that discriminative LLMs, particularly BioBERT, outperformed generative LLMs in verifying clinical research claims",The results highlight the strengths of domain-specific models like BioBERT for this task,"Strategies proposed to mitigate factual errors and vulnerability to hallucinations include incorporating domain-specific knowledge. Other approaches mentioned in the source (in the context of related work or challenges) are Chain-of-Thought (CoT) prompting (though reliability is a concern), self-correction (though challenging for autonomous correction), integrating with symbolic solvers for logical reasoning, and hypothesis testing prompting","Prompt design was used for generative LLMs, including a specific prompt instruction. Chain-of-Thought prompting is mentioned as potentially improving multi-step reasoning, but concerns exist about its faithfulness. Hypothesis testing prompting is noted for potentially improving deductive reasoning. The specific prompt used in the evaluation resulted in low zero-shot performance for generative models.
◦
Fine-tuning significantly improved the accuracy and F1-macro scores of generative LLMs on the clinical claim verification task. This demonstrates that fine-tuning is crucial for improving the performance of general-purpose generative models in this domain.
◦
Domain-specific training also positively affects accuracy. Models fine-tuned for specific domains like medicine (e.g., Meditron, Med-PaLM) often outperform general models. In the evaluation, BioBERT, a pre-trained biomedical model, achieved the highest accuracy (80.2%), likely due to its domain-specific training.
◦
The source mentions interpretability in the context of CoT prompting concerns and related work, but does not provide findings on how these factors specifically affected the interpretability of the models evaluated on CliniFact.","The source mentions ""Retrieval augmented scientific claim verification"" in relation to another dataset (CoVERt) and the concept of document retrieval in general fact verification. It also describes linking claims to PubMed abstracts as providing evidence",,,https://github.com/ds4dh/CliniFact
9,Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation,https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-Fidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf,Conference,CVPR,A*,2024,June,USA,"Computer Vision, NLP","COCO, Objaverse","VFC Pipeline (captioning models, object detection, VQA, LLM)	",Addressing lack of detail and hallucination in automatic captioning	,"Proposal, Verification, Captioning steps	",,"CLIP-Score, CLIP-Image-Score, Human Study, GPT-4V Evaluation	",Outperforms state-of-the-art open-source methods; comparable to GPT-4V with smaller model size	,"Generates high-fidelity, detailed captions; mitigates content hallucination	","This paper notes that existing open-sourced captioning methods, including some using MM-LLMs, can suffer from hallucination, producing long descriptions that do not align with the actual content of the images. The proposed VisualFactChecker aims to address these limitations by combining different models into a pipeline via an LLM, striking a better balance between accuracy and detailedness in generated captions while mitigating hallucinations. VFC specifically focuses on tackling hallucinations by employing visual grounding tools, such as object detection and VQA models, to fact-check captions for enhanced accuracy. Undetected objects mentioned in captions for 2D images are considered potential hallucinations and removed. For 3D objects, VQA models are used to answer questions about attributes, and the LLM corrects the description based on these answers, assuming targeted questions result in fewer hallucinations than a general description. Related work mentioned includes hallucination mitigation strategies like data optimization (creating negative instances in training datasets) and iterative generation methods."," The source details the use of prompt design within the VisualFactChecker pipeline, where the LLM follows specific prompts to perform tasks like summarizing initial captions, identifying objects for verification, asking questions for VQA, and revising captions based on fact-checking results. Figure 3 illustrates the specific prompts used. The LLM's ability to follow complex instructions via prompts is highlighted. The paper describes VFC as a ""training-free pipeline"", suggesting reliance on pre-trained LLMs and prompt engineering rather than fine-tuning for this specific task. The source does not discuss fine-tuning architectures, domain-specific training, or the interpretability of LLMs in the context of fact-checking tasks, beyond the reliance on pre-trained models and the use of prompt engineering.",,,,
10,A Vision Check-up for Language Models,https://openaccess.thecvf.com/content/CVPR2024/papers/Sharma_A_Vision_Check-up_for_Language_Models_CVPR_2024_paper.pdf,Conference,CVPR,A*,2024,June,USA,evaluating what Large Language Models (LLMs) learn about the visual world from text,"Visual Aptitude Dataset,scenes. The dataset was constructed using concepts from ADE20K for objects and MS-COCO",GPT-3.5 and GPT-4,The paper addresses the question of what LLMs learn about the visual world from modeling relationships between strings and whether they can be used to train vision systems for natural images,"The researchers evaluate LLMs' abilities to generate visual concepts by generating code, recognize visual concepts from code (human drawings), and correct their generated code using textual feedback. They also explore using LLM-generated images for unsupervised visual representation learning",,"CLIP image-text retrieval percentile ,LPIPS-diversity score, Fréchet Inception Distance (FID) ","LLMs demonstrate a surprising ability to generate intricate visual scenes and model spatial relations by generating code. However, they struggle with textures, precise shapes, and surface contact. Unlike humans, LLMs find recognizing visual concepts from code more challenging than generating them. Text-based feedback can effectively improve the visual quality and fidelity of LLM-generated images. ",performance generally decreasing with increasing complexity. Iterative textual feedback significantly improves the fidelity of generated images,,,,,"Yes, the verification step involves an LLM utilizing object detection and VQA models to fact-check proposed captions, enhancing transparency and interpretability.",https://vision-checkup.github.io/
11,SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.pdf,Conference,CVPR,A*,2024,June,Singapore,"news domain,  Out-Of-Context (OOC) misinformation detection ","NewsCLIPpings,News400,TamperedNews","SNIFFER,InstructBLIP,Vicuna-13B",Detecting out-of-context misinformation (authentic images with false text) and providing convincing explanations for the judgments. Existing detectors lack sufficient explanations,Two-stage instruction tuning on InstructBLIP. Stage 1 aligns the model with the news domain using image captioning data and ChatGPT4. Stage 2 fine-tunes for OOC detection using GPT-4 generated instruction data with judgments and explanations based on inconsistencies between captions of similar images. SNIFFER also incorporates internal checking (image-text consistency) and external checking (relevance of image-retrieved web evidence to the text).,SNIFFER uses retrieval. It employs reverse image search to retrieve webpages as external evidence and feeds the text from these webpages along with the news caption into the LLM for external checking and contextual verification,"Accuracy,response ratio ,ROUGE",SNIFFER outperforms existing methods in OOC misinformation detection accuracy by over 40% compared to the base MLLM and surpasses state-of-the-art methods. It also provides accurate and persuasive explanations. ,Achieved a state-of-the-art accuracy of 88.4% on the NewsCLIPpings Merged/Balance test set.,,,,,"Yes, the research heavily focuses on explainability. SNIFFER is designed to generate natural language explanations for its judgments by identifying the inconsistent news element and the specific entities in the text and image that cause the inconsistency. The evaluation metrics for explanations and the human evaluation further emphasize the XAI aspect.",https://pengqi.site/Sniffer
12,Citations and Trust in LLM Generated Answers,https://arxiv.org/pdf/2501.01303,Conference,AAAI,A*,2025,August,USA,Human-Computer Interaction (HCI),bespoke QA website,ChatGPT4,"The study addressed the problem of the opaque nature of LLM-based question answering systems and its impact on user trust. They investigated whether the presence, number, and relevance of citations can enhance user trust in these systems"," Participants submitted questions to a system that returned ChatGPT4-generated answers with varying numbers of citations (zero, one, or five) that were either relevant or randomly selected. They measured self-reported trust and recorded whether participants checked the citations. This randomized controlled trial with a between-subjects factorial design allowed them to analyze the impact of citation presence, relevance, and checking behavior on trust","The paper discusses Retrieval Augmented Generation (RAG) systems as an approach that incorporates external information and often provides explicit citations to improve transparency and reliability. While they used ChatGPT4 as the base generator, their methodology for the citation conditions involved retrieval augmentation by using a Web search API to find sources relevant to the ChatGPT response, which aligns with the principles of RAG discussed in the paper",self-reported trust ratings," The presence of citations significantly increased user trust, even if the citations were random. However, checking citations led to a significant decrease in trust. Random citations resulted in lower trustworthiness compared to valid ones. There was no significant difference in trust between one and five citations. Political and factual questions received significantly higher trust ratings. More complex questions were slightly less trusted","Citations increased perceived trustworthiness in AI chatbot responses, even when random. However, responses with random citations were less trusted than those with accurate citations. The number of citations (one vs. five) did not significantly affect trust. Trust decreased when participants checked citations, supporting the trust as anti-monitoring theory. Random citations, when checked, did not lead to higher trust than no citations. Political questions received higher trust ratings, likely due to alignment with participant biases. Fact-based questions were more trusted than non-fact-based ones. Simpler questions were slightly more trusted than complex ones, though not significantly. Nonwhite participants were slightly more trusting. Males, liberals, and urban residents were more likely to check citations. Citation checks remained consistent throughout, and encountering random citations did not reduce trust in later responses.","The source mentions concerns about occasional inaccuracy and unreliability in current chatbot versions. It notes that RAG systems tend to reduce inaccurate responses. However, the term ""hallucination"" is not mentioned in the source. The source does not discuss how hallucinations specifically affect fact-checking reliability or what strategies (like prompt design, fine-tuning, etc., beyond mentioning RAG's potential to reduce inaccuracy) have been proposed to mitigate these effects.","The source discusses the impact of prompt complexity (perplexity, length) on user trust, suggesting simpler, more familiar prompts might lead to more accurate and trusted results. It also discusses citations as a form of explainability. However, the source does not provide information on how prompt design, fine-tuning architectures, or domain-specific training specifically affect the accuracy or interpretability of LLMs in the context of fact-checking tasks.","The source states that RAG systems integrate external information and tend to reduce inaccurate responses, also providing citations for transparency and reliability. This suggests a positive impact on reliability, which is relevant to fact-checking. However, the source does not specifically discuss the impact of RAG integration within dedicated fact-checking systems or the challenges associated with implementing RAG in specialized domains.",,,
13,Evaluating LLMs at Detecting Errors in LLM Responses,https://arxiv.org/abs/2404.03602,Conference,COLM,Unranked,2024,April,USA,error detection in Large Language Model (LLM) responses,ReaLMistake,GPT-4 and Llama 2 70B,"Addressing the lack of research and benchmarks for automatically detecting errors in LLM responses, especially objective and realistic errors."," Designed three tasks (Math Word Problem Generation, Fine-grained Fact Verification, Answerability Classification) to elicit errors in four categories (Reasoning Correctness, Instruction-Following, Context-Faithfulness, Parameterized Knowledge)",,"Precision, Recall, and F1-score ","Top LLMs detect errors at very low recall and perform much worse than humans.
Explanations by LLM-based error detectors lack reliability.
Error detection is sensitive to small changes in prompts.
Popular approaches like self-consistency and majority vote do not improve error detection performance","he results demonstrate the poor performance of current LLMs as error detectors compared to humans. For instance, tables show F1 scores, precision, and recall for various models and tasks, often lower than random baselines, especially recall for stronger models on certain tasks. Manual analysis of explanations shows high rates of wrong reasoning or insufficient detail. Experiments on prompt variations and improvement techniques also show their limited or inconsistent impact on performance.","The source addresses Parameterized Knowledge errors, which are factual mistakes and are closely related to hallucinations. The benchmark includes tasks designed to elicit these errors.
◦
The study evaluates how well other LLMs can detect these factual errors. The findings indicate that detecting errors, including factual ones, is challenging for LLMs, especially achieving high recall.
◦
The source evaluates potential strategies to improve error detection performance (self-consistency, majority vote, providing evaluation steps) but finds they do not provide consistent improvement.
◦
Coverage: The source details the difficulty LLMs have in detecting factual errors (hallucinations) in other LLMs and tests strategies to improve this detection. It does not, however, discuss how hallucinations affect the reliability of LLM-based fact-checking systems in a broader sense, nor does it propose or evaluate strategies to mitigate hallucinations during the generation process itself.","The source directly investigates the impact of prompt design (wording and option order) on the accuracy (precision and recall) of LLM-based error detectors. It finds that performance, particularly recall, is sensitive to small prompt variations.
◦
It addresses the interpretability aspect by analyzing the reliability of explanations provided by the error detectors, finding them unreliable.
◦
Coverage: The source provides findings on the effect of prompt design on error detection accuracy and the reliability of explanations (related to interpretability). However, it does not discuss fine-tuning architectures or domain-specific training for either the LLMs being evaluated or the LLM-based detectors within the provided text.",,,,https://github.com/psunlpgroup/ReaLMistake
14,"Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",https://ojs.aaai.org/index.php/AAAI/article/view/30214,Conference,AAAI,A*,2024,February,China,fake news detection,Weibo21 (Chinese) and GossipCop (English),"GPT-3.5-turbo, and a small language model (SLM), BERT",The problem they solved is to improve the automatic detection of fake news by leveraging the strengths of both large and small language models. They specifically address the LLM's limitations in making final veracity judgments despite its ability to provide informative rationales,"Their approach employs an ARG network to guide SLMs using rationales from LLMs, with a distilled version (ARG-D) for cost-effective applications. Key components include rationale evaluation, judgment prediction, and interaction between news and rationales.",,"macro F1 score, F1 ","Fine-tuned SLMs like BERT outperform sophisticated LLMs like GPT-3.5 in fake news detection. Combining LLM analytical capabilities with SLM task-specific knowledge improves detection. ARG and its distilled version, ARG-D, enhance performance, with ARG achieving top results on Chinese and English datasets. ARG-D is cost-effective and shows significant improvements.",ARG achieves the best performance in macro F1 and accuracy on both Chinese and English datasets compared to the baselines. ARG-D also shows significant improvements over the baselines,"The source mentions that hallucinations might cause the unreliability of using the LLM for factuality analysis based on its internal memorization. It suggests that the analysis from the perspective of factuality leads to lower-than-average performance, speculating this is caused by the hallucination issue. The paper's proposed method (ARG) effectively avoids over-reliance on the LLM's direct factuality judgments by integrating insights from multiple perspectives and allowing the SLM to selectively use rationales, improving performance despite this LLM limitation. The source does not propose strategies to mitigate hallucination effects within the LLMs themselves.","The source investigates the effect of different prompt designs (zero-shot, zero-shot CoT, few-shot, few-shot CoT) on the LLM's performance in this study. It finds that few-shot versions outperform zero-shot, and CoT can bring additional gain, but effective use requires careful design. It also highlights that a task-specific fine-tuned SLM outperforms the prompted LLM, indicating the importance of domain-specific learning for the SLM. The paper focuses on using LLM-generated rationales to guide an SLM and does not provide a general discussion on how different fine-tuning architectures or domain-specific training of LLMs affect accuracy or interpretability in fact-checking tasks beyond the specific models and methods evaluated.",,,,https://github.com/ICTMCG/ARG
15,Detecting misinformation with LLM-predicted credibility signals and weak supervision.,https://eprints.whiterose.ac.uk/223248/,Journal,EPJ Data Science,Q1,2024,November,UK, Misinformation detection, FA-KES (Syrian war news) and EUvsDisinfo (pro-Kremlin propaganda),"GPT-3.5-Turbo, Alpaca-LoRA-30B, OpenAssistant-LLaMa-30B (for weak labeling) and RoBERTa-Base ",Automating misinformation detection in long articles without extensive ground-truth labels and improving the transparency of detection,"Prompting LLMs in a zero-shot manner to generate weak labels for 18 credibility signals, then using weak supervision (Snorkel framework) to aggregate these noisy labels for predicting content veracity. They also performed supervised fine-tuning with RoBERTa-Base for comparison",,Accuracy and F1-macro scores,Prompted weak supervision with credibility signals generally outperforms zero-shot prompting and can surpass supervised fine-tuning in some cases. The usefulness of individual credibility signals varies across datasets. LLMs can leverage pre-training knowledge to assess credibility based on external information,"Prompted PWS with credibility signals outperformed zero-shot prompting and supervised RoBERTa fine-tuning on both datasets. The FULL approach (label model + text classifier on weak labels) generally improved results over using the label model alone. OpenAssistant-30B-FULL had the best performance on FA-KES (55.3% accuracy, 54.8% F1-macro), and GPT-3.5-Turbo-FULL led on EUvsDisinfo (99.3% accuracy, 99.0% F1-macro), though data leakage concerns were noted. Excluding GPT-3.5-Turbo, OpenAssistant-30B-FULL still led on EUvsDisinfo (91.3% accuracy, 85.8% F1-macro). Prompted PWS (FULL) showed a relative F1-macro improvement of +23.1% (FA-KES) and +92.1% (EUvsDisinfo) over zero-shot prompting. On FA-KES, OpenAssistant-30B-FULL ranked second among prior work, outperforming some supervised methods without using ground-truth labels. Individual credibility signals varied in effectiveness by dataset—Sensationalism was highly predictive in EUvsDisinfo, while Low Credibility Organization and Expert Citation were more predictive in FA-KES. Signal accuracy was linked to dataset properties and the LLMs’ ability to use pre-training knowledge.","The source states that hallucinations (generating inaccurate yet convincing answers) in instruction-tuned LLMs pose limitations on their use in directly assessing veracity, as it impacts the reliability, transparency, and predictive performance of such approaches. To mitigate these effects, their proposed multi-stage approach is designed to reduce LLMs' susceptibility to hallucinations. Instead of directly asking the LLM for veracity, they ask for individual credibility signals, which is a simpler task. Additionally, the weak supervision framework aggregates these signal predictions using weighted aggregation, making the final veracity prediction less sensitive to hallucinations in the intermediate signal predictions. They also suggest that LLMs abstaining from answering when appropriate, combined with PWS, can be valuable, as it avoids forcing potentially hallucinated erroneous answers.","Prompt Design: The source uses a two-step prompting technique for credibility signals and zero-shot labels. They found that their tailored instruction and signal prompts, potentially combined with a category mapping prompt for ambiguous answers, are effective in getting LLMs to produce weak labels. They contrast their nuanced prompts requiring reasoning and information retrieval with simpler string matching prompts used in prior work. Open-ended prompts initially guide the model, followed by restrictive prompts for categorization.
◦
Fine-tuning Architectures: The study utilizes instruction-tuned LLMs. Instruction tuning, often involving fine-tuning on instructions with human feedback (RLHF), enables these models to follow instructions for tasks like extracting credibility signals and veracity prediction. They also use a fine-tuned RoBERTa-Base classifier in one variant of their PWS approach. The comparison between supervised fine-tuning (RoBERTa on GT labels) and prompted methods (zero-shot LLM, PWS) shows that PWS can outperform traditional fine-tuning without GT labels.
◦
Domain-Specific Training: The LLMs used are instruction-tuned, implying training on a broad range of tasks and data. While the datasets are domain-specific (Syrian war, pro-Kremlin propaganda), their PWS method does not require fine-tuning the LLMs or a text classifier on domain-specific ground-truth veracity labels. LLMs leverage external knowledge acquired during their pre-training for signals like Source Credibility, which relates to information outside the article text itself. The varying performance of individual signals across the domain-specific datasets highlights that domain characteristics influence signal effectiveness.",,,,
16,Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models,https://arxiv.org/pdf/2305.14623,Conference,NAACL,A,2024,June,USA,"Automated Fact-Checking, Large Language Models (LLMs), Plug-and-Play Frameworks","BINGCHECK, FEVER, WiCE","SELF-CHECKER, on OpenAI GPT-3 with standard prompting, chain-of-thought prompting, and ReAct (also using GPT-3.5).","1. A tendency in SELF-CHECKER to be overly optimistic, classifying claims that are only partially supported as fully supported.
2. Inability to update information
3. Does not contain a module to postprocess and filter the retrieved articles
4. High computational cost due to the involvement of multiple chained LLM calls in the process of fact-checking
5. Sensitivity of SELF-CHECKER to prompts
6. Selection of hyperparameters in SELF-CHECKER currently relies on heuristics.
7. A potential limitation of the BINGCHECK dataset is the potential bias during annotation.",INPUT → EXTRACT-CLAIMS → PREDICT-SEARCH-QUERIES → RETRIEVE-DOCUMENTS → SELECT-EVIDENCE → VERDICT → OUTPUT,"Yes, the Query Generator retrieves information from external knowledge sources (Bing for BINGCHECK, Wikipedia for FEVER), and the Evidence Seeker utilizes this retrieved information to assess the factuality of the generated claims","Accuracy, F1 score, FEVER score","1. Introduced SELF-CHECKER to utilize LLMs for automatic fact-checking.
2. Constructed BINGCHECK dataset
3. Evaluate the effectiveness of SELFCHECKER on BINGCHECK
4. Appreciatable performance when it does not require any fine-tuning and can be applied to any off-the-shelf LLM",,"The source states that the advent of LLMs has intensified the importance of fact-checking because LLMs carry the risk of generating false information and hallucinating facts. This directly implies that hallucinations negatively affect the reliability of LLM outputs, necessitating fact-checking.
◦
The strategy proposed in this paper to mitigate these effects is the SELF-CHECKER framework itself. This framework tackles hallucinations by systematically breaking down complex LLM-generated text into simple claims, retrieving external evidence, and verifying these claims against that evidence. The creation of the BINGCHECK dataset is also presented as a way to alleviate the problem of hallucinations in LLM generation by providing a resource to evaluate fact-checking in a realistic setting with LLM outputs.","The source focuses on a training-free approach using LLMs via prompting. It states that SELF-CHECKER's modules are implemented through carefully crafted prompts that include task instructions and in-context examples. It uses prompt-based baselines (Standard Prompt, Chain-of-Thought, ReAct) for comparison.
◦
Fine-tuning is mentioned as a traditional, resource-intensive approach used by state-of-the-art models. The source contrasts SELF-CHECKER's training-free nature with fine-tuned models and notes that SOTA fine-tuned models currently outperform SELF-CHECKER, suggesting that fine-tuning can lead to higher accuracy.
◦
The source notes that prompt design is important; sensitivity to prompts is listed as a limitation, and variations in performance were observed with different prompts. Enhancing robustness to prompts is suggested future work.
◦
The source does not explicitly discuss domain-specific training for LLMs within their framework, as it is training-free. However, the BINGCHECK dataset is domain-specific to LLM-generated content, and evaluating on different datasets (BINGCHECK, FEVER, WiCE) tests performance across different data characteristics.","the SELF-CHECKER framework clearly integrates a Retrieval-Augmented Generation (RAG) approach for its fact-checking process.

Retrieval:
The Query Generator module is responsible for predicting search queries for each claim that needs verification.
These queries are then used to retrieve relevant passages from an external knowledge source. For BINGCHECK, this is the Bing search engine; for FEVER, it's Wikipedia.

Augmentation:
The retrieved passages (up to three are considered) serve as the external knowledge that augments the LLM's internal capabilities.



Generation (or in this context, Verification/Prediction based on retrieved info):
The Evidence Seeker module processes these retrieved passages to identify and select specific evidence sentences relevant to the claim.
The Verdict Counselor module then takes these selected evidence sentences and the claim to predict a final veracity label (e.g., supported, refuted). This sequence of generating queries, retrieving external documents, selecting evidence from them, and then using this evidence to make a final determination aligns with the core principles of RAG systems, where generation (here, a veracity judgment and supporting evidence) is grounded in retrieved information.",The framework's modularity allows for potential improvements in each subtask,"The paper does not explicitly frame SELF-CHECKER's outputs as XAI or detailed natural language explanations for the end-user regarding why a claim is true or false. However, the framework's modular and step-by-step nature inherently offers a degree of transparency and interpretability into the fact-checking process:

Modular Breakdown: The process is divided into clear stages: claim processing, query generation, evidence seeking, and verdict counseling. The output or decision of each module can be inspected (as illustrated in Figure 1).
Evidence Linkage: The Evidence Seeker module identifies specific sentences from retrieved documents that support or refute claims. The Verdict Counselor then makes a prediction based on this collected evidence. This linkage to evidence provides a basis for the verdict.


The focus is more on the process of verification and arriving at a veracity label rather than generating a human-consumable narrative explanation for that label. The ""Conclusion"" in Figure 1 shows a structured output linking the claim to supporting sentences.",
17,Factuality Challenges in the Era of Large Language Models,,Journal,Nature,Q1,2024,August,Unknown,"Factuality of Large Language Models (LLMs), Hallucinations, Misinformation, Trustworthiness, Evaluation of LLMs.","TruthfulQA, BIG-bench, GLUE, SuperGLUE",Survey paper,,,,,,,"Hallucinations, defined as generating false, erroneous, or misleading content, significantly affect fact-checking reliability by:
▪
Making it difficult to discern fact from fiction in LLM-generated content.
▪
Leading to unreliable information being generated.
▪
Potentially causing users to act upon, share, and/or quote inaccurate responses.
▪
Making LLMs struggle with factual accuracy despite fluency.
▪
Creating persuasive narratives that deceive readers due to the confident tone.
▪
Burying misinformation among true and false claims in private chatbot interactions, making detection harder.
◦
Strategies proposed to mitigate hallucination effects include:
▪
Retrieval-Augmented Generation (RAG): Incorporating contextual information from external sources to enhance capabilities with external data.
▪
Hallucination Control during inference: Methods like self-consistency checking, cross-model verification, or checking against related knowledge.
▪
Knowledge Editing: Localizing and fixing factual errors by injecting factual updates into the model.
▪
Modularized Knowledge Grounded Framework: A multi-step framework for gathering and organizing real-time event information to create factually accurate content, which can then be refined by an LLM.
▪
Implementing robust guardrails in LLM-based conversations.
▪
Equipping chatbots with evidence-supporting capabilities.
▪
Using external knowledge sources like knowledge graphs alongside RAG to ensure factual consistency.
▪
Improving evaluation methods for factuality.
▪
Focusing on Alignment and Safety measures during LLM development, including data cleansing and safety instruction-tuning.","Instruction-tuning is a form of fine-tuning mentioned for adapting base models into conversational agents (e.g., LLaMA to Alpaca/Vicuna), implying it shapes the model's behavior and response style.
▪
Learned prompts are mentioned as a basis for safety checks, but these can be easily circumvented.
▪
Prompt engineering can be exploited for malicious purposes like crafting personalized disinformation.
▪
Customizing factuality instructions for specific domains (e.g., medicine, law) is suggested as a way to improve factuality assessment.
▪
Domain-specific chat support can utilize LLMs with a controlled corpus of verified articles.","◦
The impact of integrating RAG is presented as a mitigation strategy for hallucinations and a way to enhance LLM capabilities with external data. By incorporating contextual information from external sources, RAG aims to improve the factuality of generated text. It can also help ensure factual consistency by checking against external knowledge sources.
◦
Challenges associated with implementing RAG mentioned in the source include:
▪
Requirement for efficient retrieval of grounded text at scale.
▪
Need for robust evaluation mechanisms",,,
18,Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,https://arxiv.org/pdf/2302.12813,Preprints,Arxiv,Preprint,2023,March,USA,"Large Language Models (LLMs), Fact-Checking, Task-Oriented Dialogue, Open-Domain Question Answering.","News Chat (DSTC7), Customer Service (DSTC11), OTT-QA",LLM-AUGMENTER,Interactive feedback with a computationally expensive model such as ChatGPT can significantly slow down the user experience.,"User query - Retrieve evidence from external knowledge - consolidate evidence - Reasoning - Query ChatGPT - Candidate response with evidence - Check hallucination - Generate feedback message - Use that message to revise prompt to ChatGPT - Till candidate response passes verification - If passes, sent to user","Yes, retrieves evidence from external knowledge.","Knowledge F1 (KF1), BLEU-4, ROUGE-1, METEOR, BLEURT, BERTScore, chrF, BARTScore","1. Validate the effectiveness of their work
2. Reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness.
3. Improves the factuality score of the answers using external knowledge and automated feedback","Dialogue task of customer service improves by:
Usefulness - 32.3%
Humanness - 12.9%
Factuality score of the answers improves by:
F1 - 10%","Mitigation Strategies in LLM-AUGMENTER: The LLM-AUGMENTER framework is designed specifically to mitigate hallucinations and improve the factual grounding of a black-box LLM like ChatGPT. It employs two main strategies:
Grounding with External Knowledge: The Knowledge Consolidator module retrieves relevant information from external sources (web, databases, Wikipedia). This evidence is then passed to the LLM via the prompt, forcing the LLM to base its responses on this provided external information rather than relying solely on its internal, potentially flawed or outdated, parametric knowledge. This is a core RAG principle.

Iterative Revision with Automated Feedback: The Utility module assesses the LLM's candidate responses for qualities like factuality (e.g., using KF1 score). If a response is found to be inconsistent with the evidence or otherwise unsatisfactory, the Utility module generates textual feedback. This feedback is incorporated into a revised prompt, and the LLM is asked to ""try again."" This iterative process allows the system to refine responses and correct factual errors or inconsistencies, thereby reducing hallucinations in the final output.
Recent Innovations: The LLM-AUGMENTER framework itself, with its modular design allowing plug-and-play components and its iterative feedback loop driven by automated utility assessment, represents an innovative approach to enhancing LLM factuality. The use of ChatGPT for self-criticism to generate feedback for its own responses is also a notable technique explored within the Utility module.","The sources state that prompts are task-specific and incorporate task instruction, user query, dialog history, evidence, and feedback. Incorporating consolidated evidence and feedback into prompts is a core part of the methodology to improve accuracy (groundedness/factuality). Figure 1 and Tables 7, 8, and 9 show examples of prompt templates. The paper focuses on augmenting fixed (black-box) LLMs like ChatGPT and avoiding finetuning their parameters, as it is prohibitively expensive. Instead, they propose using PnP modules and a potentially trainable policy. The sources mention that the datasets used are for specific domains (news, customer service, Wikipedia QA) and require task-specific knowledge. Providing LLMs with task-specific knowledge can significantly mitigate hallucination. The sources do not discuss the interpretability of the LLMs or the system.","The sources state that prompts are task-specific and incorporate task instruction, user query, dialog history, evidence, and feedback. Incorporating consolidated evidence and feedback into prompts is a core part of the methodology to improve accuracy (groundedness/factuality). Figure 1 and Tables 7, 8, and 9 show examples of prompt templates. The paper focuses on augmenting fixed (black-box) LLMs like ChatGPT and avoiding finetuning their parameters, as it is prohibitively expensive. Instead, they propose using PnP modules and a potentially trainable policy. The sources mention that the datasets used are for specific domains (news, customer service, Wikipedia QA) and require task-specific knowledge. Providing LLMs with task-specific knowledge can significantly mitigate hallucination. The sources do not discuss the interpretability of the LLMs or the system.",Can be applied to other LLMs apart from their implementation on ChatGPT,"The LLM-AUGMENTER framework incorporates a form of explainability primarily through its automated feedback mechanism:

When the Utility module identifies issues with an LLM's candidate response (e.g., it's inconsistent with provided knowledge), it generates textual feedback. This feedback explicitly points out the detected problem. For example, ""The player Jaime Penedo is transferred in from C.S.D. Municipal, but there is no information about the number of international titles of this team"" (Figure 1)",pengbaolin/LLM-Augmenter
19,LLM-based Agent for Recommending Information Related to Web Discussions at Appropriate Timing,https://ieeexplore.ieee.org/abstract/document/10807435,,ICA,B,2024,December,Japan,"LLM-based agents, information recommendation",,GPT-4 Turbo,"The agent was developed to address issues in web discussions where participants may have limited information or understanding, which can restrict their contributions. The goal is to reduce the knowledge gap among discussion participants, making it easier for them to participate","Timing Estimation: The agent uses GPT-4 Turbo to analyze posts, extract information requests, and estimate 'information necessity' and 'searchability' (0–100 scale) to decide when to recommend information.

Information Scraping: Uses Google's Custom Search API to retrieve top search results.

Fact-checking:

Websites: GPT-4 Turbo evaluates website reliability (0–100) based on misinformation, prediction validity, and meta information.

LLM Outputs: Assesses consistency with reference sites and checks assumptions or predictions, assigning a validity score.

Opinion Generation: The LLM generates a response based on verified information. Replies are posted if reliability scores exceed a threshold (with some exceptions during experiments).

Experiments: Compared Agent A (proposed method) with Agent B (fixed-interval recommendations) in participant web discussions",No,,"The developed agent effectively recommends information at appropriate times and facilitates user participation in discussions. Its fact-checking mechanism improves both the relevance and reliability of the information and LLM-generated outputs, with fact-checking results generally accurate in preliminary tests. In experiments, the proposed agent (Agent A) outperformed the conventional agent (Agent B) in timing, frequency, and ease of contribution, and also showed higher relevance and utility compared to a prior study agent, with one group showing statistically significant improvement in utility. However, the agent was less effective when participants lacked background knowledge, as the recommended information was harder to understand, which hindered discussion. This highlights the importance of providing accessible information for all users. Additionally, using GPT-4 Turbo for generating search terms improved relevance compared to GPT-3. Decreased utility in some preliminary cases was linked to simple topics and unreliable opinion posts.","The results of the experiments are presented in tables:
•
Preliminary Experiment (Table II): Average scores for fact-checking validity (5.2), relevance (5.8), and utility (4.4). Variance values were also reported.
•
Comparison with Prior Study (Preliminary): Relevance averaged 5.8 (our method) vs 5.4 (prior). Utility averaged 4.4 (our method) vs 5.1 (prior).
•
Main Experiment - Timing/Participation (Table III): Agent A averaged 4.8 for Q1 (frequency), 4.8 for Q2 (desired moments), and 5.4 for Q3 (easier contribution). Agent B averaged 3.9 for Q1, 3.4 for Q2, and 5.0 for Q3.
•
Main Experiment - Relevance/Utility per Group (Table IV): For Question 5 (Relevance), Group 1 averaged 6.4, Group 2 averaged 4.6, and the Prior Study averaged 5.4. For Question 6 (Utility), Group 1 averaged 6.4, Group 2 averaged 5.4, and the Prior Study averaged 5.1.
•
Mann-Whitney U Test Results (Table V): The p-values for comparison with the prior study agent were 0.071 (Q5 Group 1), 0.296 (Q5 Group 2), 0.030 (Q6 Group 1), and 0.747 (Q6 Group 2). Question 6 for Group 1 showed a statistically significant difference (p < 0.05)","Based on the source: The source explicitly states that LLMs have the potential for hallucinations, which are outputs generated without relying on scientific knowledge or factual data, and that this poses a significant issue for the agent’s reliability. The primary strategy proposed and implemented in this work to mitigate these effects is fact-checking. Specifically, they perform fact-checking on the outputs made by the LLM to enhance reliability, noting that hallucinations can occur even from reliable websites due to the LLM's pre-existing knowledge. This process includes checking for discrepancies with reference sites and validating assumptions. Opinions are ideally generated repeatedly until a high reliability threshold is met.","Based on the source: The source mentions that prompts were used to generate search terms and opinions and that these prompts were modified after the preliminary experiment to incorporate information about participant needs. This suggests prompt design was considered relevant to the agent's output. However, the source does not discuss how prompt design, fine-tuning architectures, or domain-specific training of the LLM itself affect the accuracy or interpretability of fact-checking results. Their approach relies on a post-generation fact-checking process rather than modifying the core LLM training or architecture for fact-checking accuracy.",,,,
20,Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM Generation Output,https://arxiv.org/pdf/2411.01022,Conference,EMNLP 2024 Industry Track,A*,2024,November,USA,"Fact-checking of LLM-generated output, Retrieval-Augmented Generation (RAG), Natural Language Inference (NLI)","TRUE, MSMarco, TruthfulQA, HotpotQA, HaluEval, HaluBench","Provenance
Vectara","1. Lack of open-source datasets for long-answer benchmarking
2. It is hard to chunk generated answers",Crossencoder model - Determine relevance of context items - Select relevant context items - NLI model to evaluate the factual consistency,"Yes, it takes the retrieved context used by the generating LLM as a crucial input for its fact-checking process",AUC,Evaluation on a variety of open-source datasets shows our method to be effective for hallucination detection across a variety of tasks,"Provenance on summarization task - surpassed ChatGPT by 3.74%, and is
only 2.3% behind Claude2 on the QA task.","Hallucinations in LLMs are defined as nonfactual outputs or claims. They affect reliability by leading to misinformation and introducing errors. LLM-based fact-checkers themselves are noted as being prone to hallucinate.
◦
As a strategy to mitigate these effects, the Provenance method is presented as a way to detect nonfactual outputs/hallucinations. Provenance specifically enables downstream mitigation and correction of hallucinations by tracing them back to specific context chunks that did not support the generated output.
","The Provenance system itself relies on pre-trained NLI models and does not involve prompting an LLM to perform the core fact-checking logic. However, it does use a specific textual structure as input to its NLI-based Fact Checker:

Prompt Design for NLI Input (Claim Formulation):
The LLM-generated answer and the original user query are combined into a specific ""claim prompt"" format. The paper states: ""we insert the query and answer into a prompt that claims 'The answer to question &lt;QUERY> is &lt;ANSWER>.'"" This formatted string then serves as the ""claim"" or ""hypothesis"" for the NLI model.
This structured approach ensures that the NLI model evaluates the answer's factuality in the context of the original query.
Few-shot/Chain-of-Thought (CoT): These prompting techniques are not applicable to the core mechanism of Provenance, as it uses NLI models, not generative LLMs, for the final factuality assessment. The LLM whose output Provenance is checking might have used such techniques, but Provenance itself does not.
Prompting Strategies with Integrated External Retrieval: Provenance is designed to work with the output of an upstream RAG system. It takes the ""list of context items"" (retrieved by the RAG system) as input. The Relevancy Scorer within Provenance then further processes these context items with respect to the query to select the most relevant ""sources"" before they are passed to the NLI-based Fact Checker. So, while it doesn't perform the initial broad retrieval, it does refine and use externally retrieved context.
Fine-Tuning: The paper emphasizes that Provenance uses ""compact, open-source natural language inference (NLI) models"" and offers a solution with ""no need for LLM fine-tuning"". The NLI models it employs (mixedbread-ai/mxbai-rerank-base-v1 and vectara/hallucination_evaluation_model) are used as-is or as provided by their creators.
Domain-Specific Training/Model Adaptation: The Provenance framework is evaluated on a wide range of datasets covering various domains (dialogue, summarization, QA, general knowledge) to demonstrate its broad applicability. There is no mention of domain-specific training or adaptation of the core Provenance NLI models for particular knowledge areas within this work. Its adaptability comes from its light-weight nature and reliance on general NLI capabilities.","Provenance is explicitly designed as a fact-checker for the output of Retrieval-Augmented Generation (RAG) systems. It does not perform RAG itself but acts as a verification layer downstream of a RAG pipeline.


Input from RAG: Provenance expects as input the ""list of context items used by the generating LLM in the upstream RAG"" system, the user's original query, and the generated text (answer) from that RAG system.
Verification of RAG Output: The core function of Provenance is to determine if the generated text from the RAG system is factually consistent with the context items that the RAG system retrieved and presumably used for generation.
Mechanism:
It first uses a Relevancy Scorer to identify which of the provided context items (from the RAG's retrieval step) are most relevant to the original query.
Then, its Fact Checker (an NLI model) assesses whether the LLM's answer (framed as a claim including the query) is supported by these selected relevant context items.
Therefore, Provenance is tightly integrated into a RAG ecosystem by acting as a crucial validation module for the generated outputs, ensuring they are factually grounded in the retrieved knowledge. It enhances the reliability of RAG systems by flagging potential hallucinations.","The factuality scoring takes the query into
account when judging a generated answer against
the retrieved information sources.",,
21,A Large Language Model-based Fake News Detection Framework with RAG Fact-Checking,https://ieeexplore.ieee.org/abstract/document/10826000,,BigData,B,2024,December,USA,fake news detection,,"GossipCop++,PolitiFact++","The research addresses the urgent need for effective fake news detection, particularly in the context of the rise of AI-generated fake news, which is more pervasive and harder to control than traditional fake news","The system uses an LLM with task-specific prompts to extract key claims and entities from news articles in the form of triples. It then performs a two-level retrieval process: first, searching online for relevant information, and second, matching claims with semantically similar content from retrieved documents while logging retrieval behaviors such as support, contradiction, or credibility issues. These behaviors, especially the absence of evidence, help detect AI-generated fake news. Finally, the LLM integrates the article, claims, external knowledge, and retrieval logs to verify the content's truthfulness through reasoning and classifies the article as true or fake, with retraining on AI-generated content to improve performance.", Retrieval-Augmented Generation (RAG) is explicitly integrated into the FCRV framework. RAG enhances fact-checking by aligning claims with external sources and is used in the integrated retrieval approach,subclass-wise accuracy and overall accuracy. F1 score ,,,"The sources implicitly refer to hallucinations as the generation of claims or entities that lack a basis in reality, particularly prevalent in AI-generated news. This affects fact-checking reliability by making misinformation more pervasive and harder to control.
◦
A key strategy proposed to mitigate these effects is the full-context approach implemented in the FCRV framework, which integrates LLM-based claim extraction and RAG fact-checking. FCRV leverages the ""absence of evidence"" signal captured during the retrieval process as a crucial marker for detecting such misinformation. Fact-checking against external sources is also highlighted as an effective approach","◦
The sources mention using task-specific prompts to enable the LLM to identify and extract claims.
◦
The LLM used in FCRV undergoes reasoning-enhanced retraining on a dataset containing AI-generated news to improve its capability with synthetic content. Baseline transformer models are described as fine-tuned with full-context integration.
◦
The sources state that the LLM's reasoning process, integrating various factors and retrieval behaviors, enables it to classify articles and provide explanations","
Integrating RAG into LLM-based fact-checking systems (like FCRV) has the impact of providing external knowledge and enabling the system to align claims with external sources. This integration is shown to lead to more stable and robust fake news detection, enhancing scalability, accuracy, and the model's ability to handle AI-generated content. RAG facilitates capturing the crucial ""absence of evidence"" signal.
◦
The sources state that efficiently linking claims within news articles to relevant external facts remains a significant challenge, which their RAG-based retrieval workflow aims to address. However, the provided excerpts do not discuss challenges specifically associated with implementing RAG itself in specialized domains.",,,
22,Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?,https://arxiv.org/pdf/2411.05775,Conference,Socially Responsible Language Modelling Research (SoLaR) Workshop at NeurIPS 2024,A*,2024,November,Unknown,"Political Misinformation Detection, Large Language Models (LLMs) as Annotators, Natural Language Processing (NLP)",Curated politically diverse dataset of North American news articles (May 2023 to May 2024),"Llama-3-8B-Instruct, Llama-3.1-8B- Instruct, Mistral-7B-Instruct-v0.3, Gemma-2-9b-Instruct, and Phi-3-medium-128k-Instruct, GPT-4o-mini","1. LLMs own biases
2. Prompt adjustments can alter LLM judgments","Dataset construction - Annotate using opensource LLMs - Human Evaluation - Evaluate using LLMs of the annotations (binary fashion)  

Natural Language Inference (NLI) method",No,"No SOTA metrics. Two approaches:
1. Standard classification metrics (Acc, Prec, Rec, F1)
2. LLM-as-a-judge (referenced) Agreement Rate (AR)",1. Significantly improves the labeling process in NLP.,"◦
Annotations generated by LLMs closely match human annotations, evidenced by high reference-based scores and strong LLM-based evaluations.
◦
Including demonstrations (five-shot setting) significantly improves LLM annotation performance compared to the zero-shot setting across all models tested.
◦
Among the LLM annotators tested, Llama-3-8B-Instruct (5-shot) achieved the best performance against gold labels, with 89.3% Accuracy, Precision, and Recall, and 89.2% F1-score","The paper's main goal is to use LLMs to detect factual inaccuracies (misinformation) in news articles, rather than focusing on hallucinations produced by the LLM annotators themselves during the annotation task.

The introduction mentions political bias encompasses ""disinformation (false information spread intentionally to deceive) or misinformation (false information spread unknowingly)"" as the target for fact-checking.
The LLM annotators are tasked with determining if a given news article is ""Factually Correct"" or ""Factually Incorrect"" and providing an evidence-based explanation. The reliability of this process depends on the LLM annotator's ability to accurately assess the external text.
The paper does not explicitly discuss ""hallucinations"" made by the LLM annotators when they generate their labels or explanations. Instead, it measures their accuracy against human-verified gold labels  and uses LLM judges to assess the quality of these annotations. Discrepancies would indicate errors or unreliability in the LLM annotations, which could stem from various reasons including misinterpretation or lack of knowledge, not necessarily just hallucination in the narrow sense of fabricating non-existent facts within their explanation.

Mitigation of unreliable annotations (not explicitly ""hallucinations"" from the annotator):
Human Review: The process of creating ""gold data"" involves human reviewers assessing LLM-generated annotations (derived from majority voting of multiple LLMs) and resolving discrepancies. This is a direct way to ensure reliability.
LLM-as-a-Judge: Using other LLMs to evaluate the annotations is another layer of quality control.
Few-Shot Prompting: The finding that five-shot prompting improves performance over zero-shot suggests that providing examples helps guide the LLM annotators for more reliable output.◦
Strategies proposed within this framework to mitigate potential issues with LLMs in their roles as annotators and judges include using task-specific prompts with demonstrations (five-shot) which improves annotation performance, employing majority voting among multiple LLM annotators, using multiple LLM judges to get diverse perspectives and prevent bias, and incorporating human review/oversight to validate and ensure the quality and reliability of the LLM-generated annotations. Aggregating multiple LLM responses can help mitigate inherent stochasticity.","Prompt Design:
The study uses specific prompts for the LLM-as-an-Annotator task. The prompt (detailed in Section 3.2 and Appendix A.1) instructs the LLM to act as a ""helpful news fact-checking bot,"" assess the accuracy of a given article (""Factually Correct"" / ""Factually Incorrect""), and provide a ""concise, evidence-based explanation,"" referencing ""specific examples from the article and contradicting evidence from trusted sources, if applicable"".
This prompt was used in both zero-shot and five-shot settings (with five demonstrations/examples provided in the prompt for the five-shot case).
A different prompt was designed for the LLM-as-a-Judge task (Appendix A.2), asking the judge LLM to determine which annotation model (if comparing, or simply if the single annotator's output) better aligns with the given label and text, and to provide a brief justification.
Few-shot vs. Zero-shot Influence: The results (Table 1) clearly show that for all open-source LLM annotators tested, the five-shot setting consistently outperformed the zero-shot setting in terms of accuracy, precision, recall, and F1-score when compared against the gold labels. This directly answers how different prompt designs (specifically, the inclusion of demonstrations) influence accuracy.

Prompting Strategies with Integrated External Retrieval: The prompt for LLM annotators asks them to reference ""contradicting evidence from trusted sources, if applicable"". This implies that the LLMs are expected to utilize knowledge of trusted sources (likely from their pre-training). However, the methodology does not describe an explicit, integrated external retrieval mechanism that actively fetches and provides documents to the annotator LLM during the annotation task. The LLMs perform the annotation based on the provided article and their internal knowledge.

Fine-Tuning: No fine-tuning of any LLMs (either annotators or judges) is performed in this study. The framework leverages the capabilities of pre-trained open-source LLMs and the OpenAI API model through prompting.
Domain-Specific Training/Model Adaptation: The study focuses on the domain of political news in North America. The adaptation to this domain is achieved by curating a dataset from relevant news sources and topics and then prompting general-purpose LLMs to perform factuality annotation within this context. There is no domain-specific training or fine-tuning of the LLMs themselves.",,Integrating LLMs with human oversight enhances the reliability of the results,"The LLM annotators are specifically prompted to provide an ""Explanation"" for their classification (Factually Correct/Factually Incorrect). The prompt specifies: ""Provide a concise, evidence-based explanation for your classification. Reference specific examples from the article and contradicting evidence from trusted sources, if applicable"".
This generated explanation serves as a justification for the LLM's annotation decision, offering transparency into its reasoning process. Figure 1 illustrates that each LLM annotator provides both a label and an explanation.",
23,"OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and LLMs",https://aclanthology.org/2025.coling-main.755/,Conference,COLING,A,2025,January,Multiple,Factuality evaluation of LLMs and building customized automatic fact-checking systems.,"Snowball, SelfAware, FreshQA, FacTool-QA, FELM-WK, Factcheck-Bench, and FactScore-Bio.","LLaMA-2 (7B, 13B)
and GPT-4","The paper addresses the challenges of verifying the factual accuracy of LLM outputs and the difficulty in comparing different evaluation benchmarks and measurements used in the field. It aims to provide a unified framework to mitigate these issues, allowing for easy customization of fact-checkers, fair evaluation of LLM factuality, and assessment of fact-checking system reliability."," The paper proposes OpenFactCheck, a framework with three main modules: CUSTCHECKER (for building customized fact-checking pipelines with modular components like claim processors, retrievers, and verifiers), LLMEVAL (a unified LLM factuality evaluator using diverse benchmarks to generate reports on model strengths and weaknesses), and CHECKEREVAL (for evaluating the verification accuracy of automatic fact-checking systems and providing a leaderboard).","The CUSTCHECKER module explicitly supports the integration of Retrieval-Augmented Generation by allowing users to choose and configure different evidence retrieval components (e.g., using SerperAPI or Wikipedia with BM25) within their customized fact-checking pipelines. The LLMEVAL module also includes benchmarks that assess LLMs' ability to handle questions requiring world knowledge, implicitly touching upon the need for accessing and using relevant information.",LLMEVAL uses exact matching for questions with Yes/No or short answers and relies on automatic fact-checking systems to evaluate the factuality of free-form responses in other datasets.,"The evaluation of LLaMA-2 and GPT-4 on the selected factuality datasets revealed varying performance across different types of factual errors. The analysis of responses on FacTool-QA, FELM-WK, and Factcheck-Bench showed that the majority of claims were often verified as true by automatic systems, with FacTool-QA posing a greater challenge to the LLMs.","OpenFactCheck offers a unified and extensible framework for building, benchmarking, and evaluating both customized fact-checking systems and the factuality of LLMs. It enables researchers and practitioners to tailor fact-checkers to their specific needs and to compare the factuality of different LLMs and the performance of various fact-checking systems under the same evaluation criteria.","The paper extensively discusses hallucinations in LLMs and their impact on reliability, which is a core motivation for developing OpenFactCheck.

Hallucinations in Large Language Models:
LLMs like GPT-4 ""still produce content that deviates from real-world facts,"" a phenomenon that ""degrades the system performance and undermines its reliability"".
LLMEVAL categorizes factual errors LLMs are prone to, which include:
Type 1: Knowledge error: Model produces hallucinated/inaccurate information due to lacking knowledge, internalizing false knowledge, or problematic alignment. LLMs may ""confidently output unknown information"".
Type 2: Over-commitment error (Snowballing hallucination): Model fails to recognize falsehoods in the prompt or prior context and continues to build upon these false premises, even if it might have known better initially.
Type 3: Disability error: Model is unable to access up-to-date information for time-sensitive questions.
Impact on Fact-Checking Reliability: The presence of these errors makes direct LLM outputs unreliable for fact-sensitive applications. Evaluating and understanding these tendencies is crucial.
Mitigation Strategies / Innovations discussed:
External Knowledge Augmentation: For Type 3 errors, ""Retrieving external knowledge and augmenting it in the context would help"". This is a RAG approach.
Prompt Engineering: For Type 2 errors, ""engineering better prompts is helpful, such as explicitly instructing models to first detect false premises"".
Self-Correction/Awareness: For Type 1 errors, ""calibrating models to be aware of unknowns"" is mentioned. GPT-4 was noted to identify 87% of its own mistakes on the Snowball dataset when justifying prior generated content, suggesting a potential for self-correction if prompted appropriately.

OpenFactCheck as a tool:
CUSTCHECKER allows users to build systems to verify LLM outputs using external evidence.


LLMEVAL is designed to ""assess LLM's factuality ability from different aspects and then produces a report to illustrate the weakness and offer improvement advice,"" which can guide efforts to improve LLM factuality.

OpenFactCheck aims to provide tools to detect and evaluate these factuality issues in LLMs, rather than proposing a new method to directly prevent hallucinations within the LLMs themselves during their generation process.

","he OpenFactCheck framework focuses on providing a structure for building and evaluating fact-checking systems and LLM factuality, rather than proposing new LLM fine-tuning methods or specific detailed prompt engineering techniques for performing fact-checking directly within OpenFactCheck's LLMs.

Prompt Design:
The paper mentions that the verification accuracy of an LLM-based verifier (a component a user might select in CUSTCHECKER, or a system evaluated by CHECKEREVAL) ""primarily relies on the capabilities of the LLM and the effectiveness of the prompts used"".
It notes that for the FactScore system, claims are extracted by prompting LLMs to ""decompose to atomic claims"" (GPT3 was used in the original FactScore paper).

Figure 7 shows pseudo code for a verifier in CUSTCHECKER that could call_LLM(claim, evidence, prompt=""based on the evidence and your own knowledge, determine whether the claim is true or false.""). This implies that users of CUSTCHECKER can define such prompts for their chosen LLM verifier.
For LLMEVAL, when evaluating FreshQA, it uses ""few-shot in-context learning based on GPT-4"" for the FreshEval metric application.
The paper doesn't introduce novel prompting strategies (like specific zero-shot, few-shot, or CoT designs) as a core contribution but acknowledges their role in the systems it analyzes or facilitates.
Prompting Strategies with Integrated External Retrieval:
CUSTCHECKER explicitly supports building pipelines with a retriever module, which can use tools like SerperAPI or BM25 on Wikipedia to fetch external evidence before the verifier module assesses the claim. This is a direct integration of external retrieval to inform the verification step.
Many of the fact-checking systems evaluated by CHECKEREVAL (e.g., FacTool, Factcheck-GPT, Perplexity.ai) inherently use external web retrieval as a key part of their process.

Fine-Tuning Architectures/Performance-Oriented Training:
OpenFactCheck itself does not involve fine-tuning the LLMs it uses for evaluation (e.g., GPT-4, LLaMA-2 in LLMEVAL) or the LLMs that might be part of fact-checkers built/evaluated by its modules. It evaluates models as they are.
It aims to provide a framework to benchmark such models and systems, which might themselves have been fine-tuned by their developers.
Model Adaptation in Specialized Knowledge Areas:
CUSTCHECKER allows users to ""tailor their checkers according to their specific needs, such as domain specialization""  by choosing or implementing appropriate claim processors, retrievers (potentially pointing to domain-specific databases), and verifiers.
LLMEVAL's FactQA dataset is designed to cover a spectrum of domains, and the evaluation report can highlight LLM weaknesses in specific domains, implicitly guiding adaptation needs.","OpenFactCheck is highly relevant to and supports the integration of Retrieval-Augmented principles in fact-checking.

CUSTCHECKER: This module directly enables users to build fact-checking systems that incorporate retrieval. The configurable pipeline includes a retriever step, where users can select different implementations to fetch evidence from external knowledge sources (e.g., web search via SerperAPI, or specific databases like Wikipedia using BM25). This retrieved evidence is then used by the verifier module (often an LLM) to assess the claim's factuality. This is a direct application of RAG principles in a fact-checking context.
Evaluation of RAG Systems: The CHECKEREVAL module is used to benchmark various existing automatic fact-checking systems (e.g., RARR, FacTool, Factcheck-GPT, Perplexity.ai). Many of these systems are inherently RAG-based, as they retrieve external information to verify claims. OpenFactCheck thus provides a platform to compare the efficacy of different RAG approaches to fact-checking.
Mitigating LLM Weaknesses: The paper acknowledges that for LLMs' ""Type3: Disability error"" (inability to access up-to-date information), ""Retrieving external knowledge and augmenting it in the context would help"", advocating for RAG as a solution.",The three modules of OpenFactCheck are designed to be mutually reinforcing. The framework is intended to facilitate future research by providing a common platform and encouraging the development of effective fact-checking components and challenging evaluation datasets. The paper acknowledges limitations related to the quality and coverage of existing evaluation datasets and the dependence of fact-checking modules on external knowledge sources.,"The CUSTCHECKER module allows users to observe the intermediate processing outcomes of their customized fact-checking pipelines. The LLMEVAL module generates reports that analyze the factuality of LLMs from multiple aspects, including the types of factual errors they frequently make. The framework plans to incorporate the evaluation of intermediate steps in fact-checking pipelines to better understand where errors occur.",https://github.com/yuxiaw/openfactcheck
24,Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM,https://arxiv.org/abs/2404.17283,Conference,COLING 2024,A,2024,May,Singapore,Fact-checking news claims,"RAWFC (based on Snopes, with True/False/Half labels) and LIAR-RAW (based on PolitiFact, with six labels).",GPT-3.5 (text-davinci-003),"The paper addresses the challenge of efficient and accurate evidence retrieval to support LLM-based fact-checking of news claims. It argues that traditional top-K retrieval methods might not be optimal as they can include irrelevant information or miss crucial evidence, especially for complex real-world claims. The goal is to enable a more flexible exploration of potential evidence and prioritize documents that offer diverse perspectives.","The proposed FFRR method employs a reinforcement learning (RL) framework to optimize the retrieval policy. It first prompts the LLM to decompose the claim into intermediate questions. Then, FFRR gathers fine-grained feedback from the LLM on retrieved documents as rewards at two levels: document-level (assessing the usefulness of individual documents for verifying the claim) and question-level (promoting the retrieval of alternative evidence that different questions might surface). These rewards are combined into a hybrid reward to optimize the retrieval policy model using the REINFORCE algorithm. An ϵ-greedy strategy is used to balance exploration and exploitation during document selection.","This paper heavily focuses on the retrieval (""Retrieve"") component of RAG. The methodology aims to optimize the retrieval process by using intermediate questions generated by the LLM and leveraging fine-grained feedback from the LLM on the relevance and usefulness of the retrieved documents. The optimized retrieval is intended to provide better evidence for the LLM to perform the fact-checking (""Generate"") step.","Precision, Recall, and F1-score",The experimental results showed that FFRR consistently and significantly outperformed state-of-the-art LLM-enabled and non-LLM baselines on both the RAWFC and LIAR-RAW datasets. Tuning the dense retrieval model with FFRR led to substantial improvements compared to using a frozen retriever or the REPLUG method. Combining both document-level and question-level rewards in FFRR (d+q) yielded the best overall performance. Error analysis of the retrieval process indicated that document mismatch was a significant issue.,"The study demonstrates the effectiveness of using reinforcement learning with fine-grained LLM feedback to optimize evidence retrieval for black-box LLM-based fact-checking of news claims, leading to significant improvements in verification accuracy.","The paper acknowledges that LLMs ""may fall short in these aspects [up-to-date information and reliable knowledge], potentially affecting their downstream performance with bias and hallucination"".

Impact on Reliability: Hallucinations and biases in LLMs can lead to incorrect fact-checking verdicts, thereby reducing the reliability of systems that use them.
Mitigation Strategies in FFRR: FFRR aims to improve the reliability of fact-checking with a black-box LLM by optimizing the retrieval of external evidence. The core idea is that providing the LLM with high-quality, relevant evidence can help ground its reasoning and reduce its tendency to hallucinate or rely on biased internal knowledge.
The fine-grained feedback mechanism is crucial. Rewards are based on how well a retrieved document helps the LLM predict the ground-truth veracity label of the claim. This encourages the retriever to find documents that support correct, factual outcomes, implicitly steering the LLM away from responses that would rely on hallucination.
The paper argues that even if the LLM providing feedback is biased, the reward mechanism (tied to the ground-truth label) encourages the retriever to find stronger counter-evidence to overcome this bias.
Recent Innovations: The FFRR method itself, which uses reinforcement learning with fine-grained feedback from the task LLM to optimize a separate retrieval module for a black-box setting, is an innovative approach to improve the factuality of the overall system.","The methodology leverages prompt design in two key ways: (1) to guide the LLM in generating intermediate questions for claim decomposition using few-shot prompting, and (2) to have the LLM rate retrieved documents to produce reward signals for training. The system fine-tunes a dense retrieval model (not the LLM itself) using reinforcement learning with feedback from GPT-3.5, treated as a black-box. This approach, called FFRR, improves retrieval quality. The model is trained on domain-specific datasets (RAWFC, LIAR-RAW) involving complex, real-world news claims, which demand more diverse and nuanced document retrieval than simpler datasets like FEVER.","Impact of RAG: Integrating retrieval (RAG) enhances LLM-based fact-checking by providing external knowledge, improving factual accuracy—especially when LLMs lack up-to-date or reliable information. Methods combining LLM reasoning with retrieval (e.g., ReAct, Verify-and-Edit, and FFRR-frozen) outperform those without retrieval.

Challenges in Specialized Domains (e.g., News Fact-Checking):

Retrieved content quality directly affects LLM performance.

Tuning retrievers to locate precise evidence (""smoking gun"") is difficult.

Black-box LLMs prevent end-to-end optimization via backpropagation.

Relying on top-K documents risks including noise or missing key evidence.

Complex news claims need richer, more diverse evidence than simpler domains.

Common issues include ""document mismatch"" and insufficient coverage from generated questions.

RL approaches like FFRR face efficiency issues due to frequent LLM interactions.

Ethical concerns arise from low system accuracy, as mislabeling can cause societal harm.",The research highlights the benefit of using claim decomposition and question generation to explore different facets of a claim for more comprehensive evidence retrieval. The authors acknowledge that the method's performance is heavily reliant on the LLM's ability to assess the usefulness of documents and make accurate predictions.,"The paper does not explicitly focus on generating human-readable explanations for the final fact-checking verdicts as a primary XAI contribution. However, aspects of the methodology contribute to a degree of process transparency and potential for explanation:

Claim Decomposition: The use of LLM-generated intermediate questions to break down a complex claim into smaller, verifiable parts can make the reasoning process more transparent than a single end-to-end judgment.
Error Analysis: The paper includes an error analysis of the retrieval process (Table 3), categorizing failures into ""Irrelevant Questions,"" ""Insufficient Coverage,"" ""Redundant Questions,"" and ""Document Mismatch."" This type of analysis provides insights into the system's failure modes, which is a component of explainability.
Fine-grained Feedback: While the feedback is primarily numerical scores used as RL rewards, it is derived from the LLM's assessment of how well a document supports the ground-truth label. This provides an implicit signal about what makes evidence useful or not for the LLM's correct reasoning.
The ""explainable rationales"" mentioned in the introduction refer to the capabilities of LLMs in general, rather than a specific output of FFRR itself.",https://github.com/jadeCurl/FFRR
25,The perils and promises of fact-checking with large language models,https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1341697/full,Journal,Frontiers in Artificial Intelligence,Q2,2024,February,Switzerland,"Evidence retrieval, Claim verification","PolitiFact,multilingual dataset from Data Commons",GPT-3.5 and GPT-4,"The paper investigates the ability of LLMs to perform fact-checking with and without access to external contextual information. It also examines a methodology for integrating information retrieval with claim verification to automate the fact-checking process. Additionally, the study provides an initial assessment of GPT-3.5's capability to fact-check across multiple languages.","The researchers employed an iterative, agent-based reasoning approach using the ReAct framework. They equipped the LLM agents with the ability to query Google to retrieve relevant information (up to 10 search results per iteration, with a maximum of three iterations) based on the claim. To simulate a realistic scenario, search results from fact-checking websites present in the dataset were removed. If the agent couldn't conclude after three iterations, it was prompted to give a final answer based on all retrieved information. The LangChain library was used to implement this agent-based approach.","Retrieval-Augmented Generation is a central aspect of the methodology used in this study. The LLM agents were explicitly designed to perform web searches (retrieve information) to gain external context, which was then used to assess the veracity of the claim (generate a verdict). The agent's decision-making process involved determining whether to continue searching or to provide a final answer based on the retrieved information.","The primary metric used to evaluate the models' performance was accuracy, which was assessed by comparing the LLM agent's predicted verdict (based on the standardized 4-level scale) with the ground truth verdict in the Data Commons dataset. The paper also considered the verifiability and explainability of the model's verdict, as the agent was designed to justify its reasoning and cite the relevant retrieved data."," The study found that incorporating contextual information through web searches significantly improved the accuracy of both GPT-3.5 and GPT-4 in fact-checking. The LLM agents were able to justify their reasoning by citing the relevant data retrieved from their Google searches, enhancing the verifiability of their conclusions. The initial assessment indicated that GPT-3.5 has the capability to perform fact-checking across multiple languages present in the dataset.","The paper highlights the critical role of external context in improving the fact-checking abilities of LLMs and proposes an effective methodology that integrates information retrieval and claim verification using an iterative agent-based approach. The findings suggest that LLMs have significant potential as tools to assist human fact-checkers and for content moderation, especially when augmented with the ability to access and process external information.","While the term ""hallucination"" is mentioned in the broader context of LLM capabilities (e.g., ""explosion in falsified information""), the paper primarily evaluates the accuracy of LLMs in fact-checking existing claims, rather than their tendency to generate unsupported new information (hallucinate) within their fact-checking responses.


Impact on Reliability: The reliability concern addressed is whether the LLM can correctly determine the veracity of an external claim. If an LLM incorrectly labels a true claim as false, or vice-versa, its reliability as a fact-checker is compromised. The varying accuracy across different claim types and conditions demonstrates this.
Mitigation Strategies for Improving Factuality/Accuracy:
The core strategy investigated is Retrieval Augmentation: Providing the LLM with external context retrieved via Google Search. The results consistently show that access to this external context improves the LLM's fact-checking accuracy, thereby enhancing its reliability. This suggests that grounding LLMs in external, verifiable information is crucial.
Data Leakage Concern: The paper acknowledges that if fact-checks are part of the LLM's training data, successful verification might reflect ""retention of training examples"" rather than true reasoning. However, they found no sudden decrease in accuracy after the training cutoff dates, suggesting that continued learning (e.g., RLHF) or the ability to reason with new context (in the RAG condition) plays a role.
The study's focus is on whether LLMs accurately verify external claims with or without external information, rather than on preventing the LLMs from hallucinating novel statements within their reasoning or justifications. The RAG approach helps ensure the justification is based on something verifiable.

",The study shows that prompting with English translations of non-English claims significantly improved accuracy and F1 scores compared to using the original language prompts. This demonstrates the impact of prompt language. The study did not explore variations in prompt wording or instruction format beyond the inclusion or exclusion of context information,"Integrating retrieval-augmented generation (RAG) with contextual information significantly enhances model accuracy, improving the ability to distinguish true from false claims and leading to better-calibrated outputs. Empirical comparisons show consistently higher accuracy and F1 scores when context is included, across different models and languages. However, implementing RAG presents several challenges. It assumes the trustworthiness and completeness of available information, which is often unrealistic. Providing large volumes of data, such as full HTML content, can exceed the LLM's context window and degrade performance, necessitating effective information distillation. To ensure the model reasons independently, results from known fact-checking sources must be filtered out. Additionally, RAG requires an iterative retrieval process with a defined stopping criterion to balance accuracy and efficiency. Variability in the availability and quality of information—especially for less-covered or non-English topics—further complicates implementation, as reflected in performance differences across languages.","The paper emphasizes the importance of further investigating the conditions under which LLMs perform well or poorly in fact-checking, especially as they are increasingly used in high-stakes domains. The ability of the LLM agents to justify their conclusions is noted as a key advantage of their methodology.","Explainability is a core feature of the methodology. The LLM agents, using the ReAct framework, explicitly decide on actions (like web searches) and observations, and are prompted to justify their reasoning and cite the sources they used to arrive at a verdict. This provides a trace of the model's reasoning process and the evidence it relied upon.",
26,Multilingual Fact-Checking using LLMs,https://aclanthology.org/2024.nlp4pi-1.2.pdf,Conference,Proceedings of the Third Workshop on NLP for Positive Impact,Unranked,2024,November,Unknown,Multilingual fact-checking capabilities of LLMs,"X-fact dataset: Spanish, Italian, Portuguese, Turkish, and Tamil","Llama 3 8B, Llama 3 70B, GPT-3.5-turbo2,  GPT-4o, Claude 3 Haiku","1. Investigates whether LLMs possess multilingual factual knowledge and can effectively use it for fact-checking. 
2. It notes that prior research has largely focused on English and Chinese, neglecting other languages.","The researchers employed three different prompting techniques: Zero-Shot, English Chain-of-Thought, and Cross-Lingual Prompting. They also used two decoding strategies: greedy decoding and self-consistency decoding. The LLM's responses were often forced to be in English using the instruction ""Answer in English"" for zero-shot prompting.",No,Accuracy,"1. LLMs are better at fact-checking from knowledge in low-resource languages.
2. Chain-of-Thought and Cross-Lingual Prompting, which are designed to improve reasoning abilities, do not necessarily improve the fact-checking abilities of LLMs.","1. GPT-4o achieves the highest accuracy, but zero-shot prompting with self-consistency was the most effective overall","An incorrect prediction (e.g., labeling a true claim as false) is a factual error, which could stem from various reasons including gaps in knowledge, misinterpretation, or what might be termed a hallucination if the LLM internally ""believes"" or generates a false premise to reach its conclusion.

Impact on Reliability: The varying accuracy scores and the number of inconclusive responses directly reflect the reliability of these LLMs for fact-checking. Lower accuracy or a high number of inconclusive responses means lower reliability.
Mitigation Strategies/Improving Factuality: The paper investigates different prompting techniques (Zero-Shot, EN-CoT, CLP) and a decoding strategy (Self-Consistency) as ways to potentially improve the factual accuracy of LLM outputs in this task. 
It finds that Zero-Shot with Self-Consistency was generally the most effective.
Techniques designed to enhance reasoning (EN-CoT, CLP) did not consistently improve fact-checking accuracy, suggesting that better ""reasoning"" as elicited by these prompts does not equate to better factual recall or verification ability for this task.

Observation on Data and Factuality: The strong negative correlation between model accuracy and the amount of internet content for a language suggests that the nature (possibly higher quality or less contradiction in smaller datasets) of training data for low-resource languages might lead to better factual recall for those languages compared to high-resource languages where training data is abundant but potentially noisier. This is an indirect observation related to factors influencing factuality.
","The paper directly investigates the impact of prompt design (Zero-Shot, EN-CoT, CLP) and decoding strategies (Self-Consistency) on accuracy. It finds that prompting techniques have a statistically significant effect, but self-consistency decoding is the primary driver, while reasoning prompts like CoT and CLP often did not improve accuracy for fact-checking.
◦
The paper mentions the LLMs were pre-trained on multilingual corpora but does not explore or discuss the effects of fine-tuning architectures or domain-specific training within the scope of their experiments",,This study is highlighted as the first work to assess the factual multilingual knowledge and inherent fact-checking capabilities of various LLMs across a spectrum of languages using a variety of prompting techniques,"The paper touches upon explainability by employing prompting techniques designed to elicit reasoning:

English Chain-of-Thought (EN-CoT): This prompting method explicitly includes the instruction ""Let's reason step-by-step in English,"" aiming to make the LLM generate intermediate reasoning steps before providing the final true/false answer. These steps could serve as an explanation for the model's verdict.
Cross-Lingual Prompting (CLP): This method also involves a multi-step process where the first prompt is for the LLM to ""understand if the statement is true or false,"" and the second is a task-specific solver that uses this understanding. The LLM's output from the first step (understanding) could contribute to explainability.
While these methods can produce intermediate reasoning steps, the paper's primary evaluation metric is the accuracy of the final fact-checking label, not the quality or faithfulness of the generated explanations or reasoning steps themselves. The study finds that these reasoning-focused prompts (EN-CoT, CLP) do not necessarily improve fact-checking accuracy.


",
27,FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking,https://arxiv.org/pdf/2309.00240,Conference,Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) 2023,B,2023,November,China,Automated fact-checking,"RAWFC, LIAR
https://github.com/Nicozwy/CofCED",LLaMA-7B,1. Lack of up-to-date or sufficient knowledge can lead to inaccuracies in fact-checking.,Prompt - LLM and External Evidence - Match relevant evidence - LLM again - Result,No but use of external source,"Precision, Recall, Accuracy","1. Interestingly, LLaMA without tuning, i.e., zero-shot prediction, performs relatively poorly compared to the other methods.","LLaMA (w/o tuning): 0.3350, 0.3255, 0.2643
FactLLaMA (Instruct-tuning w/o external knowledge): 0.5376, 0.5400, 0.5376
FactLLaMA (Instruct-tuning with external knowledge): 0.5611, 0.5550, 0.5565","The paper addresses the issue that LLMs' ""knowledge may not always be up-to-date or sufficient, potentially leading to inaccuracies in fact-checking"". While not always using the term ""hallucination"" explicitly for this knowledge gap, this implicitly covers scenarios where an LLM might generate an incorrect fact-check due to lack of correct, current information (which can be a form of hallucination if it confidently asserts a falsehood).

Impact on Reliability: The reliance of LLMs solely on their internal knowledge raises concerns about their ability to accurately assess claim veracity, especially for evolving information. This directly impacts their reliability as fact-checkers.
Mitigation Strategies in FactLLaMA: The core strategy of FactLLaMA to improve reliability and address knowledge deficiencies is the integration of external evidence retrieval:
The model is instruct-tuned using training samples that include relevant evidence retrieved from Google API for each claim.
By learning to incorporate this external, up-to-date evidence, FactLLaMA is less reliant on its potentially incomplete or outdated internal knowledge, thereby improving the factual accuracy of its predictions.
Recent Innovations for Improving Factuality: The paper's innovation lies in combining instruction-tuning of an open-source LLM (LLaMA) with LORA for efficiency, specifically with inputs augmented by externally retrieved evidence, tailored for the fact-checking task.","The paper evaluates the effect of their specific instruct-tuning approach, which involves generating ""instruction-evidence-input samples"" as part of the prompt. They demonstrate that instruct-tuning significantly improves accuracy over zero-shot LLaMA.
◦
The paper employs LORA (Low-Rank Adaptation) as their fine-tuning architecture, chosen for its efficiency, but it does not specifically analyze how different fine-tuning architectures might affect accuracy or interpretability.
","The paper directly investigates the impact of integrating external evidence retrieval (similar to RAG). Their findings show that integrating external evidence retrieval significantly enhances the accuracy of instruct-tuned LLaMA for fact-checking, leading to state-of-the-art results on the evaluated datasets.",This excerpt serves as background information and a review of related work in the field of automated fact-checking,No,"https://tsunhincheung.com/factllama/

https://github.com/thcheung/FactLLaMA"
28,FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs,https://arxiv.org/pdf/2402.05904,Preprints,Arxiv,Preprint,2024,February,USA,Claim matching to augment the fact-checking,"COVID-19 related false claims that have been fact-checked.
Coronavirus Twitter Dataset","GPT-4, GPT-3.5-Turbo, Llama-2-13b, and Llama-2-7b","1. False claims tend to be reused and reiterated in different formats
2. Need to minimize redundant verification
3. Struggle of models categorizing posts that contradict debunked claims","Synthetic Training Data ( Generated Posts by GPT + Claims Debunked by Fact-checkers) ==> Smaller Fine-Tuned GPT ==> Identify and compare social media contents whether aligns, contradicts or irrelevant",No,"Precision, Recall, Accuracy","1. Appropriately fine-tuned, smaller LLMs can yield a performance comparable to larger models
2. The instruct-tuning approach, particularly when combined with external knowledge, consistently outperforms other methods, showcasing the value of leveraging external evidence for accurate fact-checking.","Offers an automated solution for efficient claim matching, demonstrating the potential of LLMs in supporting fact-checkers",,"Prompt design is used both for generating the synthetic training data and for formulating the textual entailment task for the models during evaluation. The study shows that fine-tuning the models using instruction-like formats (implied by the prompt-based task design and fine-tuning on synthetic data designed for this task) significantly improves accuracy compared to using pre-trained models.
◦
Fine-tuning architectures like LoRA are used for efficient tuning of LLaMa models. The study shows that fine-tuning improves accuracy, but it does not compare the effect of different fine-tuning architectures on accuracy or interpretability.
◦
The models are fine-tuned on a dataset specific to COVID-19 misinformation and the claim matching task. The results show that this focused fine-tuning leads to superior performance on this specific task and domain compared to pre-trained models.",,"1. Highlights the potential of LLMs in augmenting the fact-checking process, particularly in claim matching. 
2. It emphasizes the possibility of using more accessible and cost-effective AI solutions without compromising quality. 
3. Further refinement is needed in categorizing contradictory posts.",No,
29,"Multimodal Large Language Models to Support Real-World Fact-Checking
(Not published but exists in OpenReview)",https://arxiv.org/pdf/2403.03627,Preprints,arXiv,Preprint,2024,May,Unknown,Multimodal fact-checking using Multimodal Large Language Models (MLLMs),"Fauxtography, COSMOS, MOCHEG, Post-4V","GPT-4V, MiniGPT-v2, and LLaVA-1.5 (specifically the 7b and 13b versions)","The paper investigates the capability of MLLMs to identify multimodal misinformation, where false claims can involve manipulated or out-of-context visual elements. It aims to determine if MLLMs can perform accurate reasoning for such claims and how their fact-checking capabilities can be boosted without relying on external evidence retrieval. The study also seeks to understand the typical errors made by MLLMs in this context.","The MLLMs were evaluated using various prompts, including those designed to elicit predictions, explanations, and confidence levels. Prompt Ensembles (PE) and In-Context Learning (ICL) were used to improve the performance of open-source models. The responses were categorized into True, False, Uncertain, and Others. The study also explored the impact of the order of reasoning and prediction on model performance.","The paper hypothesizes that MLLMs trained on large amounts of data can serve as a sufficient substitute for the retrieval of evidence, performing fact-checking based solely on their parametric knowledge. However, they do leverage image captions obtained through reverse image search as additional context during evidence retrieval for certain examples in their analysis.","Overall Accuracy, True & False Accuracy","GPT-4V demonstrated high accuracy, provided useful explanations, and showed good calibration. LLaVA models showed improved performance with PE and ICL, with ICL having a greater impact. MiniGPT-v2 lacked explanatory and uncertainty-reporting capabilities. The study identified six primary categories of failure reasons for GPT-4V based on its explanations. Starting with reasoning first led to a significant increase in GPT-4V expressing uncertainty and a decline in overall accuracy.","The research indicates that state-of-the-art MLLMs, particularly GPT-4V, have the potential to assist professional fact-checkers by providing reference predictions, valuable clues, and explanatory insights. Open-source models like LLaVA fall behind but can be improved with techniques like PE and ICL.","t notes that open-source models like LLaVA may generate hallucinations when no image is present. For example, LLaVA without image input hallucinates facts when trying to verify claims.","The paper highlights the importance of prompt design in improving model performance, especially through Prompt Ensembles (PE) and In-Context Learning (ICL), which enhances explanation length and accuracy in models like LLaVA. Fine-tuning is not used due to a lack of annotated data and the need for interpretability, though domain-specific fine-tuning is suggested for future work. The study focuses on political claims from PolitiFact and finds that while CoTVP prompts did not outperform standard prompts in GPT-4 due to its strong reasoning, CoVe prompts did improve performance. Overall, the study emphasizes prompt sensitivity, interpretability through explanations, and the potential for future improvements via efficient fine-tuning and domain adaptation.",,The study provides a more comprehensive evaluation of multimodal models for fact-checking compared to previous work. It emphasizes the importance of handling multiple modalities in fact-checking due to the prevalence of manipulated or out-of-context visual content.,"A significant aspect of this work is the evaluation of the explanations provided by the MLLMs. The paper analyzes the step-by-step reasoning in the explanations and develops a taxonomy of failure reasons for GPT-4V, contributing to the explainability of these models in a fact-checking context.",
30,"RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models",https://aclanthology.org/2024.fever-1.29.pdf,Conference,ACL Anthology,A*,2024,June,Austria,Political fact-checking of multimodal claims using RAG-augmented reasoning with Multimodal Large Language Models (MLLMs).,MOCHEG,GPT-4V," The paper tackles the increasing issue of misinformation in political discourse, particularly when claims involve both text and images (multimodal claims). It investigates whether using elaborate reasoning techniques in conjunction with Retrieval-Augmented Generation (RAG) can improve the accuracy and explainability of multimodal fact-checking. The authors note that RAG has primarily been applied to textual fact-checking.","The researchers introduced two novel RAG-augmented reasoning techniques: Chain of RAG (CoRAG) and Tree of RAG (ToRAG). These methods involve multiple steps of question generation (by the LLM), multimodal evidence retrieval, and reasoning to determine the veracity of a claim. Multimodal Evidence Retrieval used DuckDuckGo Search for text-based evidence (with temporal restrictions and exclusion of fact-checking website results) and SerpAPI for extracting captions of images associated with the claims. CoRAG follows a sequential chain of question-answer pairs, while ToRAG employs a branching tree structure of questions with an elimination process to select the best evidence at each step. The pipeline also included Multimodal Claim Generation (incorporating both textual and visual elements) and Veracity Prediction and Explanation. Three variants of veracity prediction prompts (StandardVP, CoTVP, CoVe) were tested.","RAG is a fundamental aspect of both CoRAG and ToRAG. The LLM generates questions based on the claim, and external information (text snippets from the web and image captions) is retrieved to help answer these questions and provide evidence for fact-checking. The retrieved information then augments the LLM's reasoning process.","F1-score, Human Evaluation, Krippendorff’s α","The proposed RAGAR (RAG-Augmented Reasoning) techniques (CoRAG and ToRAG) achieved a weighted F1-score of 0.85, outperforming a baseline reasoning technique (SubQ+CoTVP) by 0.14 points. Human evaluation showed that the explanations generated by RAGAR methods consistently received higher ratings, with the vast majority containing all the information from the gold standard ""Ruling Outline"".","The study demonstrates that RAG-augmented reasoning techniques are effective for multimodal political fact-checking, leading to significant improvements in both the accuracy of veracity predictions and the quality of the generated fact-check explanations.","The paper mentions that RAG is used to combat hallucination in text generation by providing external data.
◦
It introduces the Chain of Verification (CoVe) approach, stating that CoVe first constructs verification questions based on an LLM-generated explanation, answers these using RAG, and then uses a correction check prompt to verify the original explanation, potentially correcting hallucination. Their results show CoVe improving performance, suggesting it successfully mitigates some issues.","The paper details the use of various prompts for different stages of their pipeline, such as generating the first question, follow-up checks, follow-up questions, QA elimination (specific to ToRAG), and veracity prediction (StandardVP, CoTVP, CoVe) , Their focus is on the political domain. They filter their dataset to include only political claims from PolitiFact.
◦
The paper discusses the effect of different veracity prediction prompts (StandardVP, CoTVP, CoVe) on performance. It also notes that applying CoTVP to the RAGAR question-answer pairs did not improve performance over StandardVP, attributing this to GPT-4's strong internal reasoning, but CoVe did help","The paper states that RAG allows LLMs to access up-to-date external information at inference time, helping combat hallucination.
◦
Integrating RAG (as RAGAR) into their system significantly improved performance (weighted F1) compared to a baseline without their RAG-augmented reasoning methods. It also contributed to the higher quality of generated explanations.
◦
Challenges associated with implementing RAG mentioned in the paper, within the scope of their study, include variance in search results from external web search tools even for similar queries, which affects the final outcome and comparison of approaches. Another challenge noted is related to retrieval quality and its connection to fact-checking ambiguous claims like those in the NEI class, which they excluded from their main evaluatio",The research highlights the use of a multimodal LLM (GPT-4V) to understand both textual and visual aspects of claims and the incorporation of image captions during evidence retrieval as a key contribution. The study focused on unambiguous claims from the MOCHEG dataset to validate the reasoning techniques.,"Explainability is a central goal of this work. Both CoRAG and ToRAG are designed to generate a series of questions and answers that lead to a veracity prediction, inherently providing a form of reasoning trace. Furthermore, the system explicitly generates a final fact-check explanation based on the collected evidence, which was then evaluated by human annotators for completeness. The use of the Chain of Verification (CoVe) prompt for veracity prediction further aims to verify and potentially correct the generated explanations.",
31,Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs,https://web3.arxiv.org/pdf/2408.12060,Conference,EMNLP 2024,A*,2024,November,USA,"Automated fact-checking for online claims, Social Media",Averitec,"1. Phi-3-medium is used for question generation. 
2. Answer generation and final classification: InternLM2.5, Llama-3.1 (including both 8B and 70B versions), Phi-3-medium, Qwen2, and Mixtral. 
3. The dunzhang/stella_en_1.5B_v5 model is used for generating dense embeddings for document retrieval.",The paper addresses the problem of the widespread dissemination of misinformation on social media and the challenge of manually verifying a large volume of online claims. The goal is to develop an automated system that not only predicts the veracity of a claim but also provides supporting evidence. Existing systems often lack evidence or focus on synthetic claims.,"The core methodology involves a Retrieve and Generate (RAG) pipeline integrated with Few-Shot In-Context Learning (ICL) using LLMs. The system first retrieves relevant documents using dense embeddings and FAISS. Then, it extracts evidence from these documents using an LLM to generate questions and answers. Finally, another LLM classifies the claim's veracity based on the extracted evidence using few-shot ICL."," The system employs a RAG pipeline for evidence extraction. First, the claim is used to retrieve the top three relevant documents from a knowledge base using dense embeddings. Then, an LLM generates questions based on the claim. These questions are posed to another LLM along with each of the retrieved documents to extract concise answers that serve as evidence.","Hungarian METEOR score, Averitec score","1. Their approach leverages recent advancements in Large Language Models (LLMs), specifically Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL)
2. The experiments showed that model performance generally improves with increasing model size on the development set. However, performance on the Open LLM leaderboard does not perfectly correlate with fact verification performance. The Refuted class was the easiest to predict, while Conflicting Evidence/Cherrypicking and Not Enough Evidence were more challenging. No single LLM excelled across all classes.","Averitec score of 0.33, a 22% absolute improvement","The source states that LLMs are prone to hallucination, and in their system, this means the extracted evidence could be incorrect due to hallucination. The reliability of the fact-checking prediction would therefore be affected if based on hallucinated evidence. As a strategy to mitigate this, the authors note that their methodology aligns with RAG pipelines which are known to alleviate hallucination. They also employed specific settings (temperature 0, greedy decoding) during question generation to minimize hallucinations. The source does not broadly survey other strategies proposed in the field to mitigate LLM hallucinations beyond mentioning RAG and specific decoding settings they used.","he source provides examples of prompt design used for question generation (Figure 3), answer generation (Figure 4), and final classification (Figure 5). It explains the purpose of these prompts in guiding the LLMs (e.g., challenging claim validity, deriving answers from text, guiding classification). The authors primarily rely on Few-Shot In-Context Learning (ICL) using these prompts, which requires only a minimal number of training samples and eliminates the need for large manually annotated datasets or extensive fine-tuning. They contrast this with the baseline which used fine-tuned models. The source notes that superior performance on a general LLM leaderboard doesn't guarantee better performance on this specific fact verification task, which touches on domain specificity, but it does not discuss the effect of domain-specific training methods or different fine-tuning architectures. While providing evidence adds to transparency, the source does not explicitly discuss the effect of prompt design, fine-tuning, or domain-specific training on the interpretability of the LLM's reasoning process itself, beyond providing the extracted evidence.","The source integrates RAG into their system. The impact observed in this research is that RAG enables the extraction of relevant evidence from a knowledge base, which is then used to support veracity predictions. The authors state that RAG pipelines alleviate hallucination. Their RAG-based system outperforms a baseline that used a different approach to evidence handling, demonstrating a positive impact on performance.","The system requires only a minimal number of training samples due to the use of few-shot ICL. Future work could explore fine-tuning LLMs using PEFT techniques, employing ensemble methods, and extending the system to multi-modal fact verification. A limitation is the reliance on high-quality LLMs and the inability to fully utilize large annotated datasets due to prompt size constraints. The authors also caution about the potential for LLM hallucination and intentional misuse of the system.",No,https://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms
32,"Fact-Checking Information from Large Language Models Can Reduce Disinformation Susceptibility
(Different name in openreview)",https://www.pnas.org/doi/epub/10.1073/pnas.2322823121,Journal,PNAS,Q1,2024,January,USA,Explores how large language models (LLMs) used for fact-checking affect the perception and dissemination of political news headlines.,40 real news headlines that are related to US politics,ChatGPT 3.5,"1. The challenge of implementing fact-checking at scale due to the overwhelming volume of online information
2. The unclear nature of how humans interact with fact-checking information provided by LLMs
3. The potential for harm stemming from AI applications in fact-checking
4. The need for policies to prevent or mitigate unintended consequences of AI fact-checking
.","1. Conducting a preregistered randomized control experiment to investigate the impact of LLM-generated fact checks
2. Comparing the effects of viewing LLM-generated fact checks with a control group and a group viewing human-generated fact checks
3. Analyzing participants' belief in and sharing intent of political news headlines under different conditions
4. Quantifying and accounting for ChatGPT's fact-checking accuracy by having authors independently label the fact-checking information
5. Exploring the causal effects of viewing LLM fact-checking information when accounting for model accuracy (correct, incorrect, unsure judgments)",No,"Custom metrics: belief in, sharing intent","1. LLM fact checks can actually reduce belief in true news wrongly labeled as false and increase belief in dubious headlines when the AI is unsure about an article’s veracity.
2. ChatGPT performs well at identifying false headlines while it mostly reports being unsure about true headlines, consistent with previous research.
3. LLM-generated fact checks do not significantly improve participants’ ability to discern headline accuracy or share accurate news, unlike human-generated fact checks.
4. Participants tended to believe true headlines less when the LLM incorrectly labeled them as false.
5. Participants showed an increased willingness to share true headlines that were correctly identified by the LLM.
6. When the LLM expressed uncertainty about the veracity of false headlines, participants were more inclined to believe and share them.",Important source of potential harm stemming from AI applications in fact-checking,,"Regarding prompt design, the sources indicate that the specific prompt used was designed to simulate realistic usage. An additional analysis (in SI Appendix) explored prompt engineering methods and found that forcing ""true"" or ""false"" judgments did not enhance overall accuracy. The sources also raise questions about which prompting techniques are most effective for creating AI fact checks. Fine-tuning architectures and domain-specific training are not discussed in the provided sources. While the sources mention interpretability as a desirable feature for robust systems, they do not discuss how these factors (prompt design, fine-tuning, domain-specific training) specifically affect the interpretability of LLMs in fact-checking tasks.","Based on the sources, the potential impact of integrating RAG in LLM fact-checking systems is discussed as a promising way to improve performance on new and evolving information (addressing the ""breaking news problem"") by augmenting LLMs with trusted data like real-time search. However, the sources do not discuss challenges associated with implementing RAG in specialized domains.
",The findings suggest that the format and style of AI-generated fact checks and effective prompting techniques require further research,No,https://github.com/osome-iu/AI_fact_checking
33,Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation,https://dl.acm.org/doi/pdf/10.1145/3589335.3651910,Conference,ACM WWW'24,A*,2024,May,USA,"Fact-checking, Misinformation, Claim matching, LLM, Textual entailment, Augmented intelligence",Synthetic dataset related to public health (COVID-19) created using LLMs,"GPT-4, GPT-3.5-Turbo, Llama-2-70b, Llama-2-13b, Llama-2-7b","1. Large inference time
2. Fact-checking has inherent bias
3. Cross-referencing are rare
4. Needs human coordination",,Not specifically but uses outer sources,"Precision, Recall, Accuracy","LLMs can reliably match claims, offering performance comparable to human ratings. If properly implemented, claim matching techniques could assist fact checkers in the early identification of recurring misinformation.",,"The paper discusses other limitations of LLMs, such as their proprietary nature, making decision-making challenging, and their probabilistic nature, which can lead to output variation and potential inconsistencies. It also notes the risk of propagating or amplifying biases present in training data.
◦
Strategies proposed for mitigating general risks and maximizing AI benefits in this context include ongoing collaboration among researchers, developers, and fact-checkers, understanding the strengths and limitations of both human and machine intelligence, careful and thoughtful implementation, and maintaining human oversight and expertise.","The paper shows that prompt design affects the performance of pre-trained LLMs, as different prompting styles yielded varied results, and no single style was consistently best.
◦
The paper explores fine-tuning by using different architectures/methods (OpenAI API for GPT-3.5, LoRA for Llama models) and training on domain-specific data (COVID-19 claims and tweets). They found that fine-tuning smaller models on high-quality synthetic domain-specific data allowed them to perform comparably to larger pre-trained models.
◦
The paper briefly mentions interpretability in the context of augmented intelligence requiring explanations for AI recommendations and suggests future work on Natural Language Explanation, but the results presented do not specifically detail how the evaluated prompt designs, fine-tuning approaches, or domain-specific training impacted the interpretability of the models.",,,No,https://github.com/EunCheolChoi0123/web24-beyondfacts-workshop-claim-matching
34,Large Language Models Help Humans Verify Truthfulness – Except When They Are Convincingly Wrong,https://aclanthology.org/2024.naacl-long.81.pdf,Conference,NAACL,A,2024,June,USA,"LLM, RAG, XAI, RAG + XAI on LLM, Search Engines",FoolMeTwice,GPT-3.5-turbo-0613,"1. The difficulty of combating over-reliance on language model outputs
2. The redundancy when combining retrieval and explanation
3. The need to determine the most helpful tool (explanations or retrieved pages) for human fact-checking","◦
1. Conducting user studies where participants verify whether claims are factually true or false
2. Comparing the effectiveness of natural language explanations generated by ChatGPT with retrieved passages from a search engine (mimicked by retrieval models)
3. Experimenting with contrastive explanations (arguing for or against a fact) versus non-contrastive explanations
4. Investigating whether there are complementary benefits in presenting both explanations and retrieved passages
5. Discouraging participants from searching the claims online
6. Using attention check questions to filter out inattentive participants
7. Using a RAG pipeline to ground explanation generation on retrieved evidence from Wikipedia",Conventional RAG pipeline to ground all explanation generation on retrieved evidence from Wikipedia to improve the factuality of the explanation.," human decision accuracy, human confidence and retrieval recall, time used","1. Taking longer time to read the retrieved passages is still more reliable than relying solely on LLM assistance
2. Human accuracy is lower when the retrieval recall is low
3. No strong correlation was observed between a participant’s average decision accuracy and time used
4. Our experiments provide nuanced empirical findings that contrastive explanations reduce overreliance at the cost of lower accuracy on cases where non-contrastive explanations are originally correct.","Natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where overrelying on wrong AI explanations could lead to critical consequences.","LLMs are prone to hallucinations, which means they can generate incorrect information or reasoning. When LLM explanations are wrong due to hallucinations or other errors, users tend to over-rely on them. This over-reliance leads to significantly lower human decision accuracy than both the baseline (no help) and using retrieval when the explanation is wrong. Hallucinated explanations can appear convincing, making it difficult for users to spot errors.
◦
Mitigation strategies explored in this paper:
▪
Contrastive explanations: Presenting both supporting and refuting arguments generated by the LLM. This method significantly reduces human over-reliance on wrong explanations, improving accuracy compared to non-contrastive explanations when the latter is wrong.
▪
Combining retrieval and explanation: Showing both retrieved passages and the explanation. This method also reduces over-reliance on wrong explanations.
▪
Grounding explanations on retrieved passages: Generating the explanation based on retrieved evidence significantly improves explanation accuracy, thus reducing the incidence of wrong explanations in the first place.","The source describes constructing prompts by concatenating retrieved passages and the claim to ground the explanation generation. For contrastive explanations, specific prompts asking ""why the claim is true"" and ""why the claim is false"" were used. Prompting was also used for citation insertion. Grounding explanations via prompt design significantly improved explanation accuracy. The structure of contrastive prompts influenced how users made decisions by presenting competing arguments","The study implemented RAG by grounding LLM explanation generation on retrieved Wikipedia passages. This integration had a significant positive impact on the accuracy of the generated explanations. Generally, RAG is described as complementing parametric knowledge and helping reduce hallucination in LLMs. Retrieved evidence is considered effective for aiding human verification.
◦
Challenges in specialized domains: Challenges related to implementing RAG in specialized domains are not discussed in the provided source text. The study focused on general fact-checking using Wikipedia as the evidence source. However, the finding that retrieval quality impacts explanation accuracy suggests a potential challenge: RAG performance would likely depend heavily on the quality and coverage of the domain-specific corpus used for retrieval.","1. The study uses an adversarial dataset. 
2. It explicitly discourages participants from using external search engines.
3. The paper poses specific research questions to guide their investigation.","1.  Contrasted model-generated explanations with passages retrieved from external sources (Wikipedia).
2. The study investigates the utility of natural language explanations generated by LLMs in assisting human fact-checking, which is directly related to the goals of Explainable AI (XAI) by providing insights into the model's reasoning (even if ultimately to compare its effectiveness to retrieval)",
35,Automated fact-checking of climate claims with large language models,https://www.nature.com/articles/s44168-025-00215-8?utm_source=chatgpt.com#Abs1,Conference,Nature,Q1,2025,February,Switzerland,automated fact-checking of climate claims ," Climate Feedback dataset, a curated collection of 170 annotated climate-related claims with verdicts provided by climate scientists. The annotations include 12 fine-grained categories of credibility","CLIMINATOR, iGPT-4o","The paper addresses the problem of accurately and scalably identifying climate misinformation in the digital age. Existing automated fact-checking tools struggle with complex claims and lack detailed reasoning, particularly in the climate domain. The sheer volume and rapid spread of misinformation also limit the impact of manual fact-checking efforts.","The researchers developed CLIMINATOR, a novel debating framework based on a Mediator-Advocate model.
◦
Advocates: Specialized LLMs (GPT-4o and RAG systems grounded on specific corpora like IPCC, WMO, AbsCC, and 1000S) examine claims against their respective text sources and provide a verdict, rationale, and references. An adversarial advocate (NIPCC) representing climate denial was also used for robustness analysis.
◦
Mediator: Another LLM (GPT-4o) collects the advocates' verdicts, assesses agreements, and initiates further debating rounds if disagreements occur by posing follow-up questions. The Mediator synthesizes the perspectives into a balanced final assessment and provides a detailed reasoning overview.
◦
The process can involve iterative discourse over multiple rounds to resolve discrepancies and reach a final verdict.
◦
The methodology is inspired by deliberation literature and modern interpretations of automated fact-checking as a deliberation setting",Yes,Precision  Accuracy Recall F1-Score,"CLIMINATOR significantly enhances the efficacy of fact-checking, surpassing the capabilities of a single LLM (GPT-4o) in isolation.
◦
CLIMINATOR achieved a binary classification accuracy exceeding 96% on the Climate Feedback dataset.
◦
CLIMINATOR demonstrates a clear advantage over GPT-4o as the classification expands to more detailed levels (Levels 1-3), highlighting its ability to handle nuanced classifications due to its reliance on trusted RAG advocates. For example, at Level 2, CLIMINATOR achieved an accuracy of 72.7% compared to GPT-4o's 56.5%.
◦
CLIMINATOR consistently balances precision and recall across different levels of classification, which is crucial for real-world applications with unbalanced datasets.
◦
The integration of an adversarial advocate (NIPCC) increases the frequency of debates and demonstrates CLIMINATOR's robustness in the presence of contrarian perspectives, with marginal improvement or slight decrease in accuracy at detailed levels but near-identical performance at coarser levels.
◦
The debate mechanism in CLIMINATOR enables the model to synthesize multiple viewpoints and provide a balanced, evidence-based assessment, as illustrated by the example claim about the Amazon rainforest.
◦
CLIMINATOR delivers verdicts with apparent justification by providing explanations and source references, potentially enhancing user trust.","High Accuracy: Achieves over 96% binary classification accuracy on the Climate Feedback dataset (Level 4).

Superior to GPT-4o: Outperforms GPT-4o across all classification levels (Level 1–3) in accuracy and F1-score, especially for detailed claims.

Robust Precision-Recall Balance: Maintains strong precision-recall tradeoffs across levels, unlike GPT-4o, which sacrifices recall for precision.

Resilience to Adversarial Input: Handles adversarial NIPCC advocates well, maintaining stable performance at coarser levels and even improving Level 1 accuracy due to enhanced debates.

Effective Debate System: The Mediator is more frequently triggered by adversarial input, indirectly improving deliberation.

Hierarchical Strength: Class aggregation helps reduce the impact of adversarial noise.

Transparent Verdicts: Provides explanations and source references, improving user trust.","The sources mention that general-purpose LLMs are usually designed to be more conservative to avoid hallucinations and off-target outputs.
◦
The sources state that this conservative design choice, while aimed at reducing hallucinations, results in GPT-4o exhibiting higher precision but reduced recall in their experiments. This implies that being overly cautious (to avoid hallucinations) can lead to missed classifications (lower recall), thereby affecting overall reliability, particularly for identifying less represented or nuanced claim types.
◦
Regarding mitigation strategies within their system, CLIMINATOR relies on RAG advocates grounded in specific, trusted sources (like IPCC, WMO reports). This approach inherently mitigates hallucinations by ensuring responses are based on predefined, credible text corpora rather than solely on the LLM's internal knowledge. The debate mechanism itself, requiring advocates to provide evidence-based rationales and references, also acts as a check.","they modified the Mediator's prompt during robustness testing with the adversarial NIPCC advocate to avoid potential biases, forcing the model to strictly mediate. They also state that their prompting is provided in the Supplementary Information. This suggests that prompt design is used to control the model's behavior (mediation vs. bias).Fine-tuning architectures & Domain-specific training: The sources note that GPT-4o is a general-purpose model and appears to struggle with intermediate classification tiers that require domain-specific knowledge. They suggest that transitioning to open-source language models specifically trained for climate science fact-checking tasks could enhance transparency and allow for continuous improvements. They also mention that recent work on domain-specific retrieval has shown that precise relevance definitions and targeted fine-tuning can significantly improve RAG systems, suggesting incorporating such approaches. This indicates that domain-specific training and fine-tuning are seen as ways to improve performance in specialized domains like climate science fact-checking.
◦
The sources imply that better handling of domain-specific knowledge through training or fine-tuning would likely improve accuracy, especially at detailed classification levels. The suggestion for open-source models specifically trained for the domain is linked to enhanced transparency.",". Impact of RAG in CLIMINATOR
Core Component: RAG (Retrieval-Augmented Generation) is central to CLIMINATOR's architecture.

Performance Boost: Major improvements over GPT-4o are due to RAG's integration.

Trusted Advocates: Leverages authoritative sources (e.g., IPCC, WMO) for factual grounding.

Nuanced Classification: Enables handling of detailed, hierarchical classification tasks with improved context understanding.

Grounded Responses: Ensures LLM outputs are backed by credible, evidence-based information.

2. Challenges of RAG in Specialized Domains (Climate Science)
a. Outdated Information
Climate science evolves rapidly.

RAG risks using stale data, necessitating continuous knowledge base updates.

b. Source Limitations
Restricted access to paywalled or non-indexed materials.

Current sources may not cover the full spectrum of relevant data.

Suggested improvement: Expand and diversify trusted source pool.

c. System Bottlenecks
RAG pipelines may introduce latency or scalability issues, affecting real-time performance.",,,
36,A Mystery for You: A fact-checking game enhanced by large language models (LLMs) and a tangible interface,No PDF,Conference,CHI EA ’24,A*,2024,May,USA,,,,,,,,,,,,,,,
37,Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger Language Models,https://arxiv.org/pdf/2402.12147,Conference,SIGIR,A*,2024,July,Norway,automated fact-checking,"ata from their production deployment at Factiverse. This dataset was originally in English and was translated into 114 languages using the Google Translate API. This dataset has a majority class of 'True' claims, unlike many existing fact-checking datasets.
•
Data from ClaimBuster.
•
Data from CLEF CheckThat Lab!.
•
A dataset collected from the Factiverse production system.
•
A sample of 100 numerical claims from a collection of past fact-checks from Factisearch.
•
FEVER data was used in combination with real-world fact-checks from Factisearch for fine-tuning the veracity prediction model.
•
The ClaimDecomp dataset was used for fine-tuning the T5-3b model for question decomposition.
•
The FinQA dataset was used to fine-tune FinQA-RoBERTa-Large for verifying numerical claims.","XLM-RoBERTa-Large (fine-tuned) for check-worthy claim detection and veracity prediction.
•
FinQA-RoBERTa-Large (fine-tuned) for verifying numerical claims.
•
Large Language Models (LLMs):
◦
GPT-4.
◦
GPT-3.5-Turbo.
◦
Mistral-7b.
•
T5-3b (fine-tuned) for question decomposition.
•
A multilingual sentence encoder was used to select the top three most similar paragraphs to a claim from search results.
•
spaCy models were used for sentence segmentation.
•
multilingual-MiniLM-L12-v2 is mentioned in Figure 1 as part of the evidence retrieval stage.","The paper addresses the challenges associated with establishing an end-to-end fact-checking pipeline in a real-world context, covering over 90 languages. Key challenges include:
•
Multilinguality: Fact-checking in a large number of languages.
•
Verifying numerical claims: Assessing the accuracy of claims involving numerical quantities.
•
Temporal aspects of facts: While mentioned as a challenge in general, this paper doesn't explicitly focus on solving this aspect.
•
Privacy concerns: Journalists and fact-checkers may be hesitant to transmit sensitive information to third-party servers hosting LLMs.","They follow a three-stage fact-checking pipeline:
1.
Check-worthy Claim Detection: Identifying sentences that require fact-checking using a fine-tuned XLM-RoBERTa-Large model. This involves segmenting sentences using spaCy and classifying them as 'Check-worthy' or 'Not check-worthy' based on criteria like inviting public scrutiny and excluding subjective statements.
2.
Evidence Search: Gathering relevant articles for detected claims. This stage involves:
◦
Using LLMs (GPT-4, GPT-3.5-Turbo, Mistral-7b) and a fine-tuned T5-3b model to generate targeted questions from the claim to diversify search results.
◦
Leveraging a diverse range of search engines (Google, Bing, You.com, Wikipedia, Semantic Scholar, Factiverse's fact-checking collection).
◦
Deduplicating search results based on URLs, titles, and content similarity.
◦
Selecting the top three most similar paragraphs to the claim using a multilingual sentence encoder.
3.
Veracity Prediction: Predicting the truthfulness of the claim based on the retrieved evidence. This involves:
◦
Excluding websites known to spread misinformation to ensure credible sources.
◦
Predicting the stance of the evidence towards the claim (supported or refuted) using a fine-tuned XLM-RoBERTa-Large model (Natural Language Inference - NLI).
◦
Aggregating stance predictions using majority voting.
◦
Summarizing evidence snippets using an LLM to provide a justification for the veracity prediction.
◦
Generating a correction for refuted claims based on the justification summary using an LLM."," The paper describes a process of evidence retrieval using search engines based on queries generated from the claims, followed by using this evidence to predict veracity and generate justifications. This process shares similarities with Retrieval-Augmented Generation (RAG)",Macro-F1 score,"
Fine-tuned Transformer models (like XLM-RoBERTa-Large) outperform larger language models (LLMs) such as GPT-4, GPT-3.5-Turbo, and Mistral-7b for fact-checking tasks like claim detection and veracity prediction, especially in multilingual settings (covering over 90 languages) and for complex claims involving numerical quantities.
•
LLMs excel in generative tasks such as question decomposition for evidence retrieval. Specifically, GPT-3.5-Turbo outperformed other methods for question decomposition for English claims.
•
Fine-tuned models like FinQA-RoBERTa-Large demonstrate superior performance for verifying numerical claims compared to a generally fine-tuned multilingual model (XLM-RoBERTa-Large). Interestingly, Mistral-7b showed better question decomposition for numerical claims.
•
Smaller, self-hostable LLMs like Mistral-7b, despite underperforming in claim detection, showed competitive or even superior performance in veracity prediction for some languages compared to larger OpenAI models. This also addresses privacy concerns related to using third-party LLM servers.",,". Negative Impact of Hallucinations
Hallucinations degrade fact-checking performance in LLMs, especially in claim detection tasks.

Mistral-7b performs worst overall, due in part to:

Instruction-following difficulties in prompts.

Higher hallucination rates compared to other models.

2. General Reliability Concerns
LLM-based chat apps and writing tools still produce erroneous text during fact-checking.

Hallucinations and errors undermine LLM reliability in fact-checking applications.

3. Missing Mitigation Strategies
The source does not offer solutions or discuss strategies to reduce hallucinations in LLMs.

","Fine-Tuning Improves Accuracy
Fine-tuning models specifically for fact-checking tasks significantly boosts accuracy.

XLM-RoBERTa-Large (fine-tuned) outperforms general-purpose LLMs in:

Claim detection

Veracity prediction

2. Domain-Specific Advantage
FinQA-RoBERTa-Large (fine-tuned for numerical claims) shows stronger performance, demonstrating the benefits of domain-specific fine-tuning.

3. Fine-Tuning vs. Prompt Engineering
The study compares prompt-engineered LLMs with fine-tuned models.

Results: Fine-tuned models are significantly more effective than using prompt engineering alone for classification tasks.

","Describes an ""Evidence Search"" stage that gathers evidence from:

Search engines

Fact-checking collections

This evidence is used for veracity prediction.

Though RAG is not mentioned, the process mirrors the retrieval component of RAG systems.

2. Impact of Question Decomposition
Question decomposition methods influence the quality of evidence retrieval and veracity prediction accuracy.

GPT-3.5-Turbo for decomposition → higher F1 scores.

T5-3b (fine-tuned) → lower performance.

Demonstrates the importance of query generation quality in the overall fact-checking pipeline.

3. Challenges Identified
Privacy concerns when sending sensitive data to third-party LLM servers.

Evidence aggregation issues:

Difficulty in merging multiple evidence snippets.

Need to evaluate source credibility.

",,No,https://github.com/vinaysetty/factcheck-editor
38,Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification,https://arxiv.org/pdf/2403.04696,Conference,ACL-2024,A*,2024,August,Multiple,fact-checking and hallucination detection in Large Language Models (LLMs),"The paper used a novel benchmark based on fact-checking of biographies of individuals generated using a range of LLMs.
•
For evaluation, they generated biographies in English, Chinese, Arabic, and Russian based on prompts asking for biographies of famous people.
•
The English data was annotated both automatically using FactScore, a fact-checking tool that leverages an external knowledge source (Wikipedia), and manually by human annotators who checked against Wikipedia articles.
•
The Chinese, Arabic, and Russian data were manually annotated by two annotators with access to the corresponding Wikipedia articles","The primary model they proposed and used in their methodology is their novel Claim-Conditioned Probability (CCP) method, which is a token-level uncertainty quantification method. They compared CCP against several baseline uncertainty quantification methods, including:
•
Maximum Probability (MP)
•
Perplexity
•
Maximum Entropy of a token in the claim (Ent)
•
P(True) (asking the LLM itself if the claim is true)
","The problem they solved is the hallucination of large language models (LLMs), which refers to their tendency to produce erroneous or factually incorrect claims in their output. These inaccuracies can be difficult for users to spot as the surrounding text might be factually correct and persuasive. Current LLM-based services often lack mechanisms for detecting such unreliable generations.","Their methodology proposes a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification.
•
The pipeline involves splitting the generated text into atomic claims. For this, they followed the FactScore approach using the OpenAI Chat API.
•
Each atomic claim is matched against the sequence of tokens in the original text, along with their probability distributions.
•
They then calculate token-level uncertainty scores using their proposed Claim-Conditioned Probability (CCP) method.
◦
CCP aims to measure only the uncertainty of a particular claim value expressed by the model by removing the impact of uncertainty about what claim to generate and what surface form to use.
◦
CCP is calculated at the word level using Natural Language Inference (NLI) to compare the original word with its top-K alternatives from the autoregressive distribution.
◦
The CCP for a word is the ratio of the sum of probabilities of alternatives that entail the original word to the sum of probabilities of alternatives that either entail or contradict the original word.
◦
Functional words are treated with a CCP of 1.
•
Token-level CCP scores are then aggregated into a claim-level uncertainty score by taking the product of the CCPs of each word in the claim.
•
Finally, the claim-level uncertainty scores are compared against a threshold to identify potentially unreliable claims, which can then be highlighted to the user.",,ROC-AUC PR-AUC ,"
The proposed CCP method consistently outperforms baseline uncertainty quantification techniques across seven LLMs (Vicuna 13b, Mistral 7b, Jais 13b, GPT-3.5-turbo, Yi 6b, GPT-4, Vikhr-instruct-0.2 7b) and four languages (English, Chinese, Arabic, and Russian) in detecting factual errors.
•
Human evaluation of their fact-checking pipeline based on uncertainty quantification is competitive with FactScore, a tool that uses external knowledge. In some cases, CCP even outperformed FactScore.
•
CCP effectively identifies incorrect claims that baseline methods like Maximum Probability might miss, as it focuses specifically on claim uncertainty while ignoring other types of uncertainty irrelevant for fact-checking.
•
Ablation studies showed that the product of token-level CCPs is the best aggregation method for claim-level uncertainty.
•
The performance of CCP is not critically dependent on the complexity of the NLI model used. Even a smaller NLI model yielded strong performance.
•
The context provided to the NLI model for calculating word-level CCP impacts performance, with the claim prefix being the most effective.
•
Excluding functional words in the CCP calculation improves performance.
•
The number of alternative tokens (K) considered in CCP affects performance, with diminishing returns after K=8.
•
The computational overhead of CCP compared to Maximum Probability is relatively low, with an increase of only around 3-8% in runtime depending on the NLI model used.
•
They observed that LLMs tend to generate more unreliable claims as the length of the generated biography increases.",,"ffect on reliability: Hallucinations are factually incorrect generations. They negatively impact the reliability of LLM output as they can be highly coherent and persuasive, making it extremely hard for users to spot the inaccuracies. This makes LLMs less trustworthy as primary information sources.
◦
Mitigation strategies: The paper proposes a strategy based on token-level uncertainty quantification. This involves leveraging uncertainty scores inherent in the model's output to detect unreliable predictions. The core idea is to highlight potentially deceptive fragments for users to take into account. Their specific method, CCP, aims to improve detection effectiveness by isolating the relevant ""claim uncertainty"" while ignoring other sources of uncertainty in the token distribution.","he paper evaluates its UQ method on various LLM architectures. Their own method relies on a fine-tuned NLI model. The study focuses on the performance of their UQ method across different LLMs rather than how the fine-tuning of the LLMs themselves affects fact-checking accuracy using their UQ method.
◦
Domain-specific training: The LLMs used are general-purpose or instruction-tuned models. The NLI model used for CCP is fine-tuned for NLI. The benchmark is domain-specific (biographies). The paper notes the NLI model was pre-trained for a slightly different use-case and suggests that analyzing its performance on diverse domains or fine-tuning it might be beneficial, implying that domain alignment is important. The study evaluates the UQ method's performance on this specific domain across models and languages.","The paper positions its UQ method as an alternative to systems that leverage external knowledge sources (akin to RAG). It shows that a UQ approach without external knowledge can be competitive with or outperform an external knowledge-based tool like FactScore (which uses retrieval). This suggests that UQ offers a potentially comparable level of fact-checking capability without the need for external retrieval components.
◦
Challenges of RAG: Challenges associated with using external knowledge sources (which RAG relies on) mentioned in the sources are their incomplete nature and the notable overhead in terms of storing the knowledge and requiring additional computational resources. The paper does not specifically discuss RAG implementation challenges in specialized domains, but the point about incomplete knowledge sources implies that obtaining comprehensive and relevant data for specialized domains could be a significant challenge.",,No,https://github.com/IINemo/lm-polygraph
39,Yours Truly: A Credibility Framework for Effortless LLM-Powered Fact Checking,https://ieeexplore.ieee.org/abstract/document/10807167,Journal,IEEE Access,Q2,2024,December,Unknown,"Fake news detection, misinformation detection, credibility assessment, natural language processing (NLP)","FactStore: A real-time database of fact-checked claims from Indian and International fact-checking initiatives. It contains 7200 documents from factcheck.org.
◦
TRUTHSEEKER 2023: A dataset of tweets with truth labels.
◦
FACTIFY-5WQA: A dataset for fact verification with explainability through question-answering.
◦
LIAR dataset: A dataset of short statements labeled for truthfulness.
◦
FEVEROUS dataset: Fact Extraction and VERification Over Unstructured and Structured information, with claims annotated with evidence from Wikipedia.
◦
Manually labeled bootstrapped samples of size 100 from the FactStore database.","Large Language Models (LLMs): Specifically, GPT-4 (gpt-4-turbo-preview), GPT-3.5 (gpt-3.5-turbo), and Mistral7B (mistralai/Mistral-7B-Instruct-v0.2). Mistral-7B was fine-tuned for claim atomization and truth value inference. ChatGPT was used for generating initial training examples for the Claim Atomizer.
◦
KeyBERT: For extracting keywords from tweets to form optimized search queries.
◦
OpinionFinder (version 2.0): For identifying subjective claims.
◦
all-MiniLM-L6-v2 embedding: For creating vector embeddings for semantic search.
◦
Bidirectional LSTM: As part of ClaimBuster's claim spotter component (although Yours Truly refines this approach).","Discerning truth from propaganda in an era of subjective realities portrayed on social media, particularly addressing the rapid spread of misinformation on platforms like Twitter. The system aims to provide a robust method to assess the veracity of social media claims, reduce the cognitive effort needed for fact-checking, and break misinformation chains sooner. Existing automated fact-checking methods often have limitations in accuracy and automation.","Yours Truly"" framework is an end-to-end credibility scoring system that automates the fact-checking process. It involves the following key stages:
◦
Data Acquisition: Utilizing FactStore, a dynamically updated database of verified information from trusted fact-checking sources.
◦
Claim Processor:
▪
Tweet Preprocessor: Cleans the input tweet by removing non-ASCII characters, duplicates, and user tags, and extracts hashtags to form optimized search queries using KeyBERT.
▪
Claim Atomizer: Breaks down compound sentences in the preprocessed tweet into atomic, independent claims using a fine-tuned LLM (Mistral7B). It also infers claim type (factual or subjective) and a checkworthy flag. Subjective and non-checkworthy claims are filtered out.
◦
Claim Evaluator:
▪
Fact Searcher: Retrieves relevant articles from FactStore using a combination of token similarity search (using MongoDB Atlas search API and TF-IDF) and vector search (using cosine similarity on embeddings). Vector search results are weighted more heavily.
▪
Fact Ranker: Reranks the retrieved articles for relevance using a Query-Based Committee Selector. This involves generating paraphrased variations of the atomic claim using an LLM (gpt-3.5-turbo) and repeating the Fact Searcher process for each variation. Articles are reranked based on their match frequency across these variations.
▪
Claim Inferrer: Infers the truth value of each atomic claim using a fine-tuned LLM (Mistral7B or GPT models) with the top-ranked articles as context via Retrieval Augmented Generation (RAG). It uses sentence window retrieval for single high-score matches and auto-merging retrieval for multiple high or medium-score matches to handle fragmented information.
◦
Performance Evaluator: Assesses the system's performance using metrics like Precision, Recall, F1 Score, Accuracy, and BERTScore on various fact-checking datasets.
◦
Dataset Updation: FactStore can be periodically updated with analyzed tweets and user feedback.
◦
User Interface: A web app built using Streamlit is used for testing, taking a tweet as input and providing a credibility report with references.","YES   In the Claim Inferrer module, the top-ranked articles retrieved by the Fact Searcher and reranked by the Fact Ranker are used as context to prompt a fine-tuned Large Language Model (LLM) to infer the truth value of each atomic claim",confusion matrix BERTScore F1 schore,"The proposed Claim Atomizer achieves a high BERTScore F1 Score of 0.74, indicating effective atomization of compound claims.
◦
Using the Claim Atomizer significantly improves the predictive power of Yours Truly from 0.64 to 0.9 on average, reducing cases where the system couldn't confirm or deny a claim due to insufficient context.
◦
The Query-Based Committee Selector effectively reranks search results by relevance.
◦
Yours Truly achieves an impressive F1 Score of 94% using GPT-4 and 90% using GPT-3.5 for truth value inference on FactStore claims. Mistral7B also performs well but slightly lower than the GPT models.
◦
The framework demonstrates versatility and competitive performance across diverse datasets like LIAR, TRUTHSEEKER 2023, and FEVEROUS, sometimes outperforming existing methods, especially when considering that it does not rely on additional metadata like some other ML algorithms.
◦
The system can be easily extended to other social media platforms as it primarily relies on text.
◦
The framework automates the entire fact-checking process without requiring manual intervention for truth labeling.","
High F1 scores for claim inference using GPT models (94% and 90%).
◦
A BERTScore F1 of 0.74 for the Claim Atomizer.
◦
Improved predictive power with the Claim Atomizer (from 0.64 to 0.9).
◦
Competitive performance on benchmark datasets compared to other state-of-the-art methods, as shown in Table 3. For instance, Yours Truly achieves an F1 score of 0.92 and accuracy of 91% on tweets synthesized from the LIAR dataset.","The sources mention that hallucinations are an issue in LLMs during news claim verification. This suggests that LLMs generating information not supported by evidence would negatively impact fact-checking reliability. The paper mentions the hierarchical Step-by-Step (HiSS) prompting method as proposed in related work to address omitting necessary thoughts and fact hallucination in LLMs. FactAgent is also mentioned as outperforming HiSS in fact verification. Within the Yours Truly framework, strategies to improve inference results (which inherently mitigate hallucination by grounding the LLM) include providing highly relevant and focused contexts to the LLMs through the Claim Atomizer and the refined search/ranking mechanisms (Fact Searcher, Fact Ranker). The integrated RAG system's use of techniques like sentence window retrieval and auto-merging retrieval ensures that the LLM receives precise, context-rich segments from retrieved articles, reducing the likelihood of generating unsupported facts. Chain of thought prompting is also mentioned as a potential future direction to improve inference recall when relevant articles are unavailable, which could also help in reasoning and potentially mitigating hallucination in low-resource scenarios.","Effective prompting is critical for guiding LLMs in tasks like atomization (breaking claims into verifiable units), query paraphrasing, and claim verification. Techniques include:

Detailed prompts with examples for instruction fine-tuning (e.g., atomizer training).

Few-shot learning (using in-prompt examples) to enhance GPT model performance.

Structured prompts that integrate retrieved context (via RAG) and claims for inference.

Advanced strategies like FRESH PROMPT (improves factuality) and future use of Chain of Thought (to boost reasoning and recall).

2. Fine-tuning Architectures:

Mistral-7B was instruction fine-tuned for atomization and inference tasks, validated by strong BERTScore performance.

Prompt tuning (adjusting prompts, not model weights) may outperform full fine-tuning in low-data scenarios.

3. Domain-specific Training:

The framework generalizes across domains but uses domain-specific FactStore (aggregates fact-checking data on diverse topics).

No traditional domain-specific training (avoids labeled datasets), relying instead on RAG to inject domain context.

The Claim Atomizer focuses on ""checkworthiness"" (public interest relevance) rather than domain specificity.

4. Interpretability:

Generates credibility reports with transparent breakdowns.

Atomic claims simplify verification, mirroring human reasoning.

Future plans include inferring statement type (factual/subjective) and sentiment to compute weighted scores, enhancing explainability.","Integrating Retrieval-Augmented Generation (RAG) into Yours Truly's framework enhances claim inference accuracy by grounding the LLM in a dynamic, up-to-date knowledge base (FactStore). This approach overcomes limitations of traditional fact-checking by leveraging advanced retrieval techniques like sentence window retrieval (for precise context) and auto-merging retrieval (to synthesize fragmented information), particularly improving performance on long or disjointed texts.

Challenges are not explicitly tied to specialized domains but include general hurdles: managing lengthy source articles (addressed via chunking) and fragmented data across sources (resolved through auto-merging). A key limitation arises when the FactStore lacks sufficient data on novel or recent claims, leading to ""insufficient data"" outcomes. While future solutions like chain-of-thought prompting are proposed, the focus remains on knowledge coverage gaps rather than domain-specific barriers. The system prioritizes adaptability across information domains but hinges on robust FactStore content.",,,
40,A Unified LLM-KG Framework to Assist Fact-Checking in Public Deliberation,https://aclanthology.org/2024.delite-1.2/,Conference,DELITE2024,Unranked,2024,May,Greece, Fact-checking in the context of public deliberation and digital democracy,The researchers used two publicly available deliberations from the European Commission’s “Have Your Say” platform: one about the export of hazardous chemicals by the EU and another about the cultivation or import of Genetically Modified Organisms. They created five example scenarios from these deliberations for their experimental evaluation," ChatGPT-3.5 as the baseline LLM and integrated it into their proposed unified LLM-KG framework, also utilizing ChatGPT-3.5.","The paper addresses the need to augment the effectiveness of digital deliberation platforms and facilitate evidence-based collective decision-making by enhancing the synergy between human and machine reasoning. Specifically, it aims to overcome the limitations of relying solely on participants' abilities to process information and to address the issues of hallucinations in LLMs and the inability of KGs to handle missing facts or possess NLU capabilities when used independently for fact-checking","They proposed a unified LLM-KG framework. This framework involves:
◦
Using an LLM to extract important entities from user input text.
◦
Querying a dynamically updated KG with these entities to retrieve related factual triplets.
◦
Using these retrieved KG triplets as contextual input to the LLM to enhance its prompt for fact-checking.
◦
Dynamically updating the KG with evidence extracted by the LLM from relevant URLs (pointing to studies) or top web search results (prioritizing .org, .gov, .eu sites with .pdf reports) related to user claims.
◦
Conducting an experimental evaluation using a questionnaire where users assessed a baseline LLM against the proposed framework based on fact-checking metrics.",The proposed framework does utilize a form of RAG. The Knowledge Graph serves as the external knowledge source from which relevant facts (triplets) are retrieved and then used to augment the context provided to the Large Language Model for fact-checking,"Metrics Used: The experimental evaluation used the following human evaluation metrics:
◦
Readability: Clarity and quality of writing and explanations.
◦
Coverage: Explanation of all important points for fact-checking with appropriate reasoning.
◦
Non-Redundancy: Provision of relevant information without repetition.
◦
Quality: Overall quality of the generated text. These metrics were measured using a Likert scale (1-5).","The experimental results were promising and confirmed the potential of combining LLMs and KGs for fact-checking in public deliberation. The proposed LLM-KG framework obtained better average scores than the baseline LLM with respect to Readability and Coverage, and a significantly improved average score with respect to Non-Redundancy. The average score for Quality was similar for both setups. The baseline LLM often referred to the lack of concrete data, while the proposed approach successfully used the KG context to provide factual responses","The user evaluations indicated that the KG-enhanced LLM outperformed the baseline LLM in readability, coverage, and significantly in non-redundancy. The overall quality was rated similarly for both","
LLMs can present factually incorrect information (hallucinations), which affects fact-checking reliability by creating plausible yet incorrect facts. Hallucinations can occur based on false training data or mistakes in the LLM's reasoning process.
◦
LLMs' generalization capabilities can lead to hallucinations when there is no proper context in the prompt.
◦
Combining LLMs with Knowledge Graphs has been proposed as a means to mitigate the hallucination issues exhibited by LLMs.
◦
The proposed unified LLM-KG framework uses KG facts as contextual sources for the LLM, which helps to improve the quality and validity of its responses and mitigates hallucination issues.","
Prompt design affects accuracy and the integration of knowledge. The proposed framework uses prompts to instruct the LLM to perform fact-checking and to extract and utilize KG facts. Augmenting the prompt with KG facts helps the LLM produce a response based on factual knowledge, overcoming the issue of relying solely on internal, potentially inaccurate, knowledge. Instructions in the prompt can also guide the output style (e.g., not repeating context verbatim).
◦
Domain-specific training/knowledge is important as KGs contain domain-specific facts. A lack of domain-specific facts in KGs can hinder the verification of user claims. Integrating diverse domain-specific knowledge into the KG is proposed to improve generalization across domains.
◦
LLM architecture affects interpretability. LLMs have a black-box architecture, storing knowledge in a non-interpretable manner. They struggle to explain their outputs. The proprietary nature of models like ChatGPT means model weights are inaccessible, preventing explainability methods. The sources do not discuss how fine-tuning architectures specifically affect accuracy or interpretability.","he sources describe a framework that integrates retrieval (from a KG) and generation (by an LLM). The impact of this integration is positive:
▪
It leverages the strengths of both technologies: contextual understanding of LLMs and structured knowledge representation of KGs.
▪
It helps mitigate hallucination and indecisiveness issues exhibited by standalone LLMs.
▪
It allows the LLM to generate responses based on provided factual knowledge rather than relying on potentially incomplete or inaccurate internal knowledge.
▪
In their evaluation, this approach led to better or similar scores in human evaluation metrics compared to a baseline LLM, specifically showing improvement in Readability, Coverage, and Non-Redundancy.
▪
It improves transparency through factual context provided by KGs A challenge associated with implementing this approach, particularly in specialized domains, is that KGs often do not contain sufficient domain-specific facts important for verifying user claims. Future work is needed to integrate diverse domain-specific knowledge from various data sources into the KG to facilitate generalization across multiple domains",,,https://forms.gle/GNZaZGXWk4PLsQch7
41,LoCal: Logical and Causal Fact-Checking with LLM-Based Multi-Agents,https://openreview.net/forum?id=f5FDfChZRS#discussion,Journal,ACM WebConf,A*,2025,April,China,"Fact-Checking, specifically addressing challenges in misinformation detection",FEVEROUS and HOVER,"The paper proposes LoCal (Logical and Causal fact-checking), a novel fact-checking framework based on multiple Large Language Model (LLM) agents. The system comprises different types of agents, each built upon LLMs:

Decomposing Agent: Utilizes the in-context learning ability of LLMs (specifically gpt-3.5-turbo) to break down complex claims into simpler sub-tasks (fact verification tasks and question answering tasks) and a deduction function. This agent uses few-shot learning.




Reasoning Agents (two types, based on Flan-T5):
Fact Verificating Agent: Solves sub-tasks that are exact atomic claims, requiring comparative analysis skills and returning a Boolean answer.




Question Answering Agent: Addresses sub-tasks that are one-hop questions, extracting direct answers from evidence.




Evaluating Agents (two types, based on gpt-3.5-turbo):
Logically Evaluating Agent: Examines if the generated solution (summary description of the reasoning process and the predicted veracity) is logically equivalent to the original claim.




Counterfactually Evaluating Agent: Determines if the solution still holds when challenged by the counterfactual (opposite) label, checking for causal consistency.","Logical Issues: It tackles problems related to how sub-tasks derived from a complex claim can be logically and accurately combined to represent the original claim.


Causal Errors: It aims to reduce errors in the reasoning process that arise from insufficient evidence or hallucinations from LLMs, which can lead to incorrect or non-unique answers for sub-tasks.

Lack of Interpretability: It seeks to improve the interpretability of the fact-checking process.","Their methodology proposes LoCal, a multi-agent framework consisting of:
•
A decomposing agent that uses in-context learning of LLMs to break down complex claims into simpler sub-tasks (fact verification and question answering).
•
Multiple reasoning agents (fact verifying and question answering) that retrieve external knowledge to address the sub-tasks.
•
Two evaluating agents (logically evaluating agent and counterfactually evaluating agent) that examine whether the generated solution is logically equivalent to the original claim and whether the solution holds when challenged by a counterfactual label. These agents provide confidence degrees and iteratively correct logical and causal errors.
•
A confidence updater estimates the confidence of the predicted veracity based on the evaluations from the evaluating agents and determines whether to continue iterations.
","Yes, LoCal uses external knowledge and incorporates a Retrieval-Augmented Generation (RAG) approach, particularly in its ""open-book setting"".


In this setting, when gold evidence is not provided, the framework utilizes the Pyserini toolkit as a retrieval tool to collect information (top 5 relevant paragraphs) for each sub-task from the Internet or associated knowledge bases of the HOVER and FEVEROUS datasets.

This retrieved evidence is then passed to the Reasoning Agents (Flan-T5), which use this information to solve the sub-tasks (fact verification or question answering). This process of retrieving external information to inform and ground the reasoning and answer generation of the LLM agents aligns with the core principles of RAG.



",macro-F1 score,"Their key findings include:
•
LoCal significantly outperforms all baseline models across different settings of evidence availability on both HOVER and FEVEROUS datasets.
•
LoCal demonstrates enhanced fact-checking performance through improvements in logical and causal consistency.
•
The multi-agent framework shows superiority over traditional LLM-based methods.
•
LoCal effectively handles insufficient and irrelevant evidence due to its logical and causal evaluations.
•
LoCal is particularly beneficial for complex tasks with more errors in decomposition and reasoning.
•
Ablation experiments confirm the effectiveness of both the logical and counterfactual evaluating agents.
•
Error analysis reveals that LoCal can correct various types of errors, including decomposing errors being the most frequent.","Their main results, as shown in Table 1, indicate that LoCal achieves the best Macro-F1 scores in 7 out of 8 evaluation settings, surpassing state-of-the-art baselines like ProgramFC and PACAR in both gold evidence and open book scenarios","Impact on Reliability: Hallucinations contribute to incorrect responses for sub-tasks, leading to inaccurate or non-unique final veracity labels for the claims being checked.
Mitigation Strategies in LoCal:
External Evidence Retrieval: In its ""open-book"" setting, LoCal's reasoning agents utilize evidence retrieved from the Internet via the Pyserini toolkit. Grounding the reasoning in external evidence is a common strategy to reduce LLM hallucinations.


Evaluating Agents for Causal Consistency: The Counterfactually Evaluating Agent is specifically designed to enhance causal consistency by checking if the derived solution holds when challenged by an opposite (counterfactual) label. This process helps identify and correct causal errors, which can include those stemming from hallucinations during the reasoning for sub-tasks. If contradictions are found or if the solution is not robust to this challenge, it signals a potential causal flaw.




Iterative Refinement: The iterative nature of LoCal, where the decomposing, reasoning, and evaluating steps can be repeated if the evaluating agents do not accept the initial solution, allows for potential correction of errors that might have arisen from hallucinations in an earlier iteration","Prompt Design:
Decomposing Agent (gpt-3.5-turbo): Uses few-shot learning (prompted with 20 examples from the target dataset's training set) to learn how to break down complex claims into simpler sub-tasks (fact verification or question answering types) and generate a logical deduction function.


Reasoning Agents (Flan-T5) and Evaluating Agents (gpt-3.5-turbo): Operate in a zero-shot manner. The paper does not provide the exact prompts but describes their functions (e.g., the logically evaluating agent takes a summary description and outputs a new veracity; the counterfactually evaluating agent takes the summary and a negated veracity to check for conflicts).



The overall process is a multi-step reasoning approach, akin to a highly structured Chain-of-Thought, where decomposition precedes sub-task solving, which is then followed by evaluation and potential iteration.
Prompting Strategies with Integrated External Retrieval:
In the ""open-book"" setting, LoCal integrates external evidence retrieval. For each sub-task generated by the Decomposing Agent, the Pyserini toolkit is used to retrieve the top 5 relevant paragraphs from external knowledge bases (Wikipedia, in the context of these datasets). This retrieved evidence is then provided to the Reasoning Agents to solve the sub-tasks.

Fine-Tuning: The LLMs used in LoCal (Flan-T5 for reasoning, gpt-3.5-turbo for decomposing/evaluating) are not fine-tuned as part of the LoCal methodology. The framework leverages their existing capabilities through prompting.
Domain-Specific Training/Model Adaptation: The paper evaluates LoCal on general complex claim datasets (HOVER, FEVEROUS) rather than adapting it to highly specialized knowledge areas through specific training. The few-shot examples for the decomposer are taken from the same dataset being evaluated, providing some in-domain guidance for that specific task. The system's adaptability is more through its structural approach to reasoning and evidence integration rather than specific model training for a narrow domain.
","Retrieval: When gold evidence is not provided, for each sub-task identified by the Decomposing Agent, LoCal uses the Pyserini toolkit to retrieve relevant evidence (top 5 paragraphs) from external knowledge sources (e.g., Wikipedia, which is the basis for HOVER and FEVEROUS evidence).

Augmentation: This retrieved external evidence is then used to augment the context provided to the Reasoning Agents (Flan-T5).
Generation (Reasoning & Answering): The Reasoning Agents then utilize this augmented information (the retrieved evidence) to generate answers for the question-answering sub-tasks or determine the veracity of fact verification sub-tasks. The overall solution and final veracity are also generated based on these steps. This process of dynamically retrieving external information to inform and ground the LLM agents' reasoning and response generation for fact-checking sub-tasks aligns directly with the principles of RAG.",,"Yes, enhancing interpretability is a key goal and feature of the LoCal framework.





LoCal provides interpretability in two main ways:
Structured Solution: The framework generates a solution that connects the decomposed sub-tasks with the sequence of validations or answers for each sub-task. This creates a structured output consisting of multiple task-response pairs, which is reviewable and understandable by humans.




Detailed Evaluating Process: The logically evaluating agent provides an assessment of whether the solution is equivalent to the original claim, and the counterfactually evaluating agent explains how contradictions arise if the predicted label is flipped. This provides users with insight into the logical and causal validation steps.",
42,Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method,https://arxiv.org/abs/2310.00305,Conference,AACL,A,2023,November,Singapore, misinformation and news claim verification,RAWFC (based on Snopes fact-check articles with True/False/Half classification) and LIAR (based on PolitiFact articles with six classes,"The primary model used in this research is a Large Language Model (LLM) from the GPT-3.5 series, specifically text-davinci-003, accessed via the OpenAI API.
The paper proposes a novel prompting method called Hierarchical Step-by-Step (HiSS) prompting to direct this LLM for news claim verification.
The performance of HiSS-prompted LLM is compared against:



Seven strong supervised baselines: CNN, RNN, DeClarE, SentHAN, SBERT-FC, GenFE, and CofCED.
Other few-shot ICL methods for LLMs: Standard Prompting, Vanilla Chain-of-Thought (CoT) Prompting, Search-Augmented CoT Prompting, and ReAct Prompting.","The paper addresses the challenges of using LLMs for news claim verification, particularly the limitations of existing In-Context Learning (ICL) methods like vanilla Chain-of-Thought (CoT). The main problems it aims to solve are:

Omission of necessary thoughts: LLMs, when using vanilla CoT, may overlook crucial parts of a complex claim, leading to inaccurate decisions.
Fact hallucination: When LLMs lack necessary information, they might generate relevant but unreliable or false ""facts"", misleading the final prediction. The research seeks to enhance the performance and explainability of LLM-based fact verification by mitigating these issues.


","The core of the paper is the Hierarchical Step-by-Step (HiSS) prompting method, which guides an LLM (text-davinci-003) through a structured verification process using 4-shot demonstration examples. The HiSS method consists of three main processes:

Claim Decomposition (Level 1): The LLM is prompted to break down a complex input news claim into smaller, more manageable subclaims that cover all explicit check-worthy points. The number of subclaims is determined automatically by the LLM based on the claim's complexity and the demonstration examples.

Subclaim Step-by-Step Verification (Level 2): Each subclaim is verified individually. The LLM generates a series of probing questions for each subclaim to delve into implicit points. For each generated question: 

The LLM is prompted to state its confidence in answering the question (""yes"" or ""no"").
If the LLM is not confident (""no""), the question is used as a query for the Google Search API to retrieve external information. The top search snippet (after filtering results from fact-checking websites to avoid ground-truth leakage) is then fed back to the LLM to generate an answer.

If the LLM is confident (""yes""), it proceeds to generate the answer directly using its internal knowledge. This process is progressive, allowing subsequent questions to be adjusted based on previous answers.

Final Prediction: After all subclaims have been verified through the question-answering steps, the LLM makes a final prediction on the factuality of the original claim, selecting from the dataset's predefined label set.","the HiSS prompting method explicitly incorporates external knowledge retrieval, which is a core component of Retrieval Augmented Generation (RAG) systems.

During the ""Subclaim Step-by-Step Verification"" phase, if the LLM indicates a lack of confidence in answering one of its self-generated probing questions, the system resorts to an external search engine.

It uses the Google Search API to obtain relevant external information based on the probing question.

The top search result snippet (after filtering out direct fact-checking sites) is then provided as context to the LLM, which uses this retrieved information to formulate its answer to the probing question. This directly augments the LLM's internal knowledge with up-to-date external information to aid in verification and mitigate hallucination.","macro-average precision (P), recall (R), and F1 (F1)","LLMs using In-Context Learning (ICL) with only 4-shot demonstration examples can achieve performance comparable to, or even better than, previous supervised models for news claim verification.

Vanilla Chain-of-Thought (CoT) prompting can underperform standard prompting for news verification due to issues of ""omission of necessary thoughts"" and ""fact hallucination"".
The proposed HiSS prompting method significantly outperforms state-of-the-art fully-supervised approaches and strong few-shot ICL baselines (including standard prompting, vanilla CoT, Search-Augmented CoT, and ReAct).


Both claim decomposition and the step-by-step subclaim verification (including the web search mechanism) are important components contributing to HiSS's effectiveness, as shown by ablation studies.
Allowing the LLM to self-decide when to use external search based on its confidence is an effective strategy and performs only slightly worse than always searching, suggesting LLMs have a reasonable estimation of their own confidence.


HiSS-prompted LLMs generate explanations (reasoning trajectories) that are superior in terms of coverage and readability compared to a strong supervised explainable model (CofCED) and are rated highly in human evaluations, closely approaching human-written gold explanations.","Claim Verification Performance (Table 2):
On RAWFC, HiSS achieved an F1-score of 53.9%, outperforming the SoTA supervised model CofCED (52.0%) and other ICL methods like ReAct (49.8%).

On LIAR, HiSS achieved an F1-score of 37.5%, significantly outperforming CofCED (29.5%) and ReAct (31.0%).

Error Analysis (Table 3):
For HiSS, ""Fact Hallucination"" errors occurred in 5% of analyzed incorrect cases, and ""Thoughts Omission"" in 13%. This was substantially lower than for Vanilla CoT (43% hallucination, 60% omission) and ReAct (28% hallucination, 53% omission).
Ablation Study (Figure 3):
Removing claim decomposition from HiSS decreased F1 by 1.5% on RAWFC.
Removing step-by-step subclaim verification led to a 2.9% F1 drop.
HiSS without any search achieved an F1 of 49.8%; HiSS that always searched achieved an F1 slightly above 54.5% (estimated from graph, paper states 54.4% for self-decide is 1.0% less than always search); HiSS with self-decided search achieved 54.4% F1.
Human Evaluation of Explanations (Table 4):
On RAWFC, HiSS explanations received an average ""Overall"" quality score of 2.54, compared to 2.69 for Gold (human-written) explanations and 1.74 for CofCED. For ""Coverage,"" HiSS (2.63) was on par with Gold (2.65).","The paper identifies ""Fact hallucination"" as one of the two main issues causing the failure of vanilla Chain-of-Thought (CoT) prompting in news claim verification. This occurs when the LLM generates relevant but unreliable or factually incorrect statements, especially when necessary information is not available to it.

Mitigation Strategies for LLM Hallucinations:
The core strategy implemented in the HiSS prompting method is to provide LLMs with access to up-to-date external information via a search engine (Google Search API). This is triggered when the LLM itself indicates a lack of confidence in answering a probing question it generated. By grounding the LLM's reasoning in retrieved external evidence, HiSS aims to reduce fact hallucination.

Recent Innovations for Reducing Hallucinations and Improving Factuality:
The HiSS method itself, particularly its mechanism of decomposing claims and performing step-by-step verification with conditional external knowledge retrieval, is presented as an innovation to improve factuality.
The error analysis (Table 3) demonstrates that HiSS significantly reduces the incidence of fact hallucination (5% of errors) compared to Vanilla CoT (43%) and ReAct (28%) in the analyzed samples.

","The paper's core contribution is the Hierarchical Step-by-Step (HiSS) prompting method, which is a sophisticated few-shot (4-shot) In-Context Learning (ICL) technique designed for LLMs.

Prompting Techniques:
HiSS directs the LLM (text-davinci-003) to first decompose a complex claim into simpler subclaims.
Then, for each subclaim, it performs step-by-step verification by prompting the LLM to generate a series of probing questions and then answer them. This multi-step reasoning is a form of structured Chain-of-Thought (CoT).

The prompts include explicit instructions for the LLM to assess its confidence in answering each question and to signal when external information is needed. Demonstration examples (Table 8 (a) and 8 (b) in Appendix B) illustrate the entire process to the LLM.
Prompting Strategies with Integrated External Retrieval:
A key part of HiSS is its dynamic integration of external retrieval. When the LLM expresses low confidence in answering a question, that question is used as a query to the Google Search API. The top retrieved snippet is then incorporated into the ongoing prompt to help the LLM generate an answer.


Fine-Tuning: The paper explicitly states that its approach uses LLMs with ICL without any gradient updates or fine-tuning of the LLM parameters. The goal is to leverage the existing capabilities of pre-trained LLMs through effective prompting.
Domain-Specific Training/Model Adaptation: The HiSS method is presented as a general approach for news claim verification. While evaluated on news datasets, the paper does not discuss specific adaptations or training for specialized knowledge areas beyond the general strategy of using external search for up-to-date information. The few-shot examples are drawn from the same domain as the test claims (news).","the HiSS prompting method directly integrates principles of Retrieval-Augmented Generation (RAG) into its fact-checking process.

Retrieval: During the subclaim verification phase, if the LLM determines it lacks confidence to answer a self-generated probing question, the system initiates a retrieval step. It uses the generated question as a query to an external knowledge source, specifically the Google Search API.

Augmentation: The top search result (snippet) obtained from Google Search is then taken and inserted back into the ongoing prompt as additional context for the LLM. This retrieved information augments the LLM's internal knowledge.


Generation (Reasoning/Answering): With this augmented context, the LLM then proceeds to generate an answer to the probing question. This answer, based on the retrieved evidence, contributes to the step-by-step verification of the subclaim and ultimately the final verdict on the original news claim. This iterative process of generating questions (queries), retrieving external information when needed, and then using that information to generate answers/reasoning steps is a clear application of RAG methodology to improve the accuracy and reduce hallucination in the fact verification task.",,"the paper emphasizes explainability as a key advantage of the HiSS prompting method.


The HiSS method inherently generates a reasoning trajectory by decomposing the claim into subclaims and then verifying each subclaim through a series of explicit question-answer steps. This step-by-step process makes the LLM's decision-making process more transparent and easier for humans to follow.
The generated explanations (the entire reasoning process including decomposition, questions, and answers) were evaluated by human judges.
Results from the human evaluation showed that HiSS explanations are more fine-grained and offer superior coverage and readability compared to explanations from a state-of-the-art supervised explainable model (CofCED), and are comparable in quality to human-written gold justifications.",https://github.com/jadeCurl/HiS
43,"AI-agent-based system for fact-checking support using
large language models",https://ceur-ws.org/Vol-3917/paper50.pdf,Conference,CEUR,Unranked,2025,February,Ukraine, fact-checking and disinformation detection,"The dataset they used for evaluation involves real-world user queries tested on the developed system to assess its ability to verify or refute claims by checking against a database and web resources. The paper does not explicitly mention using specific publicly available fact-checking datasets like Snopes, PolitiFact, or GossipCop for a quantitative performance evaluation against baselines, unlike the previous paper we discussed.",OpenAI's GPT-4o ,"The problem they solved is the increasing prevalence and impact of disinformation spread through rapid information dissemination, particularly on social media, and the need to automate the fact-checking process to improve efficiency and accuracy.","Their methodology proposes an AI-agent-based system architecture for fact-checking support. This system comprises several key modules: a user interface (UI), a user request processing module, a database of reliable sources, a module for working with web resources, the LLM (GPT-4o), and a results analysis module. When a user submits a query, the system first checks the database for previously verified information. If not found, it uses the web resources module to search for relevant data on the internet. The LLM then analyzes the retrieved information to determine the reliability of the claim. Finally, the results analysis module generates a report for the user.","Yes, the system explicitly uses a Retrieval Augmented Generation (RAG) approach and external knowledge.

The paper states, ""The vector database with a large language model implements the so-called RAG system."". This suggests that the local database is a vector database used in conjunction with the LLM for retrieval.
For information not found in the local database, the system queries external web resources. The LLM then analyzes data extracted from these internet searches to make its decision.",The paper presents tables showing the technical characteristics of various LLMs and figures illustrating the user interface and test queries with their result,"LLMs possess significant capabilities for automating fact-checking, including text analysis, comparison with reliable sources, and contextualization.

Despite their utility, LLMs can also be exploited to create sophisticated fake news.

The proposed AI-agent-based architecture aims to improve fact-checking efficiency and accuracy.
Experimental testing of the developed tool indicated it functions correctly and generally meets the specified requirements for information verification.
The tool achieved a 90% accuracy rate in the conducted experiments.
Errors in the system were attributed to LLM ""hallucinations"" and the retrieval of suboptimal sources for analysis by the LLM.
The system sometimes required a second attempt to provide the correct answer for a query, underscoring the need for human oversight.","The system correctly identified a fake news item about ""Cossack Mykhailo Havryliuk"" by matching a modified query to a record in its database.
For queries requiring web searches, the system correctly refuted claims about ""electricity blackouts in Ukraine"" and ""English becoming the second official language in Ukraine,"" providing conclusions and links to supporting sources.

In one instance involving a query about the ""Kyiv Regional CCC's drawing of mobilization armor,"" the program initially gave a false answer but corrected it upon a second request. This was compared against a refutation by the Gvara Media fact-checking organization.

Another erroneous result occurred regarding ""Zelenskyy announcing a second peace summit,"" which was later corrected on a second attempt; this error was attributed to LLM ""hallucinations"" or poor source selection.



The overall accuracy of the tool in these tests was reported as 90%.
","Hallucinations in Large Language Models: The authors state that errors in their system ""occurred due to the language model's 'hallucinations' and because it sometimes received not the best sources for analysis"".
One specific instance of an incorrect program response (Figure 10) was followed by the remark, ""This once again emphasizes the need for human control over the work of language models, as they can experience so-called 'hallucinations'."".
Mitigation Strategies for LLM Hallucinations: The paper does not explicitly propose novel or dedicated strategies to mitigate hallucinations within the LLM itself. However, the system's architecture, which involves checking a local verified facts database first and then using web searches to gather external information before the LLM makes a final analysis, can be seen as an implicit way to ground the LLM and reduce reliance on its internal knowledge alone, thereby potentially reducing the impact of hallucinations. The RAG approach itself is a form of mitigation.


Recent Innovations: The paper focuses on applying existing LLM (GPT-4o) capabilities within its proposed agentic architecture rather than introducing new techniques specifically for reducing LLM hallucinations.","The paper describes an AI-agent system where the LLM performs various roles, but it does not delve into the specifics of prompt engineering (e.g., detailed structure of prompts for zero-shot, few-shot, or CoT) used to instruct the GPT-4o model for its different tasks like query analysis, database search, or web search query generation.

The system operates by giving the LLM tasks within a larger workflow: parsing user queries, searching a local database, forming web search queries if needed, and analyzing retrieved information to make a veracity judgment. This can be considered an agentic use of the LLM.
Fine-Tuning: There is no mention of fine-tuning the GPT-4o model. The system relies on the pre-trained capabilities of the LLM.
Prompting Strategies with Integrated External Retrieval: The methodology inherently involves external retrieval. If information is not in the local database, the LLM helps formulate queries for web search (using tools like Google Search and serpapi), and then the LLM analyzes the retrieved results.


Domain-Specific Training/Adaptation: The paper does not discuss domain-specific training. The adaptability is suggested through the system's ability to process different types of queries and use web search for broad information access. The user interface allows for various queries, making it flexible.","Yes, the paper explicitly states that its system integrates Retrieval-Augmented Generation (RAG).

The authors mention: ""The vector database with a large language model implements the so-called RAG system."".
The system's workflow for fact-checking involves:
Retrieval: First, an attempt is made to retrieve relevant information from a local (vector) database of verified facts. If the information is not found or is insufficient, the system retrieves data from external web resources.




Augmentation & Generation (Analysis/Verdict Generation): The retrieved information (from the database or the web) is then used by the LLM to analyze the user's query and ""make a decision on its reliability, determining whether the information is fake."". The ""results analysis module"" then processes this LLM output to generate reports and recommendations. This process of retrieving information from a knowledge source (local DB or web) and then using that information to inform the LLM's analysis and final output (the veracity verdict and report) is characteristic of a RAG system.",,"The system architecture includes a ""results analysis module"" that processes data obtained from the LLM and web resources, generates reports, and provides recommendations on the reliability of information to the user.

For queries involving web searches, the system provides links to the sources used to support its conclusions, as shown in the experimental results (e.g., Figures 6, 7, 8, 10, 11). This allows users to inspect the evidence.




The architecture (Figure 1) shows ""Report and recommendations"" being provided to the UI, implying user-facing explanations",
44,"Large Language Model Agent for Fake News Detection
(Not published but in openreview)",https://arxiv.org/abs/2405.01593,Preprints,arXiv,Preprint,2024,April,USA,fake news detection,"Snopes, PolitiFact, and GossipCop", gpt-3.5-turbo,"Develop a fake news detection system (FactAgent) that emulates human expert behavior without requiring model training or annotated data.

Enhance the efficiency of fake news detection compared to manual verification.

Provide transparent explanations for the detection process.

Create an adaptable system applicable across various news domains.

","Their methodology introduces FactAgent, an agentic approach that utilizes LLMs for fake news detection. This involves a structured expert workflow that breaks down the complex task of news veracity checking into multiple sub-steps. In each sub-step, LLMs use their internal knowledge or external tools to complete simple tasks. The tools are categorized into those utilizing only the LLM's internal knowledge (Phrase, Language, Commonsense, Standing tools) and those integrating external knowledge (URL and Search tools). The workflow is designed based on domain knowledge and emulates human expert behavior in verifying news claims. Finally, the LLM integrates all findings to determine the news claim's veracity and provides explanations for its reasoning."," FactAgent explicitly uses external knowledge and incorporates principles similar to Retrieval-Augmented Generation (RAG), though it focuses on verification rather than generation of new content.

The Search_tool leverages the SerpApi to search for conflicting information from other online media resources. This external search is intended to augment the LLM's internal knowledge and help mitigate hallucination issues.

The URL_tool uses LLM internal knowledge initially but then augments this by checking against an external database containing URLs previously verified as sources of real or fake news. This database can be updated to ensure timeliness. The workflow integrates these pieces of retrieved external evidence in its final decision-making step.","•
Accuracy
•
F1 score
•
F1 score for real news (F1_real)
•
F1 score for fake news (F1_fake)","FactAgent, following a structured expert workflow, achieves superior performance compared to supervised baselines (LSTM, TextCNN, BERT), other LLM-based methods (HiSS, zero-shot standard prompt, zero-shot CoT), and LLMs using individual tools in isolation.

The design of the expert workflow using domain knowledge is critical. An LLM automatically self-designing its workflow can lead to inferior performance, particularly if it misprioritizes tools (e.g., overusing political bias detection for non-political news or underutilizing URL checks).


The external Search_tool plays a significant role in FactAgent's performance; excluding it leads to deterioration. Relying solely on external search is also suboptimal.

For final decision-making, allowing the LLM to reason based on a checklist of collected evidence is more effective than a simple majority vote from individual tool outputs.

FactAgent's approach of explicitly guiding LLM reasoning from specific perspectives (via tools) is more effective than standard or CoT prompting alone.","PolitiFact: FactAgent achieved an F1 score of 0.88 and accuracy of 0.88.
GossipCop: FactAgent achieved an F1 score of 0.83 and accuracy of 0.83.
Snopes: FactAgent achieved an F1 score of 0.75 and accuracy of 0.75. These results were generally superior to all baseline methods","Mitigation Strategies for LLM Hallucinations:
FactAgent's methodology incorporates the use of external tools, specifically the Search_tool, to mitigate LLM hallucination. This tool uses SerpApi to search for conflicting information reported by other media resources, allowing the system to cross-reference and verify the news claim with external knowledge rather than relying solely on the LLM's internal knowledge.

The URL_tool also integrates external knowledge by checking news claims against a database of URLs previously verified for sourcing real or fake news, which can be updated for timely accuracy.","Prompt Design:
LLMs are prompted to act as ""agents"" that utilize specific ""tools"" (e.g., Phrase_tool, Search_tool). Each tool has a defined function, and the LLM is instructed to use these tools sequentially as per the expert workflow (Figure 1) or a self-designed workflow (Figure 2).
The LLM is also prompted at the final stage to compare collected findings against a checklist to predict veracity.
The paper explicitly compares FactAgent to zero-shot standard prompts and zero-shot CoT prompts, where FactAgent shows superior performance, highlighting the benefit of its structured, tool-based agentic approach over direct, single-shot prompting.

Prompting Strategies with Integrated External Retrieval:
The Search_tool uses SerpApi to retrieve external information (conflicting reports) based on the news topic.

The URL_tool also uses an external database of verified URLs.

The information retrieved by these tools is then used by the LLM in its reasoning process within the workflow.

Fine-Tuning: FactAgent does not require any fine-tuning of the LLM. This is a key advantage highlighted by the authors.

Domain-Specific Training/Adaptation:
FactAgent is designed to be highly adaptable to various news domains. This adaptation is achieved not through training, but by allowing for straightforward updates to its tools or modifications to the workflow itself using domain knowledge.



For instance, the Standing_tool is specifically designed for political news and is skipped if the LLM determines the news is not political. The paper demonstrates flexibility by adjusting tools for the Snopes dataset due to the unavailability of URLs.


The importance of domain knowledge in designing the expert workflow is emphasized, as experiments show that an expert-designed workflow tailored to the dataset's characteristics generally performs better than an LLM's self-designed workflow.","Retrieval: FactAgent actively retrieves external information using:
The Search_tool, which employs the SerpApi to search for related news articles and identify conflicting reports from the web.
The URL_tool, which accesses an external database of domain URLs previously verified for credibility.

Augmentation & Generation (Reasoning):
The information retrieved from these external sources serves as ""evidence"" or ""findings"".

This retrieved evidence is then integrated into the LLM's reasoning process. The LLM is tasked to ""summarize your findings from internet searches"" for the Search_tool  and to ""compare all findings against a checklist to predict the credibility of a news"" in the final step.


The LLM generates an analysis for each tool and a final verification decision along with reasoning. This process of retrieving external information to inform and ground the LLM's subsequent reasoning and decision-making aligns well with the core concept of RAG. The system uses the retrieved data to augment the LLM's internal knowledge for a more robust and factually grounded verification.",,"It offers transparent explanations at each step of its structured workflow and during the final decision-making process.
This enhances interpretability and helps end-users understand the reasoning behind a fake news detection verdict.
Figure 8 provides a case study demonstrating this: the LLM outputs explicit observations from each utilized tool in natural language and then provides reasoning by comparing these observations with a checklist to arrive at a final conclusion.
This contrasts with supervised models that may lack such transparent decision pathways. The process can also educate users on what aspects to scrutinize when evaluating news.",
45,FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models,https://arxiv.org/abs/2502.17924,Preprints,ACL,A*,2025,July,China," Natural Language Processing (NLP), specifically focusing on fact-checking evaluation of Large Language Models (LLMs). It addresses the challenges in systematically assessing the fact-checking capabilities of LLMs",,"FACT-AUDIT, to evaluate target LLMs. The framework itself utilizes LLM-powered autonomous agents for various tasks within the evaluation process. The experiments were conducted on 13 state-of-the-art LLMs, including both open-source models like Mistral-7B, Llama2 (7B, 13B), Llama3 (8B), Llama3.1 (8B, 70B), Qwen2.5 (7B, 72B), GLM4 (9B), Gemma2 (9B), and proprietary models like Gemini-Pro, Claude3.5-Sonnet, and GPT-4o. The agent controllers within FACT-AUDIT primarily utilize GPT-4o for tasks such as evaluating the target LLM's responses and ensuring the quality of generated data","The paper addresses the limitations of existing automated fact-checking evaluation methods for LLMs. These limitations include:
•
Reliance on static datasets that are costly to scale and prone to issues like test data leakage and leaderboard swamping.
•
Oversimplification of evaluation to a classification paradigm focusing only on accuracy, neglecting other critical capabilities like justification production.
•
Failure to automatically evaluate the justification production and uncover nuanced limitations of LLMs in fact-checking.
•
Lack of methods to adaptively and dynamically assess LLMs' evolving fact-checking capabilities.","The paper proposes FACT-AUDIT, an agent-driven framework for adaptive and dynamic assessment of LLMs' fact-checking capabilities. The core design involves:
•
Dynamically updated fact-checking test data: This is achieved through an iterative process involving multiple agents.
◦
Appraiser agent: Develops a taxonomy of fact-checking scenarios and updates it based on the target LLM's performance.
◦
Inquirer agent: Generates prototype test data (source claim, auxiliary information, key point, test mode) according to the taxonomy.
◦
Quality Inspector agent: Ensures the quality and diversity of the generated test data using external tools and LLMs.
◦
Prober agent: Iteratively generates more diverse and unseen test cases under each scenario based on the target model's past performance stored in a memory pool, using importance sampling principles.
•
In-depth evaluation of model-generated justifications: The framework evaluates both the fact verification (verdict prediction) and the justification provided by the target LLM.
◦
Evaluator agent: Scores the target LLM's verdict and justification using LLM-as-a-Judge (primarily GPT-4o), providing a rating grade and a natural language assessment.
•
Adaptive Auditing Process: FACT-AUDIT operates in a loop where the taxonomy of fact-checking scenarios is updated based on the target LLM's weaknesses revealed during evaluation. This allows the framework to focus on challenging areas for the specific LLM being audited.
•
Importance Sampling: The framework leverages the concept of importance sampling to adaptively and efficiently sample test data according to the fact-checking limits of the LLM, focusing on areas where the LLM is likely to underperform.",No,,Insight Mastery Rate (IMR) Justification Flaw Rate (JFR) Grade,"The extensive experiments on 13 state-of-the-art LLMs revealed several key findings:
•
There is a notable performance gap between closed and open-source models, with GPT-4o, Qwen2.5-72B, Claude3.5-Sonnet, and Gemini-Pro forming the leading tier. GPT-4o achieved the best IMR.
•
The LLaMA series generally exhibited relatively poorer performance.
•
The JFR metric suggests that FACT-AUDIT can elicit specific fact-checking limitations of individual target LLMs based on their aptitudes, as strong models might have more low-scoring cases due to poor justifications despite correct verdicts.
•
LLMs generally perform relatively well on fake news but struggle with complex claims, potentially due to the higher reasoning demands of complex claims. Performance on social rumors was fluctuating due to contextual dependence and linguistic complexity.
•
The adaptive updating process of FACT-AUDIT can discover new challenging test scenarios that were not part of the initial taxonomy.
•
Iterative probing helps in comprehensively evaluating model performance and identifying less obvious weaknesses as the test data expands.","The term ""hallucinations"" is not explicitly used in the provided text. However, the source mentions that LLMs struggle with identifying factual errors and are prone to reasoning mistakes, which limits their credibility in fact-checking. Errors in stored knowledge or deficiencies in fact reasoning capabilities can affect their reliability. FACT-AUDIT addresses these issues by evaluating justification production alongside verdict prediction to reveal limitations. The JFR metric specifically measures cases of correct verdicts with poor justifications, indicating potential underlying issues. The case study illustrates a factual error in a GPT-4o justification (incorrect unit conversion) despite a correct verdict. The proposed strategy within this paper's scope is the adaptive, multi-agent evaluation framework focusing on identifying these justification issues. Future work plans to incorporate RAG to enhance accuracy by accessing up-to-date information. The source does not discuss other external mitigation strategies.","The source provides extensive details on the prompt design used for its various agents (Appraiser, Inquirer, Quality Inspector, Evaluator, Prober). These prompts define the tasks, constraints, and evaluation criteria, directly influencing the generation of test cases and the assessment of LLM responses. The quality and structure of these prompts are crucial for the framework's operation. Fine-tuning architectures are not discussed in the provided text; the evaluation is conducted on existing, pre-trained LLMs in a zero-shot setting. The source evaluates LLMs across different fact-checking objects (domains) like Complex Claims, Fake News, and Social Rumors, and also discusses performance on diverse topics (Politics, Finance, Law, Healthcare, etc.). The results show varied performance across these domains/topics. This variability is attributed to potential differences in the volume and quality of training data or the model's ability to process specific types of information within these domains, suggesting domain relevance in training data does affect performance/accuracy. The framework evaluates both accuracy (via Grade and IMR) and aspects related to interpretability (via justification evaluation, JFR, and evaluation comments). Poor justification impacts the assigned grade and can make the response less reliable/interpretable, as highlighted in the case studies.",,,, https://github.com/DanielLin97/FACT-AUDIT
46,When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits,https://arxiv.org/abs/2503.03417,Preprints,ACL-25,A*,2025,July,UK,robustness of embedding models against misinformation edits in the context of online fact-checking and claim matching,"For applying perturbations and evaluating robustness:
CheckThat22: Matches tweets with short, verified claims. Perturbations are applied to the test split.

FactCheckTweet: Matches claims with full fact-check articles. Perturbations are applied to a 10% test split created by the authors.


For out-of-domain (OOD) generalization testing of mitigation approaches:
OOD Dataset: Composed of novel claim-fact-check pairs provided by Meedan, sourced from fact-checking organizations. This entire dataset is used for evaluation.


For training/fine-tuning mitigation approaches:
The CheckThat22 training set (1,195 claims) is used for:
Fine-tuning embedding models (all-mpnet-base-v2 and sentence-t5-large) using Multiple Negatives Ranking (MNR) loss.


Generating parallel sentences for Knowledge Distillation (KD). Initially 11,593 unperturbed-perturbed pairs were generated, then expanded to 70,954 pairs by pairing different perturbations of the same claim.","Perturbation Generation:
A two-LLM system: an ""LLM As a Perturber"" and an ""LLM As a Verifier"". Both roles are fulfilled by GPT-4o.


Embedding Models (Retrievers) Evaluated: A wide range of models are assessed:
BERT-based: all-MiniLM-L12-v2, all-mpnet-base-v2, all-distilroberta-v1.
T5-based: sentence-t5-base, sentence-t5-large, instructor-base, instructor-large.
Decoder-only LLM-based: SFR-Embedding-Mistral, NV-Embed-v2 (both derived from Mistral-7B).

Fine-tuned versions: all-mpnet-base-v2-ft and sentence-t5-large-ft (fine-tuned on CheckThat22).
Lexical Baseline: BM25.
Reranker Model:
The primary reranker used in experiments is bge-reranker-v2-gemma. This was selected after evaluating seven rerankers, where RankGPT using GPT-4o performed best but bge-reranker-v2-gemma offered comparable accuracy at a lower cost.


Mitigation Approach Models:
Knowledge Distillation (KD): The student model is all-mpnet-base-v2, and the teacher model is also all-mpnet-base-v2.
Claim Normalization (CN): GPT-4o is used as the claim normalizer.
","The paper primarily addresses the following:

It evaluates the robustness of sentence embedding models, crucial for claim matching systems, when faced with real-world misinformation edits. Users often alter claims (e.g., changing entities, using slang, switching dialects) before sharing them, and these edits can degrade the performance of systems designed to retrieve existing fact-checks.



It aims to identify weaknesses in current embedding models and reranking strategies when handling these edited claims.

It proposes and evaluates mitigation approaches (both train-time and inference-time) to enhance the robustness of these embedding models, especially computationally efficient ones, against such edits and to improve their generalization to out-of-domain data.","Taxonomy of Edits: Introduces a taxonomy of six common real-world misinformation edits: Casing, Typos, Negation, Entity Replacement, LLM rewrites, and Dialect Changes.

Perturbation Generation Framework: A two-stage framework using LLMs to generate claim variations. An ""LLM as a Perturber"" creates candidate rewrites, and an ""LLM as a Verifier"" checks if these candidates are valid (preserve the original claim's meaning and applicability to the fact-check) and natural.

Multi-Stage Retrieval Evaluation: Evaluates the robustness of various embedding models (first-stage retrievers) by measuring performance on original versus perturbed claims within a retrieval pipeline that includes a strong reranker (bge-reranker-v2-gemma). Performance is assessed using retrieval gaps.



Mitigation Approaches:
Knowledge Distillation (KD): A train-time approach where a student embedding model (all-mpnet-base-v2) is trained to map both original and perturbed versions of a claim to the same embedding space as a teacher model (also all-mpnet-base-v2). This uses synthetically generated unperturbed-perturbed claim pairs.


Claim Normalization (CN): An inference-time approach where an LLM (GPT-4o) rewrites perturbed input claims into a standard form before they are embedded.
Fine-tuning: Standard task-specific fine-tuning of embedding models on the CheckThat22 training set using contrastive learning (MNR loss).


These mitigations are evaluated for in-domain robustness and out-of-domain generalization.","The paper focuses on the retrieval aspect of claim matching, which is a core component of RAG systems. Specifically, the task is to retrieve relevant, previously existing fact-checks from a database given an input claim. The multi-stage pipeline involves a retriever (embedding model) to select candidate fact-checks and a reranker to refine this selection.





While the system retrieves external knowledge (fact-checks from a database ), it does not explicitly detail a ""generation"" component where an LLM synthesizes an answer or explanation based on the retrieved fact-checks. The end goal is to rank and present the most relevant existing fact-check. Thus, it heavily emphasizes the ""R"" (Retrieval) and evaluation of that retrieval robustness, rather than a full RAG generative loop","Mean Reciprocal Rank (MRR).
•
Mean Average Precision truncated to rank k (MAP@k)Perturbation Generation Framework Evaluation (Human Evaluation):
Perturbation Accuracy: Proportion of instances where the LLM as a Perturber correctly applied the intended edit.

Validity Precision, Recall, F1: For the LLM as a Verifier in identifying valid and natural perturbations.


Lexical Similarity Metrics (for characterizing perturbations):
ROUGE F1 scores (R1, R2, RL).
Normalized Levenshtein distance","Standard embedding models are vulnerable: Common embedding models (BERT-based, T5-based) struggle with user-introduced misinformation edits, showing significant performance drops.


LLM-distilled embeddings are more robust: Embeddings derived from larger language models like Mistral-7B (e.g., SFR-Embedding-Mistral, NV-Embed-V2) exhibit superior robustness to these edits, likely due to their increased representational capacity and training on diverse data. However, they are computationally more expensive.


Rerankers offer partial recovery: A strong reranker can help mitigate some of the retrieval performance loss caused by perturbations, especially for weaker embedding models, but it cannot fully compensate for the gaps created in the first-stage retrieval. In some cases, for stronger models on short-target tasks, reranking a large set of candidates can even negatively impact performance.



Specific model weaknesses:
NLI-finetuned embedding models (e.g., Sentence-T5) show low robustness to negation edits.
Models with case-sensitive tokenizers struggle significantly with casing variations.
Mitigation strategies are effective:
Train-time (Knowledge Distillation, fine-tuning) and inference-time (Claim Normalization) mitigation approaches can substantially improve the robustness of weaker, computationally efficient embedding models.

These improvements generalize to out-of-domain datasets, enhancing performance by up to 10 percentage points over baselines.

Combining multiple mitigation approaches often yields the largest improvements.
LLM Rewrites can improve performance: When the perturbation is an LLM rewriting the claim, it can sometimes act as a form of query expansion, leading to improved retrieval performance.","Overall Improvement from Mitigations: Train- and inference-time mitigation approaches enhance in-domain robustness by up to 17 percentage points and boost out-of-domain generalization by 10 percentage points over baseline models.
Performance Gaps: Table 2 details the $\Delta$retrieval gap, $\Delta$recovery gap, and $\Delta$overall gap for various embedding models on the CheckThat22 dataset under 14 different perturbation types (MAP@20). For instance, BERT-based models like all-distilroberta-v1 show significant negative gaps across many perturbations, while NV-Embed-v2 generally has smaller gaps.

Mitigation Effects (In-Domain): Table 3 shows that for all-mpnet-base-v2 on CheckThat22 (MAP@20), combining fine-tuning and claim normalization (mpnet-ft+CN) often yields the best performance on perturbed sets for typos, negation, entity replacement, and dialect edits. For example, on Pidgin dialect edits, the baseline mpnet drops to 0.65 MAP@20, while mpnet-robust-ft+CN achieves 0.87.


Mitigation Effects (Out-of-Domain): Table 4 shows results on the OOD Dataset. While all models experience a performance drop, mitigation approaches still improve scores over the baseline all-mpnet-base-v2. The combination mpnet-ft+CN achieves the best overall MAP@20 of 0.6213, compared to the baseline's 0.5254.

Perturbation Framework Human Evaluation: The LLM as a Perturber achieved 96.70% accuracy in applying edits, and the LLM as a Verifier achieved 100% precision in selecting valid perturbations, though with a lower recall of 55.67%.

",,"Prompt Design:

Perturbation Generation: A two-stage, multi-prompting framework leverages GPT-4o for perturbing input claims and verifying them. 

The ""LLM As a Perturber"" takes an input claim and its fact-check to generate candidate rewrites based on one of the six edit types (e.g., Dialect, Typos, Entity Replacement). Prompts are designed to ensure the edits are valid and natural (an example prompt for Dialect Perturbation is provided in Appendix §I). Temperature is set to 0.9 for diverse generations.


The ""LLM As a Verifier"" checks if these generated rewrites adhere to constraints (applicability of fact-check, same main claim, naturalness) using a binary label. Temperature is 0 for reproducibility. An example verification prompt for Dialect edits is provided.

Claim Normalization (CN): An LLM (GPT-4o) is prompted to rewrite noisy input claims into a more formal and standardized version while preserving the original meaning, before the claim is embedded. An example prompt is provided. Temperature is 0 for this task.    Fine-Tuning:

Specific embedding models (all-mpnet-base-v2 and sentence-t5-large) are fine-tuned on the CheckThat22 training set.

The fine-tuning uses contrastive learning with Multiple Negatives Ranking (MNR) loss.
Hard negatives are introduced by using BM25 to select top-ranked negative fact-checks for the training process.
Knowledge Distillation (KD): This is a train-time mitigation.

A student model (all-mpnet-base-v2) is trained to make its embeddings of perturbed claims similar to the teacher model's (all-mpnet-base-v2) embeddings of the original claims.

This uses a dataset of 70,954 parallel unperturbed-perturbed claim pairs generated from the CheckThat22 training set.
Domain-Specific Training: The fine-tuning and KD are performed on in-domain data (CheckThat22). The effectiveness of these domain-specific adaptations is then also tested for out-of-domain generalization on the OOD Dataset. The perturbations (like dialect changes) also inherently test adaptation to variations often found in specific online communities or domains.","The paper's primary focus is on the robustness of the retrieval component in claim matching systems, which is a foundational part of RAG. The goal is to accurately retrieve a pre-existing fact-check that matches an input claim, even if the claim has been edited.




The methodology involves:

Retrieval: Using various sentence embedding models to fetch candidate fact-checks from a database.

Reranking: Using a more powerful model to refine the ranking of these retrieved candidates.

While this is a multi-stage retrieval pipeline, the paper does not explicitly describe or evaluate a ""Generation"" component where an LLM would, for instance, synthesize an answer, explain the fact-check, or generate a summary",,"The taxonomy of real-world edits provides a structured way to understand the types of linguistic variations that challenge embedding models.
By testing specific, interpretable perturbations, the study reveals specific vulnerabilities (e.g., case-sensitivity, issues with negation for NLI-trained models). This helps in diagnosing why models might fail, which is a step towards explainability.

The perturbation generation framework itself is designed to produce ""valid"" and ""natural"" edits, making the evaluation scenarios more interpretable compared to purely adversarial or synthetic noise.

",
47,BiDeV: Bilateral Defusing Verification for Complex Claim Fact-Checking,https://arxiv.org/abs/2502.16181,Conference,AAAI2025,A*,2025,August,China,complex claim fact-checking and disinformation detection,The paper uses two widely used challenging fact-checking benchmarks: Hover (Jiang et al. 2020) and Feverous-s (Pan et al. 2023),"The proposed framework, Bilateral Defusing Verification (BiDeV), integrates multiple role-played Large Language Models (LLMs). Specifically, they use gpt-3.5-turbo as the base model for the Perceptor (Mp), Rewriter (Mr), Decomposer (Md), and Filter (Mf). For the Querier (Mq) and Checker (Mc), they leverage Flan-T5-XL (3B) without additional fine-tuning. They also compare BiDeV against various baselines, including BERT-FC, LisT5, RoBERTa-NLI, DeBERTaV3-NLI, MULTIVERS, FLAN-T5, Codex, HiSS, FOLK, ProgramFC, and FactcheckGPT","Claim Vagueness: This includes latent information (unresolved entities, undetermined attributes) and complex relations (referential and comparative relations) that make claims difficult to understand and verify.

Evidence Redundancy: This refers to the presence of extensive, irrelevant, or noisy information within retrieved evidence documents, which complicates the verification process and can distract the model.


","Vagueness Defusing (VD): This module simplifies complex claims in a two-stage process.


Stage 1: Perceive-then-Rewrite: An iterative process where a Perceptor LLM identifies latent information and generates questions, a Querier LLM answers these questions using evidence, and a Rewriter LLM incorporates the answers to clarify the claim. This is repeated for 3 iterations.



Stage 2: Decompose-then-Check: A Decomposer LLM breaks the simplified claim into simple, declarative sub-claims, resolving complex relations. A Checker LLM then verifies each sub-claim against the evidence to reach a final verdict.


Redundancy Defusing (RD): This module improves evidence quality using a Filter LLM. It segments the retrieved evidence into paragraphs and eliminates irrelevant ones to provide more precise and pertinent information for the Querier and Checker.","Augmented Generation (RAG) system, especially in its ""open setting"".


The framework first retrieves coarse-grained evidence from an external source like Wikipedia.
The Redundancy Defusing module then acts as an advanced filtering step on this retrieved information to improve its quality.

This filtered, pertinent evidence is then used to augment the context for the Querier and Checker LLMs, which then perform their reasoning and generation tasks (answering questions and verifying claims)",Macro-F1 as the evaluation metric to assess the performance of their method and compare it with baselines. They chose Macro-F1 to better handle unbalanced proportions between support and refute samples.,"BiDeV is Effective: The proposed BiDeV framework achieves state-of-the-art performance on both benchmarks in both gold and open settings, significantly outperforming 11 different baselines without any model training.
Modules are Crucial: The Vagueness Defusing module is effective at simplifying claims and reducing the complexity of fact-checking, while the Redundancy Defusing module successfully filters evidence to improve its quality. Ablation studies showed that the Perceptor (for asking questions about latent information) had the most impact on performance.

Superior on Complex Claims: BiDeV shows remarkable improvements on more complex claims (e.g., 3-hop and 4-hop), where the performance of other methods tends to drop significantly.

Handles Redundancy Better: Unlike baseline methods whose performance degrades when too much evidence is retrieved, BiDeV shows consistent improvement as more evidence becomes available, highlighting the effectiveness of its Redundancy Defusing filter.
","The paper presents quantitative results in Table 1, showing the Macro-F1 scores of BiDeV and various baselines on the Hover and Feverous-s datasets under both gold and open settings. They report significant improvements (p < 0.05) for BiDeV. Figures 3, 4, 5, and 6 further illustrate the results of their ablation studies and analyses of redundancy and vagueness defusing, as well as the impact of model scale and iteration numbers. Table 2 shows the impact of different decomposition strategies, and Table 3 presents results under a close-setting.","it focuses on specific input challenges that can lead to unreliable outputs: ""claim vagueness"" and ""evidence redundancy"". The entire BiDeV framework is a mitigation strategy for these issues.

Mitigation Strategies for Improving Factuality:
Vagueness Defusing: This module directly tackles claim ambiguity. By forcing the LLM to first identify and resolve latent information (e.g., ""who is 'the writer'?"") and complex relations (e.g., what does ""She"" refer to?), it prevents the model from reasoning over unclear or incomplete information, thereby improving the reliability of the verification.

Redundancy Defusing: This module addresses the problem of noisy and irrelevant evidence, which can confuse LLMs and lead to incorrect conclusions. By filtering evidence to retain only the most pertinent information, the framework ensures the model is grounded in high-quality, relevant facts. This directly improves factuality by minimizing distraction from noisy data.




In essence, BiDeV reduces the risk of errors by simplifying both the claim and the evidence before the final verification step, making the reasoning task easier and more reliable for the LLM.
","Prompting Techniques:
Few-Shot Prompting: The framework uses few-shot demonstrations to guide the gpt-3.5-turbo model in performing its specialized roles (Perceptor, Rewriter, Decomposer, Filter).
Role-Playing LLMs: The core of the methodology is assigning specific roles to different LLMs, each with its own prompt and instructions, to break down the complex task into manageable sub-tasks.

Iterative Step-by-Step Thinking: The perceive-then-rewrite stage is an iterative process where the model identifies a problem, asks a question, gets an answer, and refines the claim. This structured, multi-step approach ensures thoroughness and clarity before verification.
Prompting Strategies with Integrated External Retrieval: This is central to the ""open setting"" experiments. The process involves: 1) generating a query (Perceptor), 2) retrieving a set of documents from Wikipedia (BM25), 3) filtering those documents to get relevant evidence (Filter), and 4) using that evidence to answer the query (Querier) or verify a claim (Checker).



Fine-Tuning: The paper explicitly states that the LLMs are used without additional fine-tuning. Its novelty lies in the framework and prompting strategy, not in model training.

","BiDeV is a sophisticated implementation of a RAG workflow tailored for fact-checking.

Retrieval: In its open setting, the system retrieves evidence documents from an external knowledge base (Wikipedia).

Augmentation: The framework's key innovation is how it augments the process. Instead of just feeding all retrieved information to the generator, it uses the Redundancy Defusing (RD) module to filter and refine the retrieved evidence. This ensures that only the most relevant and high-quality information is used to augment the context for the LLM.
Generation: The Querier and Checker LLMs use this filtered, augmented evidence to generate their outputs—be it an answer to a question or the final verdict on a sub-claim.

The paper's results demonstrate that this enhanced RAG approach, which actively curates the retrieved information, is more robust than baseline methods that are overwhelmed by noisy evidence.

Sources





",,"Yes, the paper emphasizes that BiDeV provides an intuitive and explainable reasoning process by imitating the fact-checking process of human experts.


The explainability comes from the framework's transparent, step-by-step workflow: identifying latent information, asking specific questions, rewriting the claim for clarity, decomposing it into verifiable sub-claims, and then checking each one.

This structured process provides a clear trail of how the model arrived at its conclusion, as demonstrated in the case study (Figure 7), which contrasts BiDeV's clear steps with the error-prone outputs of other models",https://github.com/EthanLeo-LYX/BiDeV
48,Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning,https://arxiv.org/abs/2502.14765,Conference,NAACL 2025,A,2025,April,Germany,"The research domain is fact verification (FV), specifically applied to medical claims. It intersects with Natural Language Processing (NLP)","SCIFACT
•
HEALTHFC
•
COVERT
","
Traditional Pipeline: Uses semantic search (similarity of query and corpus embeddings), sentence embedding models, and an encoder-only model like DeBERTa-v3 fine-tuned on NLI datasets.
•
Step-by-Step LLM System: Experiments with several Large Language Models (LLMs):
◦
GPT-4o-mini-2024-07-18
◦
Mixtral 8x7B
◦
LLaMa 3.1 (70B) These LLMs are used for question generation (Mq), evidence summarization (Ms), and reasoning (Mr","The paper addresses the lack of exploration of iterative, step-by-step fact verification (FV) systems on domain-specific and realistic claims, particularly in the medical field. It aims to bridge the gap between existing work that primarily focuses on encyclopedic or multi-hop claims and the need for robust FV for health-related information. The paper also investigates whether such an iterative approach can outperform traditional FV pipelines for domain-specific claims","he core of the paper is a step-by-step, iterative LLM-based system inspired by QACheck. The process is as follows:

Question Generation: Given a claim, the system uses an LLM to generate up to five follow-up questions to gather necessary evidence. This step uses few-shot prompts to guide the LLM.


Evidence Retrieval: For each generated question, the system retrieves evidence. This is done either by querying the LLM's own internal knowledge or by performing an external web search using the DuckDuckGo search engine and using snippets from the top 5 results.


Reasoning: The retrieved evidence is presented to a reasoning LLM module. This module uses mechanisms like chain-of-thought reasoning  to decide if there is enough information to make a final verdict. If not, it continues to generate more questions.


Verdict and Explanation: Once sufficient evidence is gathered, the system predicts a final verdict (SUPPORTED or REFUTED) and generates a natural language explanation to justify its reasoning.
Structured Reasoning (Experimental): The study also tests an alternative setting that incorporates predicate logic, where the LLM generates a structured predicate (e.g., Treats(aspirin, headache)) to help create more targeted questions and guide the final verdict","Yes, the system uses external knowledge and an approach similar to RAG, though it isn't explicitly named as such.

It has two modes for evidence retrieval: using the LLM's internal knowledge and using an external knowledge source.

The external knowledge source is the ""whole web,"" which is queried via the DuckDuckGo search engine. The system retrieves snippets from the top five search results to use as evidence for the reasoning module. This process of retrieving external information to ground the model's reasoning is a core component of RAG."," binary precision, recall, and F1 score ","Superiority of Step-by-Step System: The iterative LLM-based system significantly outperformed the traditional three-part pipeline on all three medical datasets, improving F1 scores by +4.3 to +4.9 points. This answers the main research question affirmatively.

Internal vs. External Knowledge: For the highly technical SciFact dataset, external web search consistently improved performance. However, for the more common health claims in HealthFC and COVERT, the LLMs' internal knowledge was sometimes sufficient and even outperformed web search, demonstrating that LLMs encode a great deal of useful medical knowledge.

Predicate Logic: Using structured predicates led to the best overall performance on the HealthFC dataset by generating more precise questions. However, it degraded performance on the COVERT dataset, which contains informal language, suggesting predicates are better suited for well-formed queries.

Choice of LLM: GPT-4o-mini was the top-performing LLM overall. GPT generated the most general questions, while LLaMa and Mixtral produced more specific ones, which could sometimes complicate evidence retrieval.","The traditional pipeline's best F1 scores were 76.5% (HealthFC), 82.5% (COVERT), and 82.7% (SciFact).
The GPT-4o-mini step-by-step system without predicates improved these F1 scores to 79.6% (HealthFC), 85.9% (COVERT), and 87.6% (SciFact).
The highest F1 score for HealthFC (81.7%) was achieved by the GPT system using predicate logic.

The highest F1 score for COVERT (85.9%) was achieved by the GPT system with web search.
The highest F1 score for SciFact (87.6%) was also achieved by the GPT system with web search.","The paper discusses reliability issues that align with the concept of hallucinations, referring to them as ""knowledge conflicts"" and ""misleading medical advice.""

Hallucinations in LLMs: The paper notes that a key issue is when an LLM predicts an incorrect label even when presented with evidence to the contrary, due to its strong internal, pre-existing knowledge. It also explicitly acknowledges in its Ethics Statement that LLM responses can ""contain hallucinations or misleading medical advice that should always be manually verified"".

Mitigation Strategies: The paper's core methodology—the step-by-step verification process—is implicitly a strategy to mitigate this. By forcing the model to break down a claim, seek external evidence, and reason over it, the system aims to ground the final verdict in retrieved facts rather than relying solely on the LLM's unverified internal knowledge. Using an external web search is a direct attempt to improve factuality and reduce reliance on potentially faulty internal knowledge.

Recent Innovations: The paper itself proposes an innovative approach by applying the step-by-step method to domain-specific claims. It also experiments with using predicate logic as a way to introduce more structured and accurate reasoning, which can be seen as an innovation to improve reliability.","Prompting Techniques:
Few-shot Prompts: The system uses few-shot examples to guide the LLM in generating both the initial and follow-up questions, as well as in deciding when enough evidence has been gathered. The paper notes that GPT-4o-mini was particularly effective at following the style of these examples.




Chain-of-Thought (CoT) Reasoning: The system explicitly uses CoT-style reasoning in its reasoning module to process multiple pieces of evidence before arriving at a verdict.

Step-by-Step Prompting: The entire methodology is a form of step-by-step prompting, where the problem is decomposed into a sequence of question-generation and question-answering steps.

Prompting Strategies with Integrated External Retrieval: The methodology directly integrates external retrieval. Generated questions are used as queries for a web search engine, and the retrieved text snippets are then integrated back into the prompt for the reasoning module.

Fine-Tuning: The paper does not perform any fine-tuning of the LLMs. Its approach is entirely based on prompting pre-trained models. It only references a fine-tuned DeBERTa model as part of the baseline it compares against.
Domain-Specific Training: The paper's focus is on verifying claims in the specific domain of medicine. The results show that the step-by-step method is highly effective for these claims because they contain complex concepts and relationships that benefit from being broken down and clarified, similar to multi-hop claims.","The system actively retrieves information from an external source (the web via DuckDuckGo) to inform the generation process.
This retrieved evidence is then provided as context to the LLM's reasoning module, which augments its ability to make an accurate final prediction and generate an explanation.
The study directly compares the performance of using this external retrieval approach against using only the LLM's internal knowledge, finding that external retrieval is crucial for claims requiring up-to-date or highly specific biomedical knowledge (as in the SciFact dataset). This demonstrates a core value proposition of RAG: grounding LLM outputs in external, verifiable facts to improve accuracy and trustworthiness.",,"the system is designed for explainability, which is a core feature of the methodology.

The iterative, step-by-step process itself provides explainability. The reasoning process can be traced through the sequence of generated question-answer pairs, which justifies the final verdict.
The final output of the system includes not only a verdict but also a generated natural language explanation based on the collected evidence. Figure 1 provides a visual example of this, showing how evidence points ([1], [2], [3]) are linked to the final explanation.

The paper suggests that the explanation generation step could be improved in the future by considering different user perspectives.",https://github.com/jvladika/StepByStepFV.
49,Zero-shot and Few-shot Learning with Instruction-following LLMs for Claim Matching in Automated Fact-checking,https://arxiv.org/abs/2501.10860,Conference,COLING 2025,A,2025,January,UK,"The domain of the research is automated fact-checking, specifically focusing on the claim matching (CM) task","ClaimMatch: A new dataset created by the authors by extending data from Check-That 2022 (Nakov et al., 2022) for claim matching with short texts. It consists of tweet-verified claim pairs and was extended to include negative examples. The test set contains 1,000 examples (500 positive, 500 negative).
•
A dataset based on Kazemi et al. (2022) for claim matching with long texts. A sample of 1,000 pairs of long texts and short claims was initially taken, and a final test set of 258 examples (129 positive, 129 negative) was created","
GPT-3.5-turbo-0125 (GPT-3.5)
•
Gemini-1.5-flash (Gemini)
•
Mistral-7B-Instruct-v0.3 (Mistral)
•
Llama-3-8B-Instruct (Llama)
•
As a state-of-the-art (SOTA) baseline, they used a fine-tuned multilingual XLM-RoBERTa-base (XLM-R) model.
•
As semantic similarity (SS) baselines, they used All-MiniLM-L6-v2 (All-MiniLM) and text-embedding-3-small (embedding3)","The problem they solved is claim matching (CM), which is the task of determining if two claims can be verified using the same piece of evidence or fact-check. The paper explores whether instruction-following LLMs can effectively perform this task in zero-shot and few-shot learning scenarios, without extensive fine-tuning. They also investigated whether CM can be approached using prompts from more mature, similar tasks like natural language inference (NLI) and paraphrase detection (PD).
Their methodology involved experimenting with zero-shot and few-shot learning approaches using the aforementioned instruction-following LLMs. They formulated CM as a binary classification task (match/not match).","The core methodology involves using prompt-based zero-shot and few-shot learning with LLMs.

They treat CM as a binary classification task (""match"" vs. ""not match"").

They experiment with different prompt templates designed specifically for Claim Matching (CM), Paraphrase Detection (PD), and Natural Language Inference (NLI) to see which is most effective.

In the few-shot setting, they use the priming method (in-context learning) with 10 labeled examples (5 positive, 5 negative) included in the input sequence.
They test both a ""single instruction"" setup (with a default system prompt) and an ""ensemble instruction"" setup (combining a task-specific prompt in both the system and user instructions).",No,"F1 score (weighted)
•
Accuracy
•
In error analysis, they also used precision (weighted) and recall (weighted).","Task Reframing is Effective: Framing the Claim Matching (CM) task using prompt templates from Paraphrase Detection (PD) and Natural Language Inference (NLI) generally yields superior results compared to bespoke CM templates.

Few-Shot > Zero-Shot (Usually): Providing a few training examples in the prompt (few-shot) generally improves performance over providing none (zero-shot). However, some models, like Llama, were distracted by the examples in certain setups and performed better with zero-shot.


Ensemble Instructions Boost Performance: Combining templates (e.g., using a PD template in the system instruction and an NLI template in the user instruction) led to significantly better and more consistent results, with some models achieving state-of-the-art performance.

Model-Specific Prompting is Crucial: Different LLMs perform best with different prompt templates and setups, highlighting the need for careful, model-specific prompt engineering.


Performance on Longer Texts: The proposed pipeline works effectively on longer texts. Interestingly, using domain-independent (short text) training examples yielded better results than using in-domain (long text) examples.","In the ""single instruction"" few-shot setup, Mistral with a PD template (PD-6) achieved the best performance (F1 95.0%), getting close to the fine-tuned XLM-R SOTA model (F1 96.2%).

With ""ensemble"" instructions, the results improved dramatically. In a zero-shot setting, Mistral matched the SOTA score (F1 96.2%), and Gemini surpassed it (F1 97.1%).
The overall best score was achieved by Gemini using a few-shot ""ensemble"" instruction (NLI-5 & PD-6 templates), which reached an F1 score of 97.2%, outperforming the fine-tuned SOTA model.","Task Misinterpretation: A significant reliability issue identified was the models' tendency to misunderstand the task. For example, when using NLI templates with ""true""/""false"" labels, some models incorrectly addressed the task as a fact-checking exercise (i.e., verifying if a claim is true) rather than a claim matching one (i.e., determining if two claims are about the same thing). This shows how prompt wording can lead a model to perform the wrong task, impacting the reliability of the results for the intended purpose.

","Prompt Design: Prompt design is central to the methodology. The study extensively explores the effect of different prompt templates (CM, PD, NLI), single vs. ensemble instructions, and zero-shot vs. few-shot setups on accuracy (F1 and accuracy metrics). Findings show that PD and NLI templates generally outperform CM templates, few-shot generally helps compared to zero-shot (though ensemble zero-shot can be better than single few-shot), template wording matters significantly, different models respond differently to templates, and combining templates improves results. Accuracy is heavily affected by prompt design. Regarding interpretability, the error analysis section discusses how model explanations in the output help understand why the model made a certain prediction or error. This suggests prompt design, by eliciting explanations, can potentially affect interpretability, and interpreting these explanations is part of understanding the model's behavior. Chain-of-thought methods, related to prompt design, are suggested for future work to enhance explainability.
•
Fine-tuning Architectures: The paper focuses on zero-shot and few-shot learning without task fine-tuning of the LLMs themselves, arguing this is useful given the high cost of manually labeling data for fine-tuning. Fine-tuning is mentioned for the state-of-the-art baseline (XLM-RoBERTa), which was fine-tuned on a larger training dataset. The LLM results in few-shot and zero-shot settings are compared against this fine-tuned model to assess their performance without fine-tuning. Fine-tuning approaches for LLMs with more training data are suggested as future work to potentially reduce the impact of few-shot example selection. The paper does not discuss different fine-tuning architectures.
•
Domain-specific Training: This was investigated in the context of longer texts. The researchers experimented with using domain-independent train examples (from the short texts) versus domain-specific train examples (from the longer text dataset) in the few-shot setup. The results showed that using domain-independent train examples led to better accuracy for both Mistral and Gemini models on the longer text dataset. This suggests that for the LLMs and datasets used, domain-independent examples were more effective for domain transfer in few-shot learning. The paper does not explicitly discuss the effect on interpretability in this context.",,"The authors noted that for most templates, the models' answers were accompanied by explanations for their reasoning.
The Error Analysis section is built upon studying these outputs to understand why models made certain errors, such as confusing claim matching with fact-checking  or being overly strict about minor differences in details.

The authors propose that analyzing these model explanations should be a formal step in the pipeline, allowing for post-processing corrections. For instance, if a model labels a pair as ""not a match"" but its explanation indicates the claims are about the same event with minor variations, the label could be corrected to ""match"".
The paper suggests that chain-of-thought methods could be implemented in future work to further enhance model explainability.
",,
50,Logical Consistency of Large Language Models in Fact-checking,Logical Consistency of Large Language Models in Fact-Checking | OpenReview,Conference,ICLR 2025,A*,2025,January,Unknown,"The research lies in the domain of Natural Language Processing (NLP), specifically focusing on fact-checking using Large Language Models (LLMs) and their logical consistency when dealing with propositional logic queries over Knowledge Graphs (KGs)","FreebaseLFC (derived from Freebase)
◦
NELLLFC (derived from NELL)
◦
WikiLFC (derived from WikiKG90Mv2)","Llama2-7B (7 billion parameters)
◦
Llama2-13B (13 billion parameters)
◦
Gemma-2B (2 billion parameters) They also conducted experiments with larger and closed-source models like Llama2-70B and GPT-4o","The paper addresses the problem of logical inconsistency in LLM responses when presented with complex logical queries (involving negation, conjunction, and disjunction) in the context of fact-checking. Existing research primarily focused on consistency based on paraphrasing, neglecting the need for logical reasoning","Methodology: The researchers proposed a framework to assess and improve the logical consistency of LLMs in fact-checking using retrieval-augmented generation (RAG) over knowledge graphs. Their methodology includes:
◦
Benchmark Creation: Developing three logical fact-checking datasets (FreebaseLFC, NELLLFC, WikiLFC) over KGs.
◦
Consistency Measures: Defining quantitative measures for logical consistency of LLM responses on propositional logic-based queries. These measures assess if the LLM's response aligns with the semantics of logical operators (negation, conjunction, disjunction) and logic rules (commutative, associative, distributive).
◦
Retrieval-Augmented Generation (RAG): Utilizing KGs as an authoritative knowledge base to provide context to the LLMs for fact-checking beyond their training data. They explored different context retrieval methods, including graph traversal (BFS) and vector embedding-based retrieval.
◦
Supervised Fine-tuning: Employing supervised fine-tuning (using QLoRA for efficiency) to improve the logical consistency of LLMs on complex fact-checking tasks with KG contexts. They created instruction datasets with primitive logic facts and their ground-truth labels from the KGs","the study is centered around a Retrieval-Augmented Generation (RAG) framework.

It uses real-world Knowledge Graphs (KGs) as the authoritative external knowledge base to provide context for the LLM.
Context is retrieved from the KG using methods like Breadth-First Search (BFS) graph traversal  and vector-embedding-based retrieval. ","Logical Consistency,Accuracy","Existing LLMs, while often accurate when provided with KG contexts, lack logical consistency, especially on complex queries.

Supervised fine-tuning significantly improves the logical consistency of LLMs. An average improvement of 14% in consistency and 19% in accuracy was observed.


Models fine-tuned on simple facts (with single operators) generalize well, improving consistency on more complex, unseen facts and rules.

For this task, supervised fine-tuning is more effective than zero-shot, 2-shot, or Chain-of-Thought (CoT) instruction prompting.



Adding KG context to the prompt improves both accuracy and consistency compared to not using a context.
Parameter-Efficient Fine-Tuning (PEFT) is about 3 times more efficient than full fine-tuning for this task","After fine-tuning, the average logical consistency across all models and datasets increased from 0.74 to 0.88, and accuracy increased from 0.69 to 0.88.
On the Llama2-13B model with the FreebaseLFC dataset, fine-tuning improved consistency on conjunction facts (p∧q) from 0.67 to 0.83 and on disjunction facts (p∨q) from 0.73 to 0.97.
Instruction prompting on Llama2-13B for simple facts lowered consistency from 0.81 to 0.79, whereas fine-tuning increased it to 0.86.
For the complex rule p∨(q∧r), fine-tuning Llama2-13B improved accuracy from 0.74 to 0.85 and consistency from 0.78 to 0.81.","Hallucinations in Large Language Models: The paper notes that LLMs are infamous for inconsistent responses and can experience ""faithfulness hallucination,"" where generated content diverges from user inputs (like a KG context) or is internally inconsistent when the prompt is logically manipulated. This directly harms the reliability of fact-checking, as the model might validate a fact and its logical opposite, as shown in an example where an LLM returns '1' (true) for both a fact and its negation.

Mitigation Strategies for LLM Hallucinations: The primary mitigation strategy proposed and tested is a two-pronged approach:
Retrieval-Augmented Generation (RAG): Grounding the LLM with an authoritative knowledge base (a KG) to provide factual context and reduce reliance on potentially faulty internal knowledge.

Supervised Fine-Tuning: The authors argue that a ""pre-train, prompt, and predict"" paradigm is often insufficient. Instead, they use supervised fine-tuning to explicitly teach the model to correctly classify primitive logical facts. This improves the model's internal weights to make it a more consistent responder, thereby reducing faithfulness hallucination","The paper extensively explores the trade-offs between different prompting strategies and fine-tuning.

Prompting Strategies: The study tests several prompting methods and finds them inferior to fine-tuning for achieving logical consistency.
Zero-Shot Prompting: Using a direct instruction prompt (e.g., ""Answer Yes when the fact is in the context and No otherwise"") was tested. It was found to be less effective and sometimes even negatively impacted the base model's performance compared to fine-tuning, with consistency improving by up to 46% more with fine-tuning.



Few-Shot (2-Shot) Prompting: Providing one positive and one negative example in the prompt was also evaluated. This performed even worse than zero-shot prompting, which the authors suggest is due to the nuances and intricacies of logical facts that are not easily learned from a few examples.

Chain-of-Thought (CoT) Prompting: A CoT prompt including reasoning steps was created. However, it also did not show improvement over supervised fine-tuning, leading the authors to hypothesize that achieving logical consistency requires updating the model's internal weights.

Fine-Tuning Architectures: The authors use Supervised Fine-Tuning (SFT) as their primary method for improvement. To make this process efficient, they adopt QLORA (Quantized Low-Rank Adaptation), a parameter-efficient fine-tuning (PEFT) technique that is more memory-efficient than standard fine-tuning. This approach updates only a small number of parameters, making it feasible for large models.



Domain-Specific Training: The training is highly specific to the task of logical fact-checking. The fine-tuning dataset is constructed to help the LLM learn to correctly classify primitive logic facts (negation, conjunction, disjunction) as true or false based on a provided KG context. The hypothesis, which is validated experimentally, is that fine-tuning on these fundamental building blocks allows the model to generalize to more complex, unseen logical rules and facts","ntegrating RAG by providing KG contexts significantly improves LLM performance in fact-checking, leading to higher accuracy and logical consistency compared to operating without context. It provides the necessary factual premise for the LLM to validate claims against an authoritative source, helping to reduce hallucinations.
◦
Challenges in Implementation:
▪
Limited context window in LLMs requires pruning the retrieved context.
▪
Performance degradation can occur with very long contexts.
▪
Designing effective context retrieval methods for structured KG data is necessary, requiring techniques like graph traversal (BFS) or embedding-based search.
▪
Building relevant context is more complex for complex facts.
▪
Retrieval methods, especially BFS on dense KGs, can add to the end-to-end processing time, although some steps like embedding can be done offline.
▪
Ensuring the retrieved context intuitively contains the necessary knowledge for fact validation is crucial.
▪
Implementing RAG in specialized domains like logic fact-checking requires domain-specific adaptation of retrieval and prompting strategies.",,NO,https://github.com/bishwamittra/llm_logical_consistency
51,FIRE: Fact-checking with Iterative Retrieval and Verification,https://arxiv.org/abs/2411.00784,Conference,NAACL 2025 Findings,A,2025,April,Multiple," Natural Language Processing (NLP), specifically focusing on fact-checking of text generated by large language models (LLMs).","Factcheck-Bench,FacTool-QA,
FELM-WK,BingCheck","Proprietary Models:
GPT family: GPT-4o, GPT-4o-mini, o1-preview, o1-mini.
Claude family: Claude-3 Haiku, Claude-3 Opus, Claude-3.5 Sonnet.
Open-source Models:LLaMA 3.1-Inst 8B.Mistral-Inst 7B","The problem this research addresses is the challenge of efficiently and cost-effectively fact-checking long-form text generated by LLMs, which are known to produce factually incorrect information despite high confidence. Existing methods that retrieve a fixed number of evidence pieces before verification are often not cost-effective and do not mimic human iterative reasoning.","Final Answer or Next Search Query: The core component is a unified mechanism where the LLM agent assesses an atomic claim. Based on its confidence—using both its internal knowledge and any previously gathered evidence—it decides to either provide a final answer or generate a new, targeted search query. Initially, with no external evidence, this decision relies solely on the model's internal knowledge.



Web Search: If a search is deemed necessary, a query is sent to Google Search via SerpAPI, and the retrieved snippets are added to the evidence set for the next iteration.
Final Verification: To prevent endless loops, an upper limit is set on the number of retrieval steps. Once this limit is reached, the model must perform a final verification using all the evidence gathered.
Prevention Mechanisms: The framework also includes methods to address common issues like the generation of repetitive search queries (using early termination and diversity prompts) and verification overconfidence (using specific prompts like ""At Least One/Two"" and ""Inclusive"").","FIRE framework is fundamentally a dynamic and interactive Retrieval-Augmented Generation (RAG) approach tailored for fact-checking. It uses external knowledge retrieved from the web via Google Search to verify claims. Unlike traditional RAG pipelines that retrieve a fixed set of documents, FIRE iteratively retrieves evidence, with each step informing the next. The decision to retrieve is dynamic and based on the model's confidence, allowing it to rely on its internal knowledge when possible and seek external information only when necessary.","Precision, Recall, and F1 scores for both positive (True) and negative (False) classes","
FIRE achieves slightly better fact-checking performance compared to other strong frameworks while significantly reducing LLM costs (average of 7.6 times) and search costs (average of 16.5 times) when using GPT-4o-mini.

Proprietary language models generally outperform open-source models in fact-checking tasks.

The most advanced proprietary models (o1-preview and Claude-3.5 Sonnet) exhibit the best performance, but more economical models like GPT-4o-mini can be a sufficiently capable alternative at a much lower cost.

Step-by-step reasoning (Chain-of-Thought) enhances the model's confidence in fact-checking, particularly for GPT-4o-mini, leading to fewer unnecessary searches. Omitting reasoning leads to increased search costs and inferior overall performance.

The study identified several quality issues in the benchmark datasets, including ungrounded claims and debatable labels. Overly strict reasoning by LLMs can also lead to incorrect classifications of debatable claims.","FIRE framework, particularly when paired with GPT-4o-mini, proved to be highly efficient. For example, on the Factcheck-Bench dataset, GPT-4o-mini within FIRE achieved a False F1 score of 0.67 for a combined LLM and search cost of only $0.63 ($0.19 + $0.44). In contrast, the top-performing ol-preview model achieved a slightly better F1 score of 0.69 but at a cost of $146.46, representing a 766-fold cost saving for GPT-4o-mini for a marginal drop in performance. Across all datasets, FIRE with GPT-4o-mini reduced LLM expenses by 7.6 times and search costs by 16.5 times compared to other frameworks.","LLMs are capable of producing outputs that are highly confident but factually incorrect. The auto-regressive learning objective doesn't guarantee factual accuracy, leading models to produce content that deviates from facts. A reported 5%-10% of responses from models like GPT-4 and LLaMA-2 on world-knowledge questions can contain false claims. Error analysis indicates that LLM verifiers can make wrong decisions due to inaccurate parametric knowledge stored internally [41, Table 8]. LLM reasoning can also be incorrect, sometimes hallucinating information not present in the claim
◦
Mitigation Strategies (proposed or used in the source):
▪
Post-generation fact-checking is considered essential for ensuring accurate knowledge dissemination.
▪
The FIRE framework mitigates hallucinations by integrating iterative evidence retrieval with the LLM's internal knowledge and verification process. By retrieving external evidence, the system can potentially correct or supplement incorrect internal knowledge. Retrieval plays a pivotal role in determining verification results.","Prompt Design: Prompt design is crucial for the core ""Final Answer or Next Search Query"" mechanism. Different prompt designs were tested for preventing overconfidence and repetitive searches.
▪
The Default prompt, which encourages step-by-step reasoning, enhances confidence and leads to fewer searches, particularly for smaller models like GPT-4o-mini. This reasoning process contributes to interpretability by showing how the model arrives at its decision and facilitates error analysis.
▪
Prompts forcing more searches (""At Least One/Two"") increased cost without improving performance.
▪
The ""Inclusive"" prompt (encouraging less strictness) reduced searches but lowered performance.
◦
Fine-tuning Architectures: The paper does not discuss the effect of fine-tuning architectures on performance or interpretability; it evaluates existing state-of-the-art models.
◦
Domain-specific Training: The paper notes using datasets from ""multiple domains"" but focuses on claims needing ""world knowledge"". It does not discuss the effects of domain-specific training on LLMs for fact-checking. It mentions that FACTOOL is adaptable across domains but notes its reliance on external tools can introduce complexity and dependency on tool accuracy.","What they've done: FIRE represents a novel approach to integrating RAG into fact-checking. Instead of a one-off retrieval step, it creates an iterative and interactive framework where retrieval and verification are intertwined. The decision to retrieve is dynamic and confidence-based, which more closely simulates human cognitive processes and makes better use of the LLM's internal knowledge.



Challenges: A challenge for this iterative RAG approach is ensuring the retrieved evidence is sufficient and accurate. The error analysis found that the largest category of errors (50 out of 135) was ""Knowledge Issue,"" which includes cases where the ""collected evidence is insufficient to cover all aspects mentioned by a long claim"" or is simply inaccurate. This shows that even with an iterative process, finding comprehensive and correct evidence remains a core challenge",,"Yes, the framework incorporates an explainability component through its reasoning process. The default prompt explicitly instructs the LLM to ""think through the process step-by-step"" and include a summary of key points in its reasoning before making a final decision.



An ablation study, FIRE (No Reason), was conducted where the model was instructed not to articulate this reasoning. The results showed that this step-by-step reasoning is crucial; its absence reduces the model's confidence, leading to more searches and lower overall performance. This demonstrates that the explicit reasoning process is not just for human interpretation but is integral to the model's performance and confidence estimation",
52,On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness Evaluation,https://arxiv.org/abs/2410.12222,Conference,NAACL 2025 Findings,A,2025,April,USA,"evaluating the faithfulness of generated content in Guided NLG scenarios,travel domain","
ConvoAS: An abstractive summarization dataset of customer support transcripts.
ConvoTS: A topic-specific summarization dataset of customer support transcripts.
ReviewTS: A topic-specific summarization dataset from reviews.
JsonTG: A dataset of key-value JSON pairs for stylized text generation Crucially, because the production datasets lacked sufficient examples of hallucinations, the researchers generated synthetic hallucination dat by gpt 4o a from the gold datasets. This was done for both intrinsic (contradictory) and extrinsic (added new knowledge) types of hallucinations","
LLMs: GPT-4, GPT-4o-mini, Llama3.1-8B, Llama3.1-70B, Claude3 Haiku. They also briefly experimented with Qwen-7B and Qwen-72B.
◦
NLI Model: Vectara's open-source HHEMv1.01, a fine-tuned Deberta model. They also fine-tuned two versions of HHEM on an extension of JsonTG: HHEM-intrinsic and HHEM-extrinsic","The research addresses the critical need for automated and quantifiable faithfulness evaluation in guided Natural Language Generation (NLG). Key problems solved include:

Developing a rubric-based template to use LLMs to score faithfulness on a discrete 1-to-5 scale, moving beyond simple binary classification.

Creating a methodology to generate synthetic unfaithful data by type (intrinsic and extrinsic) to overcome the lack of organic negative examples in production datasets.
Providing a comparative analysis of the performance, sensitivity, cost, and latency of popular LLMs versus NLI models for the task of faithfulness evaluation","Rubrics-based LLM Evaluation: They developed a rubric-based approach where LLMs acted as judges to score the faithfulness of (reference, hypothesis) pairs on a scale of 1 to 5. The rubric considered four aspects: factual consistency, adjective regularity, knowledge congruence, and style alignment. They also instructed the LLMs to provide detailed reasoning for their scores. They experimented with both scoring with and without reasoning.
◦
Entailment-based Evaluation: They used the HHEM model, which scores the level of entailment between a reference and a hypothesis on a scale of, later scaled to. To handle longer references, they implemented a sliding window segmentation technique. They also fine-tuned HHEM models on synthetic data.
◦
Synthetic Data Generation: They developed methods to generate synthetic intrinsic and extrinsic hallucinations using GPT-4o based on guidelines to mimic different types of unfaithful content. They also simulated varying percentages of hallucination in a subset of data by replacing gold sentences with their hallucinated counterparts.","The study operates within the context of guided NLG, where the external knowledge is a provided source document (reference) against which the generated text (hypothesis) is evaluated. This is not a traditional Retrieval-Augmented Generation (RAG) system where the model must first retrieve information from a large, unstructured corpus. Instead, it's a faithfulness evaluation task that assumes the grounding data is already supplied. The paper does, however, note that detecting hallucinations in RAG systems is a related field of research.","absolute error between expected and predicted faithfulness scores ,The paper also uses a baseline metric composed of ROUGE-L, BLEU, and BERTScore for comparison. To measure sensitivity, they analyze the change in score (""delta per step"") as the percentage of hallucinated content increases","GPT-4 is the best zero-shot judge: Among all models tested, GPT-4 performed the best in a zero-shot setting, accurately scoring both gold and hallucinated data.

Reasoning improves LLM performance: Instructing an LLM to provide reasoning along with its score makes it more sensitive and less lenient towards unfaithful content. Removing the reasoning step caused performance degradation across all LLMs.


Fine-tuning improves NLI models: Fine-tuning the HHEM model on domain-specific synthetic data significantly improved its performance, even outperforming some LLMs.

Models are less sensitive to extrinsic hallucination: All models found it more difficult to detect extrinsic (added, unverified) hallucinations compared to intrinsic (contradictory) ones.

Smaller LLMs struggle: LLMs with fewer parameters, like Llama3.1-8B and Claude3-Haiku, struggled to follow the rubrics, with one being overly strict and the other overly lenient, making them unreliable for production","Scoring Quality: GPT-4 demonstrated the lowest overall residual error, indicating the highest accuracy. The fine-tuned HHEM-extrinsic model also performed very well, second only to GPT-4.



Scoring Progression: As the percentage of hallucinated content in a text increased from 0% to 100%, all models showed a corresponding decrease in their faithfulness score, confirming their sensitivity to the degree of hallucination.
Latency and Cost: GPT-4, while the most accurate, was also the most expensive and had the highest latency (6.82s per call with reasoning). HHEM models were inherently faster, with a batch-inference latency of just 0.007s per pair.","Hallucinations (unfaithful content) can result in poor data quality or loss of trust from end users in real-world applications. Hallucination means the generated content is either wrong or unverified from the source.
•
Strategies mentioned (from related work): Retrieval-augmented generation, chain-of-thoughts, tree-of-thoughts, self-consistency, self-reflection, controlled decoding, and instruction tuning.
•
Strategies investigated/proposed in the paper: Implementing an automated faithfulness evaluation system using rubric-based LLM scoring or NLI models as an additional layer of security. Tuning NLI models on synthetic data can improve performance. Reasoning during LLM grading helps LLMs gain sensitivity. The paper quantifies the effect by showing that the faithfulness score decreases as the percentage of hallucination increases.","Prompt Design:
◦
The rubric prompt design guided LLMs to evaluate specific aspects (factual consistency, etc.).
◦
Instructing LLMs to provide detailed reasoning in the prompt improved grading precision and sensitivity towards hallucination. Scoring without reasoning degraded performance.
◦
Customizing rubrics (part of prompt design) to a per-use case basis is recommended for better accuracy, especially for subjective aspects like adjective consistency, which otherwise conflicted with the JsonTG task.
◦
Specific prompts were designed for generating subtle synthetic hallucinations.
•
Fine-tuning architectures:
◦
Fine-tuning the HHEM NLI model on synthetic data improved its performance.
◦
Over-tuning HHEM made it less tolerant to creative content.
◦
Tuning on stringified JSON was explored to generalize the model to that data structure.
•
Domain-specific training: The evaluation was conducted on travel-domain datasets. Fine-tuning of NLI models was done on domain-specific synthetic data derived from these datasets. This domain-specific tuning allowed tuned HHEM models to potentially outperform zero-shot LLMs in domain-specific evaluation.
•
Accuracy and Interpretability: Accuracy is directly assessed through residuals and sensitivity to hallucination percentage. Interpretability, though not labeled XAI, is addressed by evaluating the impact of the reasoning component, which explains the score. The reasoning provides insights into why a particular score was assigned. LLMs with sufficient parameters and reasoning instruction demonstrate better accuracy and provide justification (interpretability). NLI models, in contrast, lack this explainability","Input Length Limitations: The NLI-based HHEM model has a maximum input size of 512 tokens, which is a significant challenge when evaluating against long source documents and required the implementation of a complex sliding window segmentation method.

Handling Data Structures: Encoder models like HHEM perform poorly on unseen data structures, such as the raw JSON in the JsonTG dataset, necessitating either specialized preprocessing or fine-tuning to handle them effectively.
",,"the paper heavily utilizes an explainability component. The rubric-based LLM evaluation method explicitly instructs the model to output a detailed reasoning for the score it assigns. The authors find that this self-reflection step is crucial; including it improves the precision and sensitivity of the LLM judges. The paper argues that this reasoning is the ""key to keeping the model 'sharp'""",
53,Medico: Towards Hallucination Detection and Correction with Multi-source Evidence Fusion,https://aclanthology.org/2024.emnlp-demo.4/,Conference,EMNLP 2024 Demonstrations,A*,2024,November,China, hallucination detection and correction in Large Language Models (LLMs),HaluEval,"Llama3-8B-Instruct and Qwen2-7B-Instruct(hallucination detector  as well as hallucination corrector,)   for the ensemble method in the detection phase, a Logistic Regression classifier is used. 


","The prevalence of hallucinations in LLMs where generated content is factually incorrect. 
The inadequacy of single-source evidence retrieval, which can lead to erroneous judgments due to outdated or incomplete information. 


The lack of explainability in existing systems, which often only provide a veracity label without explaining why the content is incorrect. 

The labor-intensive nature of manual fact-checking, highlighting the need for an automatic, end-to-end system. ","Their methodology, named MEDICO (Multi-source evidence fusion enhanced hallucination detection and correction), involves three main stages:
•
Multi-source Evidence Fusion: Retrieves evidence from diverse heterogeneous sources: Search Engine (Web) using Google Search API, Knowledge Base (KB) using English Wikipedia (01/01/2023), Knowledge Graph (KG) using Wikidata5m, and optional User-uploaded Files (UF). The retrieved evidence is then combined, reranked based on its relevance to the user query and generated content using bge-reranker-large, and finally fused either by concatenation or summarization (using Llama3-8B-Instruct).
•
Hallucination Detection with Evidence: Checks the generated content for factual errors based on the fused evidence. This is done in two ways:
◦
Directly prompting a detector LLM (Md) to identify conflicts with the fused evidence and provide a rationale.
◦
Using an ensemble method where evidence from each source is fed separately to the detector, and a Logistic Regression classifier is trained on the likelihood scores to predict the veracity.
•
Hallucination Correction with Rationale: If hallucinations are detected, a corrector LLM (Mc) is prompted to identify and revise the incorrect parts based on the generated rationale. This process involves multiple rounds of correction. A preservation score based on the character-level Levenshtein edit distance is used to prevent excessive modifications. Corrections are iteratively performed until a predefined threshold or approval by the detector is reached.","MEDICO framework is fundamentally based on a Retrieval-Augmented Generation (RAG)-like approach. It explicitly resorts to external knowledge to detect and correct hallucinations.  The system is designed to retrieve information from diverse external sources, including:

Web Search Engine 
Knowledge Base (Wikipedia) 
Knowledge Graph (Wikidata5m) 
User-uploaded files 
This retrieved evidence is then used to ""ground"" the detection and correction process, which is the core principle of RAG.","Evidence Retrieval: Hit Rate (HR) and Mean Reciprocal Rank (MRR), both at different recall levels (@1, @3, @5). Hallucination Detection: Precision, Recall, and F1 score.. Hallucination Correction: Approval rate, which is the rate at which the corrected answer passes the detector.Preservation: A metric based on the character-level Levenshtein edit distance to measure how much the corrected text deviates from the original.","Their key findings include:
•
Multi-source evidence fusion significantly improves the recall of golden evidence compared to using a single source.
•
Fusing evidence from diverse sources considerably enhances hallucination detection performance.
•
An ensemble classifier leveraging likelihoods from different evidence sources achieves the best detection results in most cases, albeit with higher computational cost.
•
Correcting hallucinations with fused evidence significantly outperforms correction based on evidence from a single source.
•
Multi-turn correction can effectively improve the accuracy of the generated content, but a moderate number of rounds (around 4) is sufficient.","Their key results demonstrate the effectiveness of MEDICO:
•
Evidence retrieval with fusion achieved the best HR@5 (0.964) and MRR@5 (0.908).
•
Hallucination detection with fused evidence achieved high F1 scores (0.927-0.951).
•
Hallucination correction showed a high approval rate (0.973-0.979).
•
The fused evidence consistently led to better retrieval, detection, and correction performance compared to individual sources.","hallucinations in LLMs present a significant challenge to their reliability and widespread application.  The paper states that LLMs can ""confidently state non-existent facts,"" which undermines their trustworthiness.  This makes external fact-checking essential.


The primary mitigation strategy proposed by MEDICO is to resort to external knowledge.  The paper's core innovation is the use of multi-source evidence fusion to create a more robust and reliable knowledge source for fact-checking.  By retrieving and fusing information from the web, knowledge bases, and knowledge graphs, the system is less likely to be misled by a single, potentially outdated, or incomplete piece of evidence, which is a common failure point.  The system then uses this fused evidence to detect and correct the hallucinated content.                                                                        Reliance on Outdated or Single-Source Evidence: The paper's motivation shows that relying on a single, non-real-time knowledge base can lead to ""erroneous judgment,"" as the evidence itself may be outdated.                                                                                                                                                                                                                                                                           Noise from Multi-Source Retrieval: A major challenge is the ""Noisy Issue"". Retrieving evidence from diverse sources ""inevitably brings lots of noise information"". This noise can ""slip through the net"" despite reranking and negatively impact the accuracy of the detection and correction stages.","Prompting Strategies: The paper explicitly mentions using prompting techniques. For the corrector model, it adopts a chain-of-thought (CoT) approach, where it prompts the model to first identify the specific hallucinated spans that need editing before proceeding to correct them.  For generating the rationale behind a detection verdict, the paper employs in-context learning (ICL), a training-free technique, to enhance the detector's ability to produce reasonable explanations. 

Domain-Specific Adaptation: The framework is designed to handle domain-specific knowledge by allowing users to upload their own files (in formats like TXT, PDF, etc.).  This allows the system to retrieve evidence from a customized, specialized knowledge source when dealing with domain-specific queries, thereby adapting the model's fact-checking capability to that area.               Data Privacy Concerns: For domain-specific adaptation, the paper acknowledges a significant ethical risk. Allowing users to upload files ""may have data privacy concerns"". This presents a major challenge for using the system in specialized fields that rely on proprietary or sensitive information.                                                                                                                                                                                                                                             ","Retrieval: The system retrieves evidence from a diverse set of external sources (web, KB, KG, user files).  This is the ""retrieval"" part of RAG.

Augmentation: This retrieved evidence is then fused and provided as context to the LLM (the detector).  The LLM is prompted to make a judgment conditioned on this evidence. This grounds the model's response in verifiable facts.

Generation (and Correction): If an error is found, the evidence and the rationale derived from it are used to ""generate"" a corrected version of the text.  This is a corrective, rather than a de novo generative, application of the RAG principle.",This paper is fully based on RAG,"MEDICO incorporates explainability. A core feature of the framework is that it doesn't just provide a veracity label (correct/incorrect) but also generates a rationale for its decision.  This rationale explains how the retrieved evidence supports the verdict, which helps users understand why and where the generated content is incorrect.  The rationale is then used to guide the correction module",
54,Fake News Spreaders Detection: Sometimes Attention Is Not All You Need,https://openreview.net/forum?id=lqQiweBeco,,MDPI,Q2,2022,September,Italy,,,,,,,,,,,,,,,
55,GPT Hallucination Detection Through Prompt Engineering,,,CLEF,B,2024,January,Italy,,,,,,,,,,,,,,,
56,BrainLlama at SemEval-2024 Task 6: Prompting Llama to detect hallucinations and related observable overgeneration mistakes,,,SemEval,A,2024,June,Italy,,,,,,,,,,,,,,,
57,End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models,https://dl.acm.org/doi/pdf/10.1145/3539618.3591879,Conference,SIGIR,A*,2023,July,USA,,,,,,,,,,,,,,,
58,Red-dot: Multimodal fact-checking via relevant evidence detection,https://ieeexplore.ieee.org/abstract/document/10948326/,Journal,IEEE Transactions on Computational Social Systems,Q1,2025,April,Greece,,,,,,,,,,,,,,,
59,MAFT: Multimodal Automated Fact-Checking via Textualization,https://ojs.aaai.org/index.php/AAAI/article/view/35354,Conference,AAAI-25,A*,2025,March,Japan,,,,,,,,,,,,,,,
60,LLM-Enhanced multimodal detection of fake news,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0312240,Journal,PLOS One,Q1,2024, October,China,,,,,,,,,,,,,,,
61,Multi-fact: Assessing factuality of multilingual llms using factscore,https://openreview.net/forum?id=lkrH6ovzsj,Conference,COLM,Unranked,2024, October,Korea,,,,,,,,,,,,,,,
62,Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,https://aclanthology.org/2023.emnlp-main.491/,Conference,EMNLP,A*,2023,December,Canada,,,,,,,,,,,,,,,
63,When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification,https://aclanthology.org/2025.fever-1.5/,Conference,FEVER,Unranked,2025,July,Germany,,,,,,,,,,,,,,,
64,Multilingual Symptom Detection on Social Media: Enhancing Health-related Fact-checking with LLMs,https://aclanthology.org/2025.fever-1.4/,Conference,FEVER,Unranked,2025,July,Japan,,,,,,,,,,,,,,,
69,,,,,,,,,,,,,,,,,,,,,,,
70,,,,,,,,,,,,,,,,,,,,,,,
71,,,,,,,,,,,,,,,,,,,,,,,
72,,,,,,,,,,,,,,,,,,,,,,,
73,,,,,,,,,,,,,,,,,,,,,,,
74,,,,,,,,,,,,,,,,,,,,,,,
75,,,,,,,,,,,,,,,,,,,,,,,
76,,,,,,,,,,,,,,,,,,,,,,,
77,,,,,,,,,,,,,,,,,,,,,,,
78,,,,,,,,,,,,,,,,,,,,,,,
79,,,,,,,,,,,,,,,,,,,,,,,
80,,,,,,,,,,,,,,,,,,,,,,,