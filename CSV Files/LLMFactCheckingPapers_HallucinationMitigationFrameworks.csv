Framework,Year,Reference Paper Link,Core Pipeline,Datasets Used,Headline Quantitative Gains,Experimental Settings
MiniCheck,2024,MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents - ACL Anthology,"Generates synthetic data for multi-sentence/multi-fact reasoning to fine-tune small, efficient models (e.g., Flan-T5) for sentence-level fact-checking against grounding documents.","Training: Custom synthetic data (14K) + a subset of ANLI (21K). Evaluation: A new aggregated benchmark, LLM-AGGREFACT, unifying 10 public datasets (e.g., AGGREFACT, TOFUEVAL, WICE).",Achieved GPT-4-level accuracy on the LLM-AGGREFACT benchmark at 400x lower cost. Outperformed prior SOTA specialized models like AlignScore by 4.3% in Balanced Accuracy (BAcc).,"Models: Fine-tuned Flan-T5-large, DeBERTa-v3-large, and RoBERTa-large. 
Baselines: Specialized fact-checkers (e.g., AlignScore) and various LLMs (e.g., GPT-4, Claude 3). 
Metric: Balanced Accuracy (BAcc)."
SNIFFER,2024,SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,"An MLLM (InstructBLIP) undergoes a two-stage instruction tuning process: first for news-domain entity alignment, then for OOC-specific detection logic. It integrates internal consistency checks with external verification via reverse image search to produce a judgment and explanation.","Training/Evaluation: NewsCLIPpings. OOC instruction data was generated using GPT-4. 
Generalization: Tested on News400 and TamperedNews.",Surpassed the base InstructBLIP model by over 40% in detection accuracy (88.4% vs 47.4%). Outperformed previous SOTA (CCN) by 3.7%. Outperformed GPT-4V by 11% on a sampled test set.,"Base Model: InstructBLIP with Vicuna-13B. 
Baselines: SAFE, EANN, VisualBERT, CLIP, CCN, and Neu-Sym detector. 
Metrics: Accuracy for detection; quantitative (e.g., ROUGE) and human evaluation for explanations."
Self-Checker,2024,Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models - ACL Anthology,"A training-free, plug-and-play framework using LLM-prompted modules. A policy agent coordinates a pipeline of: 1) Claim Processor, 2) Query Generator, 3) Evidence Seeker, and 4) Verdict Counselor to systematically fact-check complex text.","New Dataset: Introduced BINGCHECK, containing LLM (Bing Chat) responses annotated by humans. 
Evaluation: Also tested on FEVER and WiCE.","On BINGCHECK, achieved 63.4% response-level accuracy, significantly outperforming standard prompting (19.4%) and CoT (15.7%). Performance on FEVER (56.7% accuracy, 47.9% FEVER Score) and WiCE (71.5% accuracy, 47.7% F1) was below SOTA fine-tuned models but better than other prompt-based methods.","Core LLM: GPT-3.5 (text-davinci-003) with few-shot prompts for each module. 
Knowledge Source: Bing Search for BINGCHECK, Wikipedia for FEVER. 
Baselines: Prompt-based (Standard, CoT, ReAct) and fine-tuned models (BEVERS for FEVER, T5-3B for WiCE)."
LLM-Augmenter,2023,Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,"An agent system that augments a black-box LLM. A policy module selects actions (e.g., consolidate knowledge, query LLM). A Knowledge Consolidator retrieves and processes external evidence. A Utility module provides automated feedback for iterative response refinement.","Evaluation: News Chat (from DSTC7), Customer Service (from DSTC11), and Wiki QA (OTT-QA).","On Customer Service, human evaluation showed a 32.3% improvement in Usefulness over baseline ChatGPT. On Wiki QA, achieved an absolute F1 score gain of over 10% by grounding ChatGPT responses in external knowledge.","Core LLM: Black-box ChatGPT. Policy Module: T5-Base trained with reinforcement learning. 
Knowledge Sources: BM25 for dialogue tasks; DPR and CORE for Wiki QA. 
Metrics: KF1, BLEU, ROUGE (automatic), chrF, METEOR, BERTScore, BLEURT; Usefulness, Humanness (human)."
Provenance,2024,Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM Generation Output - ACL Anthology,"A lightweight fact-checker for RAG using compact NLI models. It scores context items for relevance, selects a subset of ""sources"", prepares a claim from the query and answer, computes factuality scores for each source-claim pair, and aggregates them into a final score.","Evaluation: TRUE, MSMarco, TruthfulQA, HotpotQA, HaluEval, and HaluBench.","On the TRUE benchmark, achieved the best AUC score on 3 out of 7 comparable subsets against models up to 11B parameters. On HaluEval (summarization), surpassed ChatGPT by 3.74% accuracy. On HaluBench, surpassed GPT-3.5-Turbo.","Core Models: Two small cross-encoders (≈300M total parameters): one for relevance scoring, one for hallucination detection. 
Metrics: Primarily Area Under the ROC Curve (AUC); also accuracy. 
Baselines: Compared against results from original dataset papers, including various LLMs (GPT-4, Claude-3)."
"Full-Context Retrieval
and Verification (FCRV)",2024,A Large Language Model-based Fake News Detection Framework with RAG Fact-Checking | IEEE Conference Publication,"Constructs a ""full-context"" for a news article. The pipeline involves: 1) LLM-based extraction of claims as triples, 2) A two-level retrieval process to gather external evidence and log ""retrieval behaviors"" (e.g., no information found), and 3) A reasoning stage where an LLM classifies the article based on the full context.","Training/In-Domain: GossipCop++ and PolitiFact++. 
Out-of-Domain: An AI-generated dataset and a human-written set from PolitiFact++.","On the GossipCop dataset, achieved 88.0% accuracy and 89.4% F1. On the out-of-domain AI-generated dataset, achieved 86.4% accuracy and 88.5% F1, outperforming transformer baselines. On the out-of-domain PolitiFact set, achieved 71.6% accuracy and 82.8% F1, significantly higher than baselines (50-63%).","Core LLM: Llama 3.1 was used for a baseline comparison. 
Retrieval: DuckDuckGo Search API. 
Baselines: H-LSTM and fine-tuned transformers (BERT, RoBERTa, DeBERTa) that were provided the full-context information."
OpenFactCheck,2025,"OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and LLMs - ACL Anthology",A unified framework with three modules: 1) CUSTCHECKER for building custom fact-checking pipelines; 2) LLMEVAL for evaluating LLM factuality on a unified benchmark; 3) CHECKEREVAL for benchmarking automated fact-checkers.,"LLMEVAL: Uses FactQA, a new collection of 6,480 questions from seven corpora (Snowball, SelfAware, FreshQA, FacTool-QA, FELM-WK, Factcheck-Bench, FactScore-Bio). 
CHECKEREVAL: Uses FactBench, an aggregation of four human-annotated datasets (FacTool-QA, FELM-WK, Factcheck-Bench, HaluEval).","On Snowball, showed high error rates for LLaMA-2 (>80%) and GPT-4 (65.5%), highlighting snowballing hallucinations. Found that >90% of claims in LLM responses to open-domain questions are factually correct. GPT-4 can identify 87% of its own mistakes","Settings: Framework supports various back-end LLMs (LLaMA-2 7B and 13B, GPT-4), retrievers (e.g., SerperAPI, BM25), and verifiers (LLMs, NLI models). 
Metrics: Accuracy, Precision, Recall, F1-score."
FactLLaMA,2023,FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking,Enhances an instruction-following LLM (LLaMA) with external evidence. The pipeline retrieves relevant evidence from search engines (Google API) for a given claim. This evidence is then used to instruct-tune the LLaMA model to predict the claim's veracity.,Evaluation: RAWFC and LIAR,"On RAWFC, achieved an F1-score of 0.5565, surpassing the previous SOTA. On LIAR, achieved an F1-score of 0.3044, surpassing the previous SOTA.","Core LLM: LLaMA-7B. 
Tuning: Parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA). 
Baselines: SVM, CNN, RNN, DeClarE, dEFEND, SBERT-FC, CofCED, and zero-shot LLaMA. 
Metrics: Precision, Recall, F1-score."
RAGAR,2024,"RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models","A multimodal fact-checking framework using RAG-augmented reasoning. A multimodal LLM first generates a unified claim. Then, Chain of RAG (CoRAG) or Tree of RAG (ToRAG) reasoning techniques iteratively generate questions, retrieve evidence (text and image captions), and reason to determine veracity.","Evaluation: A balanced subset of 300 claims from the MOCHEG dataset, focusing on political claims from PolitiFact.","The best performing method (ToRAG+CoTVP+CoVe) achieved a weighted F1-score of 0.85, surpassing the baseline reasoning technique by 0.14 points. Human evaluation confirmed the high quality of generated explanations.","Core LLM: GPT-4V. 
Retrieval: DuckDuckGo for text and SerpAPI for reverse image search. 
Reasoning Baselines: A Sub-question Generation with Chain of Thought (SubQ+CoTVP) approach. 
Metric: Weighted F1-score."
Logical and Causal (LoCal),2025,LoCal: Logical and Causal Fact-Checking with LLM-Based Multi-Agents | OpenReview,"A multi-agent framework for logical and causal fact-checking. A decomposing agent breaks claims into sub-tasks. Reasoning agents solve them using retrieved evidence. Two evaluating agents then check the solution for logical consistency and robustness, triggering iterative correction if needed.",Evaluation: Two complex claim verification datasets: FEVEROUS and HOVER.,"Significantly outperformed SOTA baselines including PACAR on both datasets. On HOVER, achieved an average performance improvement of 2.14% (gold evidence) and 2.31% (open book) over the best baselines.","Core LLMs: gpt-3.5-turbo for decomposing/evaluating agents; Flan-T5 for reasoning agents. 
Baselines: Pre-trained (Bert-FC), Fine-tuned (RoBERTa-NLI), LLM-based (ChatGPT, ProgramFC), and Agent-based (PACAR) models. 
Metric: Macro-F1 score."
MEDICO,2024,Medico: Towards Hallucination Detection and Correction with Multi-source Evidence Fusion - ACL Anthology,"A multi-source evidence fusion framework for hallucination detection and correction. It retrieves evidence from multiple sources (web, KB, KG, user files), fuses them, detects hallucinations, provides a rationale, and iteratively revises the content.",Evaluation: A randomly sampled subset of 1000 triplets from the HaluEval benchmark.,"On retrieval, achieved 0.964 HR@5  and 0.908 MRR@5. On hallucination detection, achieved F1-scores of 0.927 on Qwen2-7B and 0.951 on Llama3-8B. On correction, achieved an approval rate of 0.973 on Qwen2-7B and 0.979 on Llama3-8B.","Core LLMs: Llama3-8B and Qwen2-7B for detection and correction. 
Retrieval Sources: Google Search API, Wikipedia, Wikidata5m, and user-uploaded files. 
Metrics: Hit Rate (HR) (retrieval), Mean Reciprocal Rank (MRR) (retrieval); F1-score (detection); Approval Rate (correction)."
Yours Truly,2024,https://ieeexplore.ieee.org/abstract/document/10807167,"An end-to-end framework that uses FactStore, a real-time database of fact-checked claims. It breaks compound sentences into atomic claims, retrieves and reranks evidence for each, and uses a fine-tuned LLM to infer truth values and generate a credibility report.","Knowledge Source: FactStore, a custom database of articles from Indian and international fact-checking sites. 
Evaluation: TRUTHSEEKER 2023, FACTIFY-5WQA, LIAR, FEVEROUS.","Achieved an F1 Score of 94%. Outperformed contemporary fact-checking systems on multiple misinformation baselines, achieving 0.92 F1 on a synthesized LIAR dataset.","Core LLMs: GPT-4, GPT-3.5, and Mistral-7B. 
Retrieval: Text and vector search on FactStore, with reranking using a query-based committee selector.
Metrics: Precision, Recall, F1 Score, Accuracy."
CLImate Mediator for INformed Analysis and Transparent Objective Reasoning (CLIMINATOR),2025,Automated fact-checking of climate claims with large language models,"A Mediator-Advocate debate framework for fact-checking climate claims. Specialized ""Advocate"" LLMs, grounded in different authoritative text sources, present arguments. A central ""Mediator"" LLM synthesizes these views, asks follow-up questions if needed, and delivers a final verdict.","Knowledge Sources: IPCC reports, WMO reports, scientific abstracts (AbsCC), and top climate scientist abstracts (1000S). An adversarial NIPCC corpus was used for robustness tests. 
Evaluation: Climate Feedback dataset (170 annotated claims).","Achieved a binary classification accuracy exceeding 96% on the Climate Feedback dataset. Significantly outperformed a standalone GPT-4o, especially on more granular classification levels (72.7% vs. 56.5% accuracy at Level 2).","Core LLM: GPT-4o as both the Mediator and the base for RAG-based Advocates. 
Baselines: Standalone GPT-4o. 
Metrics: Accuracy, Precision, Recall, F1-score across four hierarchical classification levels."
Multi-FAct,2024,Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore | OpenReview,"A pipeline that adapts FActScore for multilingual factuality evaluation. It generates biographies in multiple languages, translates them to English, decomposes them into atomic facts, and verifies each fact against English Wikipedia using open-source models.","Knowledge Source: Primarily English Wikipedia for verification. Also explored using non-English Wikipedia articles. 
Evaluation Topics: Biographies of national leaders from 2015, curated for geographical diversity.",Found that English generations consistently have higher factual accuracy and quantity of facts. Revealed a strong bias towards North American and European topics across all evaluated languages.,"Core LLMs (for generation): GPT-4 and GPT-3.5. 
Core Models (for verification): Mistral-7B for decomposition and verification. 
Translation: GPT-3.5. 
Metric: FActScore."
FIRE,2025,[2411.00784] FIRE: Fact-checking with Iterative Retrieval and Verification,"An agent-based framework that iteratively integrates evidence retrieval and verification. It decides at each step whether to output a final answer based on confidence or to generate another search query, mimicking human search strategies to reduce cost.","Evaluation: Factcheck-Bench, FacTool-QA, FELM-WK, and BingCheck.",Achieved slightly better performance than strong baselines while reducing LLM costs by 7.6x and search costs by 16.5x.,"Core LLMs: GPT-4o, GPT-4o-mini, Claude models, LLaMA 3.1, Mistral 7B. 
Retrieval: Google Search via SerpAPI. 
Baselines: FACTOOL, FACTCHECK-GPT, SAFE. 
Metrics: Precision, Recall, F1-score, and computational/financial costs."
FND-LLM,2024,LLM-Enhanced multimodal detection of fake news | PLOS One,"A multimodal framework combining SLMs and LLMs. It uses separate branches to extract textual features (BERT), visual semantics (ViT), and visual tampering traces (EAViT). A co-attention network and CLIP handle cross-modal fusion, while an LLM branch provides explanatory justifications.","Evaluation: Weibo (Chinese), Gossipcop, and Politifact (English).","Outperformed existing models on all three datasets, achieving accuracy improvements of 0.7% on Weibo, 6.8% on Gossipcop, and 1.3% on Politifact.","Core Models: BERT, ViT, EAViT for feature extraction; CLIP for cross-modal analysis; MMoE for feature integration; LLMs (e.g., GPT-4) for the rationale/justification branch. 
Baselines: att-RNN, EANN, MVAE, SAFE, Spotfake+, CAFE, CMC. 
Metrics: Accuracy, Precision, Recall, F1-score."
Multi-Source Knowledge-enhanced Graph Attention Network (MultiKE-GAT),2024,Multi-source Knowledge Enhanced Graph Attention Networks for Multimodal Fact Verification,"A multimodal fact verification model using a multi-source knowledge-enhanced graph attention network. It extracts fine-grained entities (textual, visual, key info from LLMs), constructs a heterogeneous graph, and uses a Knowledge-aware Graph Fusion (KGF) module to learn representations and reduce noise.",Evaluation: FACTIFY and MOCHEG.,"Achieved state-of-the-art performance on both datasets, reaching 79.64% weighted F1 on FACTIFY (5-way) and 70.14% F1 on MOCHEG.","Knowledge Extraction: TAGME (textual entities), Faster R-CNN (visual objects), GPT-3.5-turbo (key info). 
Core Models: DeBERTa and Swin Transformer for initial embeddings; Graph Attention Network for fusion. 
Baselines: DeBERTa, Swin Transformer, GPT-3.5, CLIP, ConcatNet, UofA-Truth, Pre-CoFact."
"Multimodal Automated
Fact-checking (MAFT)",2025,MAFT: Multimodal Automated Fact-Checking via Textualization | Proceedings of the AAAI Conference on Artificial Intelligence,"A multimodal automated fact-checking system that textualizes all inputs (images, videos, audio) using ML models (VLM, speech recognition, deepfake detectors). An LLM then analyzes the textualized content alongside retrieved external web evidence to generate an interpretable report.",The paper does not specify datasets used for a quantitative evaluation but describes the system's architecture and provides a qualitative example.,"The paper focuses on the framework description and a qualitative example, without reporting specific quantitative gains against benchmarks.","Core LLM: GPT-4. 
Textualization Models: GPT-4o (VLM), Whisper (speech recognition), ICT (face deepfake detector), Whisper-based detector (audio deepfake detector). 
Retrieval: Google Custom Search API."
FACT-GPT,2024,FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs,"A system that leverages LLMs for automated claim matching in fact-checking. It is trained on a synthetic dataset to classify social media posts as entailing, contradicting, or being neutral to previously debunked claims.","Training: A synthetic dataset of 3,675 tweets generated by GPT-4, GPT-3.5, and Llama-2-70b based on debunked COVID-19 claims. 
Evaluation: A ground truth dataset of 1,225 real tweets paired with claims and annotated by humans.","Fine-tuned smaller models (GPT-3.5-Turbo, Llama-2-7b/13b) achieved performance comparable to or better than a pre-trained GPT-4, with the best models reaching 73% accuracy.","Core LLMs: GPT-4, GPT-3.5-Turbo, Llama-2 models. 
Tuning: GPT-3.5 was fine-tuned via API; Llama models were fine-tuned using LoRA. 
Task: 3-class textual entailment (Entailment, Neutral, Contradiction). 
Metrics: Precision, Recall, Accuracy, F1-score."
"Relevant Evidence Detection Directed
Transformer (RED-DOT)",2025,https://ieeexplore.ieee.org/abstract/document/10948326/,"A multimodal fact-checking framework with a ""relevant evidence detection"" (RED) module. It reranks retrieved web evidence, fuses text and image modalities, and simultaneously performs verdict prediction and determines which evidence is relevant to the claim.","Training: NewsCLIPpings+. 
Evaluation: VERITE, a benchmark for real-world multimodal misinformation.",Achieved up to 33.7% accuracy improvement on the VERITE benchmark compared to baselines without evidence. Surpassed state-of-the-art on NewsCLIPpings+ by up to 3% without needing multiple encoders or extensive evidence.,"Core Architecture: Transformer encoder with element-wise modality fusion and multitask learning for verdict prediction and RED. 
Backbone Encoders: Tested CLIP ViT B/32, L/14, ALBEF, and BLIP2. 
Baselines: CCN, ECENet, SEN, DT."
Planning And Customized Action Reasoning (PACAR),2024,PACAR: Automated Fact-Checking with Planning and Customized Action Reasoning Using Large Language Models,"An LLM-based fact-checking framework with four modules: a claim decomposer with self-reflection, a planner, an executor, and a verifier. It focuses on dynamic planning and execution of customized actions (e.g., numerical reasoning, entity disambiguation).","Evaluation: SciFact, FEVEROUS, and HOVER","PACAR model exhibits 4.66% and 4.81% in the open-book settings in HOVER’s 4-hop claims and FEVEROUS dataset, respectively. There are 4.9%, and 3.24% improvements on the SciFact dataset in open-book setting and gold-evidence setting.","Core LLM: gpt-3.5-turbo-0301 for all modules. 
Retrieval: Google service via Serper API. 
Baselines: Fine-tuned (BERT-FC, MULTIVERS, LisT4, RoBERTa-NLI), Few-shot (CodeX, FLAN-T5, ProgramFC), and Zero-shot (ChatGPT) models.
Metric: Macro-F1 score."
"Learning and Evaluation Augmented
by Fact-Checking (LEAF)",2024,LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models,A dual-strategy framework to improve LLM factualness. Strategy 1 (Fact-Check-Then-RAG): Uses fact-checking results to guide RAG without updating model parameters. Strategy 2 (Self-Training): Updates model parameters via SFT on fact-checked responses or SimPO using fact-checking as a ranker.,"Evaluation: Five medical QA datasets: USMLE, MMLU-Medical, PubMedQA, BioASQ, MedMCQA.","Filtered accuracy via LEAF significantly improved over the original Llama-3 70B's accuracy (73.5% to 86.5% on USMLE). 
Fact-Check-Then-RAG (FC-RAG) showed a 4.99% improvement on USMLE, 1.66% on MMLU-Medical, 13.0% on PubMedQA, 7.28% on BioASQ, and 1.56% on MedMCQA consistently improved accuracy across all datasets. 
The SFT approach using LEAF shows notable improvements: an increase of 4.71% on USMLE, 4.87% on MMLU-Medical, 6.60% on PubMedQA, 4.53% on BioASQ, and 2.97% on MedMCQA compared to the original model performance. 
The SimPO approach using LEAF shows significant improvements: an increase of 4.08% on USMLE, 2.67% on MMLU-Medical, 6.80% on PubMedQA, 7.45% on BioASQ, and 2.89% on MedMCQA compared to the original model performance.","Core LLMs: Llama 3 70B Instruct (no parameter updates), Llama 3 8B Instruct (self-training). 
Fact-Checker: An adapted version of SAFE using Qwen2-72B as the rater and MedRAG corpus for retrieval.
Baselines: Standard LLM, MedRAG, Factcheck-GPT, ArmoRM."
Bilateral Defusing Verification (BiDeV),2025,[2502.16181] BiDeV: Bilateral Defusing Verification for Complex Claim Fact-Checking,"A Bilateral Defusing Verification workflow using role-played LLMs. It consists of two modules: Vagueness Defusing simplifies complex claims by resolving latent info and complex relations, and Redundancy Defusing filters evidence to enhance its quality.",Evaluation: Hover and Feverous-s.,"Achieved the best performance on both benchmarks, improving Macro-F1 by an average of 3.88% in both gold and open settings compared to the previous SOTA.","Core LLMs: gpt-3.5-turbo for perception, rewriting, decomposition, and filtering; Flan-T5-XL for querying and checking. 
Retrieval: BM25 for the open setting. 
Baselines: Pre-trained (BERT-FC, LisT5), Fine-tuned (RoBERTa-NLI, DeBERTaV3-NLI, MULTIVERS), LLM-ICL (Codex, FLAN-T5), and LLM-reason (HiSS, FOLK, ProgramFC, FactcheckGPT) methods."
FACT-AUDIT,2025,[2502.17924] FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models,"An adaptive multi-agent framework for dynamically evaluating LLM fact-checking capabilities. It uses importance sampling and multi-agent collaboration to generate scalable test datasets and perform iterative, model-centric evaluations that include justification production.","Data Generation: Dynamically generates and updates test scenarios based on a taxonomy of complex claims, fake news, and social rumors. Seed questions were also sampled from the Pinocchio dataset for a reliability check.","The framework effectively differentiates state-of-the-art LLMs, identifying GPT-4o and Qwen2.5-72B as the top performers. GPT-4o showed overall IMR (lower is better) of 12.02, JFR (lower is better) of 3.55, and an overall Grade (higher is better) of 7.21. Qwen2.5-72B showed overall IMR of 16.00, JFR of 3.50, and an overall Grade of 7.17. It also reveals that LLMs struggle more with complex claims than fake news.","Core LLM (for agents): GPT-4o. 
Target LLMs (for evaluation): 13 SOTA models including GPT-4o, Claude 3.5, Gemini-Pro, Llama 3.1, Qwen2.5, and Mistral models. 
Metrics: Insight Mastery Rate (IMR), Justification Flaw Rate (JFR), and Grade (1-10 scale)."